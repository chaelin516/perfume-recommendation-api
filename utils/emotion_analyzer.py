# utils/emotion_analyzer.py - Whiff ê°ì • ë¶„ì„ ì‹œìŠ¤í…œ (ë²¡í„°ë¼ì´ì € ì—°ë™)

import re
import logging
import json
import os
import asyncio
import time
from typing import List, Dict, Tuple, Optional, Union, Any
from datetime import datetime
from collections import Counter
import numpy as np

# ğŸ†• ê°ì • ëª¨ë¸ ë¡œë” import
from .emotion_model_loader import (
    get_vectorizer,
    get_emotion_model,
    is_emotion_models_available,
    get_emotion_models_status
)

logger = logging.getLogger(__name__)


class EmotionAnalyzer:
    """
    Whiff ì‹œí–¥ì¼ê¸° í…ìŠ¤íŠ¸ì˜ ê°ì • ë¶„ì„ ë° íƒœê·¸ ìƒì„± ì„œë¹„ìŠ¤

    Features:
    - ğŸ†• ë²¡í„°ë¼ì´ì € ê¸°ë°˜ ML ë¶„ì„ (ìš°ì„ ìˆœìœ„ 1)
    - ë£° ê¸°ë°˜ ê°ì • ë¶„ì„ (í´ë°±)
    - 8ê°œ í•µì‹¬ ê°ì • ì§€ì›
    - í–¥ìˆ˜ ë„ë©”ì¸ íŠ¹í™” í‚¤ì›Œë“œ
    - í´ë°± ë©”ì»¤ë‹ˆì¦˜
    - ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
    """

    def __init__(self):
        """ê°ì • ë¶„ì„ê¸° ì´ˆê¸°í™”"""
        logger.info("ğŸ­ ê°ì • ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹œì‘...")

        # ğŸ¯ ê°ì •ë³„ íƒœê·¸ ë§¤í•‘ (ê¸°ì¡´ ìœ ì§€)
        self.emotion_to_tags = {
            "ê¸°ì¨": ["#joyful", "#bright", "#citrus", "#happy", "#cheerful"],
            "ë¶ˆì•ˆ": ["#nervous", "#sharp", "#spicy", "#anxious", "#tense"],
            "ë‹¹í™©": ["#confused", "#mild", "#powdery", "#surprised", "#bewildered"],
            "ë¶„ë…¸": ["#angry", "#hot", "#burntwood", "#intense", "#fiery"],
            "ìƒì²˜": ["#hurt", "#cool", "#woody", "#sad", "#melancholy"],
            "ìŠ¬í””": ["#sad", "#deep", "#musk", "#blue", "#tearful"],
            "ìš°ìš¸": ["#depressed", "#dark", "#leather", "#gloomy", "#heavy"],
            "í¥ë¶„": ["#excited", "#fresh", "#green", "#energetic", "#vibrant"],
            "ì¤‘ë¦½": ["#neutral", "#calm", "#balanced"]  # ğŸ†• ì¤‘ë¦½ ê°ì • ì¶”ê°€
        }

        # ğŸ” ë£° ê¸°ë°˜ ê°ì • í‚¤ì›Œë“œ ì‚¬ì „ (í–¥ìˆ˜ ë„ë©”ì¸ íŠ¹í™”) - ê¸°ì¡´ ìœ ì§€
        self.emotion_keywords = {
            "ê¸°ì¨": [
                "ì¢‹ì•„", "í–‰ë³µ", "ê¸°ë»", "ì¦ê±°ì›Œ", "ë§Œì¡±", "ì™„ë²½", "ìµœê³ ", "ì‚¬ë‘",
                "ìƒì¾Œ", "ë°ì€", "í™”ì‚¬", "ì‹±ê·¸ëŸ¬ìš´", "ìƒí¼", "ë‹¬ì½¤", "í¬ê·¼", "ë”°ëœ»",
                "ì‚¬ë‘ìŠ¤ëŸ¬ìš´", "ì˜ˆìœ", "ê³ ê¸‰ìŠ¤ëŸ¬ìš´", "ìš°ì•„í•œ", "ì„¸ë ¨ëœ", "ë§¤ë ¥ì ",
                "ë¶€ë“œëŸ¬ìš´", "ì€ì€í•œ", "ê¹”ë”í•œ", "ê¹¨ë—í•œ", "ì²­ëŸ‰í•œ", "ì‹œì›í•œ"
            ],
            "ë¶ˆì•ˆ": [
                "ë¶ˆì•ˆ", "ê±±ì •", "ê¸´ì¥", "ë–¨ë ¤", "ë‘ë ¤ìš´", "ë¬´ì„œìš´", "ì¡°ë§ˆì¡°ë§ˆ",
                "ì–´ìƒ‰", "ë¶€ë‹´", "ì••ë°•", "ìŠ¤íŠ¸ë ˆìŠ¤", "ë¶ˆí¸", "ì–´ìƒ‰í•´",
                "ì´ìƒí•´", "ì–´ìƒ‰í•œ", "ë‹µë‹µ", "ë¬´ê±°ìš´"
            ],
            "ë‹¹í™©": [
                "ë‹¹í™©", "ë†€ë€", "í˜¼ë€", "ì–´ë¦¬ë‘¥ì ˆ", "ë©í•œ", "ëª¨ë¥´ê² ë‹¤", "í—·ê°ˆë ¤",
                "ì´ìƒ", "ì˜ˆìƒê³¼ ë‹¬ë¼", "ì˜ì™¸", "ì‹ ê¸°", "íŠ¹ì´", "ë…íŠ¹",
                "ì˜ˆìƒëª»í•œ", "ëœ»ë°–ì˜", "ê°‘ì‘ìŠ¤ëŸ¬ìš´"
            ],
            "ë¶„ë…¸": [
                "í™”ê°€", "ì§œì¦", "ì—´ë°›", "ë¶„ë…¸", "ê²©ì •", "ì‹«ì–´", "ë³„ë¡œ", "ìµœì•…",
                "ìê·¹ì ", "ê°•ë ¬", "ê³¼í•´", "ë¶€ë‹´ìŠ¤ëŸ¬ì›Œ", "ë…í•´", "ì—­ê²¨ìš´",
                "ë”ì°", "ëª»ì°¸ê² ", "ê²¬ë”œìˆ˜ì—†", "ê·¹í˜"
            ],
            "ìƒì²˜": [
                "ìƒì²˜", "ì•„í”ˆ", "ì„œìš´", "ì‹¤ë§", "ì•„ì‰¬ì›Œ", "ìŠ¬í”ˆ", "í˜ë“ ",
                "ì„­ì„­", "ë§ˆìŒì•„í”ˆ", "ì“¸ì“¸", "ê·¸ë¦¬ìš´", "ê·¸ë¦½", "ì• í‹‹", "ì•ˆíƒ€ê¹Œìš´", "ì•„ë ¨í•œ"
            ],
            "ìŠ¬í””": [
                "ìŠ¬í¼", "ëˆˆë¬¼", "ì• ì ˆ", "ì²˜ëŸ‰", "ê³ ë…", "ì™¸ë¡œìš´", "ì“¸ì“¸",
                "ë¨¹ë¨¹", "ì°¡í•œ", "ìš¸ì»¥", "ì§„í•œ", "ê¹Šì€", "ì°¨ê°€ìš´", "ë¬´ê±°ìš´", "ì¹¨ìš¸í•œ", "ì•”ìš¸í•œ"
            ],
            "ìš°ìš¸": [
                "ìš°ìš¸", "ë‹µë‹µ", "ë¬´ê¸°ë ¥", "ì ˆë§", "ì–´ë‘ ", "ì¹¨ìš¸", "ë©œë‘ì½œë¦¬",
                "ë¸”ë£¨", "ê·¸ëŠ˜ì§„", "ì–´ë‘ìš´", "ë§‰ë§‰í•œ", "ì ˆë§ì ", "í¬ë§ì—†ëŠ”", "ì˜ìš•ì—†ëŠ”", "ê³µí—ˆí•œ"
            ],
            "í¥ë¶„": [
                "í¥ë¶„", "ì‹ ë‚˜", "ë‘ê·¼", "ì„¤ë ˜", "í™œê¸°", "ìƒë™ê°", "ì—ë„ˆì§€",
                "í™œë°œ", "ì—­ë™ì ", "í„ë–¡", "í†¡í†¡", "íŒ¡íŒ¡", "ìƒìƒí•œ", "í™œë ¥", "ì Šì€", "ë°œë„í•œ"
            ]
        }

        # ğŸŒ¸ í–¥ìˆ˜ ë„ë©”ì¸ íŠ¹í™” ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ (ê¸°ì¡´ ìœ ì§€)
        self.perfume_context_keywords = {
            "positive_intensity": {
                "mild": ["ì€ì€", "ë¶€ë“œëŸ¬ìš´", "ê°€ë²¼ìš´", "ì‚´ì§", "ì€ê·¼"],
                "medium": ["ì ë‹¹", "ê´œì°®ì€", "ë¬´ë‚œí•œ", "ê· í˜•ì¡íŒ"],
                "strong": ["ì§„í•œ", "ê°•í•œ", "ê¹Šì€", "í’ë¶€í•œ", "ë†ì¶•ëœ"]
            },
            "positive_quality": [
                "ì¢‹ì•„", "ë§ˆìŒì— ë“¤ì–´", "í–¥ì´ ì¢‹ì•„", "ì˜ˆë»", "ê³ ê¸‰ìŠ¤ëŸ¬ì›Œ", "ìš°ì•„í•´",
                "ì„¸ë ¨ëœ", "ë§¤ë ¥ì ", "í¬ê·¼í•´", "ë”°ëœ»í•´", "ì‹œì›í•´", "ì²­ëŸ‰í•´", "ìƒí¼í•´",
                "ë‹¬ì½¤í•´", "ë¶€ë“œëŸ¬ì›Œ", "ì€ì€í•´", "ê¹”ë”í•´", "ê¹¨ë—í•´", "fresh", "nice"
            ],
            "negative_quality": [
                "ì‹«ì–´", "ë³„ë¡œ", "ì•„ì‰¬ì›Œ", "ì´ìƒí•´", "ì–´ìƒ‰í•´", "ë¶€ë‹´ìŠ¤ëŸ¬ì›Œ", "ê³¼í•´",
                "ë‹¨ì¡°ë¡œì›Œ", "ë°‹ë°‹í•´", "ë…í•´", "ìê·¹ì ", "ì—­ê²¨ìš´", "ë”ì°"
            ],
            "intensity_negative": [
                "ì§„í•´", "ì•½í•´", "ì„¸", "ê°•í•´", "ë…í•´", "ìê·¹ì ", "ì••ë„ì "
            ],
            "temporal_context": [
                "ì²˜ìŒ", "ì²«", "ë‚˜ì¤‘", "ì‹œê°„ì§€ë‚˜", "ì§€ì†", "ë³€í™”", "ë°”ë€Œ"
            ]
        }

        # ğŸ¯ ëª¨ë¸ ìƒíƒœ ê´€ë¦¬
        self.vectorizer_loaded = False
        self.emotion_model_loaded = False
        self.model_version = "v3_vectorizer"  # ğŸ†• ë²¡í„°ë¼ì´ì € ë²„ì „
        self.analysis_count = 0
        self.performance_stats = {
            "total_analyses": 0,
            "successful_analyses": 0,
            "average_response_time": 0.0,
            "method_distribution": {"vectorizer_based": 0, "rule_based": 0},
            "confidence_distribution": {"high": 0, "medium": 0, "low": 0}
        }

        # ğŸ†• ë²¡í„°ë¼ì´ì € ëª¨ë¸ ìƒíƒœ í™•ì¸
        self._check_vectorizer_availability()

        logger.info("âœ… ê°ì • ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"  - ì§€ì› ê°ì •: {list(self.emotion_to_tags.keys())}")
        logger.info(f"  - ëª¨ë¸ ë²„ì „: {self.model_version}")
        logger.info(f"  - ë²¡í„°ë¼ì´ì € ì‚¬ìš© ê°€ëŠ¥: {'âœ…' if self.vectorizer_loaded else 'âŒ'}")
        logger.info(f"  - ì´ í‚¤ì›Œë“œ: {sum(len(keywords) for keywords in self.emotion_keywords.values())}ê°œ")

    def _check_vectorizer_availability(self):
        """ğŸ†• ë²¡í„°ë¼ì´ì € ì‚¬ìš© ê°€ëŠ¥ì„± í™•ì¸"""
        try:
            self.vectorizer_loaded = is_emotion_models_available()

            if self.vectorizer_loaded:
                # ë²¡í„°ë¼ì´ì € ì •ë³´ ë¡œê¹…
                vectorizer = get_vectorizer()
                if vectorizer and hasattr(vectorizer, 'vocabulary_'):
                    vocab_size = len(vectorizer.vocabulary_)
                    logger.info(f"ğŸ“Š ë²¡í„°ë¼ì´ì € ì–´íœ˜ í¬ê¸°: {vocab_size:,}")

                emotion_model = get_emotion_model()
                self.emotion_model_loaded = emotion_model is not None

                logger.info(f"ğŸ¤– ê°ì • ëª¨ë¸ ë¡œë”©ë¨: {'âœ…' if self.emotion_model_loaded else 'âŒ'}")
            else:
                logger.warning("âš ï¸ ë²¡í„°ë¼ì´ì € ì‚¬ìš© ë¶ˆê°€ - ë£° ê¸°ë°˜ ë¶„ì„ë§Œ ì‚¬ìš©")

        except Exception as e:
            logger.error(f"âŒ ë²¡í„°ë¼ì´ì € í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
            self.vectorizer_loaded = False
            self.emotion_model_loaded = False

    async def analyze_emotion(self, text: str, use_vectorizer: bool = True) -> Dict[str, Any]:
        """
        í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„ (ë²¡í„°ë¼ì´ì € ìš°ì„ , ì‹¤íŒ¨ ì‹œ ë£° ê¸°ë°˜ í´ë°±)

        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸
            use_vectorizer: ë²¡í„°ë¼ì´ì € ì‚¬ìš© ì—¬ë¶€ (Falseë©´ ë£° ê¸°ë°˜ë§Œ ì‚¬ìš©)

        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        start_time = time.time()
        self.analysis_count += 1

        logger.info(f"ğŸ­ ê°ì • ë¶„ì„ ì‹œì‘ (#{self.analysis_count})")
        logger.info(f"  - í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)}ì")
        logger.info(f"  - ë²¡í„°ë¼ì´ì € ì‚¬ìš©: {'âœ…' if use_vectorizer and self.vectorizer_loaded else 'âŒ'}")

        # ì…ë ¥ ê²€ì¦
        if not text or not text.strip():
            return self._create_empty_result()

        if len(text) > 2000:
            logger.warning(f"âš ï¸ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤: {len(text)}ì")
            return self._create_error_result("text_too_long", "í…ìŠ¤íŠ¸ê°€ 2000ìë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤.")

        try:
            # ğŸ¤– ë²¡í„°ë¼ì´ì € ê¸°ë°˜ ë¶„ì„ ì‹œë„ (ìš°ì„ ìˆœìœ„)
            if use_vectorizer and self.vectorizer_loaded:
                try:
                    logger.info(f"ğŸ¤– ë²¡í„°ë¼ì´ì € ê¸°ë°˜ ë¶„ì„ ì‹œì‘...")
                    vectorizer_result = await self._analyze_with_vectorizer(text)

                    if vectorizer_result.get("success"):
                        response_time = time.time() - start_time
                        self._update_performance_stats(vectorizer_result, response_time)

                        logger.info(f"âœ… ë²¡í„°ë¼ì´ì € ë¶„ì„ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {response_time:.3f}ì´ˆ)")
                        logger.info(f"  - ê°ì •: {vectorizer_result.get('primary_emotion')}")
                        logger.info(f"  - ì‹ ë¢°ë„: {vectorizer_result.get('confidence', 0):.3f}")

                        return vectorizer_result
                    else:
                        logger.warning("âš ï¸ ë²¡í„°ë¼ì´ì € ë¶„ì„ ì‹¤íŒ¨, ë£° ê¸°ë°˜ìœ¼ë¡œ í´ë°±")

                except Exception as e:
                    logger.error(f"âŒ ë²¡í„°ë¼ì´ì € ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")

            # ğŸ“‹ ë£° ê¸°ë°˜ ë¶„ì„ (í´ë°± ë˜ëŠ” ê¸°ë³¸)
            logger.info(f"ğŸ“‹ ë£° ê¸°ë°˜ ê°ì • ë¶„ì„ ì‹œì‘...")
            rule_result = await self._analyze_with_rules(text)

            response_time = time.time() - start_time
            self._update_performance_stats(rule_result, response_time)

            logger.info(f"âœ… ë£° ê¸°ë°˜ ë¶„ì„ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {response_time:.3f}ì´ˆ)")
            logger.info(f"  - ê°ì •: {rule_result.get('primary_emotion')}")
            logger.info(f"  - ì‹ ë¢°ë„: {rule_result.get('confidence', 0):.3f}")

            return rule_result

        except Exception as e:
            logger.error(f"âŒ ê°ì • ë¶„ì„ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            return self._create_error_result("analysis_exception", str(e))

    async def _analyze_with_vectorizer(self, text: str) -> Dict[str, Any]:
        """ğŸ†• ë²¡í„°ë¼ì´ì €ë¥¼ ì‚¬ìš©í•œ ê°ì • ë¶„ì„"""
        try:
            vectorizer = get_vectorizer()
            emotion_model = get_emotion_model()

            if not vectorizer:
                raise Exception("ë²¡í„°ë¼ì´ì €ê°€ ë¡œë“œë˜ì§€ ì•ŠìŒ")

            # 1. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ë²¡í„°í™”
            logger.debug(f"ğŸ“ í…ìŠ¤íŠ¸ ë²¡í„°í™”: '{text[:50]}...'")

            # í…ìŠ¤íŠ¸ ì •ì œ (ì„ íƒì )
            cleaned_text = self._preprocess_text_for_vectorizer(text)

            # ë²¡í„°í™”
            text_vector = vectorizer.transform([cleaned_text])

            logger.debug(f"ğŸ“Š ë²¡í„° í¬ê¸°: {text_vector.shape}")

            # 2. ê°ì • ë¶„ë¥˜ (ML ëª¨ë¸ì´ ìˆëŠ” ê²½ìš°)
            if emotion_model and self.emotion_model_loaded:
                try:
                    # ML ëª¨ë¸ë¡œ ê°ì • ì˜ˆì¸¡
                    emotion_probs = emotion_model.predict_proba(text_vector)[0]
                    emotion_classes = emotion_model.classes_

                    # ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ ê°ì • ì„ íƒ
                    best_emotion_idx = np.argmax(emotion_probs)
                    primary_emotion = emotion_classes[best_emotion_idx]
                    confidence = emotion_probs[best_emotion_idx]

                    logger.debug(f"ğŸ¤– ML ëª¨ë¸ ì˜ˆì¸¡: {primary_emotion} (ì‹ ë¢°ë„: {confidence:.3f})")

                except Exception as e:
                    logger.warning(f"âš ï¸ ML ëª¨ë¸ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
                    # ë²¡í„° ê¸°ë°˜ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ í´ë°±
                    primary_emotion, confidence = self._vectorizer_heuristic_analysis(text_vector, text)
            else:
                # ML ëª¨ë¸ì´ ì—†ìœ¼ë©´ ë²¡í„° ê¸°ë°˜ íœ´ë¦¬ìŠ¤í‹± ì‚¬ìš©
                primary_emotion, confidence = self._vectorizer_heuristic_analysis(text_vector, text)

            # 3. ê°ì • íƒœê·¸ ìƒì„±
            emotion_tags = self.emotion_to_tags.get(primary_emotion, ["#neutral"])

            return {
                "success": True,
                "method": "vectorizer_based",
                "primary_emotion": primary_emotion,
                "confidence": round(confidence, 3),
                "emotion_tags": emotion_tags,
                "analysis_details": {
                    "vector_shape": list(text_vector.shape),
                    "text_length": len(text),
                    "cleaned_text_length": len(cleaned_text),
                    "model_used": "ML" if self.emotion_model_loaded else "Heuristic"
                },
                "analyzed_at": datetime.now().isoformat(),
                "analyzer_version": self.model_version
            }

        except Exception as e:
            logger.error(f"âŒ ë²¡í„°ë¼ì´ì € ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return {"success": False, "reason": "vectorizer_error", "message": str(e)}

    def _preprocess_text_for_vectorizer(self, text: str) -> str:
        """ğŸ†• ë²¡í„°ë¼ì´ì €ìš© í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        # ê¸°ë³¸ ì •ì œ
        cleaned = text.strip()

        # ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±° (ì„ íƒì )
        cleaned = re.sub(r'[^\w\sê°€-í£]', ' ', cleaned)

        # ì—°ì†ëœ ê³µë°± ì •ë¦¬
        cleaned = re.sub(r'\s+', ' ', cleaned)

        return cleaned.strip()

    def _vectorizer_heuristic_analysis(self, text_vector, original_text: str) -> Tuple[str, float]:
        """ğŸ†• ë²¡í„° ê¸°ë°˜ íœ´ë¦¬ìŠ¤í‹± ê°ì • ë¶„ì„"""
        try:
            # ë²¡í„°ì˜ íŠ¹ì„±ì„ ë¶„ì„í•˜ì—¬ ê°ì • ì¶”ì •
            vector_array = text_vector.toarray()[0]

            # ë²¡í„° í†µê³„ ê³„ì‚°
            vector_sum = np.sum(vector_array)
            vector_mean = np.mean(vector_array)
            vector_std = np.std(vector_array)
            non_zero_count = np.count_nonzero(vector_array)

            logger.debug(f"ğŸ“Š ë²¡í„° í†µê³„: sum={vector_sum:.2f}, mean={vector_mean:.4f}, "
                         f"std={vector_std:.4f}, non_zero={non_zero_count}")

            # íœ´ë¦¬ìŠ¤í‹± ê·œì¹™ìœ¼ë¡œ ê°ì • ë¶„ë¥˜
            confidence = 0.6  # ê¸°ë³¸ ì‹ ë¢°ë„

            # 1. ë²¡í„° ë°€ë„ê°€ ë†’ìœ¼ë©´ ê°•í•œ ê°ì •
            if vector_sum > np.percentile(vector_array, 95):
                if "ì¢‹" in original_text or "ì‚¬ë‘" in original_text:
                    return "ê¸°ì¨", 0.75
                elif "ì‹«" in original_text or "í™”" in original_text:
                    return "ë¶„ë…¸", 0.75
                else:
                    return "í¥ë¶„", 0.65

            # 2. ë²¡í„° ë°€ë„ê°€ ë‚®ìœ¼ë©´ ì°¨ë¶„í•œ ê°ì •
            elif vector_sum < np.percentile(vector_array, 25):
                if "ìŠ¬" in original_text or "ìš°ìš¸" in original_text:
                    return "ìŠ¬í””", 0.7
                else:
                    return "ì¤‘ë¦½", 0.5

            # 3. ì¤‘ê°„ ë²”ìœ„ëŠ” ë£° ê¸°ë°˜ìœ¼ë¡œ ë³´ì™„
            else:
                # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì²´í¬
                text_lower = original_text.lower()
                for emotion, keywords in self.emotion_keywords.items():
                    for keyword in keywords[:5]:  # ìƒìœ„ 5ê°œ í‚¤ì›Œë“œë§Œ ì²´í¬
                        if keyword in text_lower:
                            return emotion, 0.65

                return "ì¤‘ë¦½", 0.5

        except Exception as e:
            logger.warning(f"âš ï¸ ë²¡í„° íœ´ë¦¬ìŠ¤í‹± ë¶„ì„ ì‹¤íŒ¨: {e}")
            return "ì¤‘ë¦½", 0.3

    async def _analyze_with_rules(self, text: str) -> Dict[str, Any]:
        """ë£° ê¸°ë°˜ ê°ì • ë¶„ì„ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)"""
        try:
            text_lower = text.lower().strip()
            text_words = text.split()

            # ğŸ” 1ë‹¨ê³„: ê¸°ë³¸ ê°ì • í‚¤ì›Œë“œ ë§¤ì¹­ (ê¸°ì¡´ ë¡œì§)
            emotion_scores = {}
            keyword_matches = {}
            total_keywords_found = 0

            for emotion, keywords in self.emotion_keywords.items():
                score = 0
                matched_keywords = []

                for keyword in keywords:
                    pattern = r'\b' + re.escape(keyword) + r'\b'
                    matches = len(re.findall(pattern, text_lower))

                    if matches > 0:
                        weight = self._get_keyword_weight(keyword, emotion)
                        score += matches * weight
                        matched_keywords.extend([keyword] * matches)
                        total_keywords_found += matches

                if score > 0:
                    emotion_scores[emotion] = score
                    keyword_matches[emotion] = matched_keywords

            # ğŸŒ¸ 2ë‹¨ê³„: í–¥ìˆ˜ ë„ë©”ì¸ ì»¨í…ìŠ¤íŠ¸ ë³´ì • (ê¸°ì¡´ ë¡œì§)
            context_boost = self._analyze_perfume_context(text_lower, text_words)

            for emotion, boost in context_boost.items():
                if emotion in emotion_scores:
                    emotion_scores[emotion] += boost
                elif boost > 0.5:
                    emotion_scores[emotion] = boost
                    keyword_matches[emotion] = ["context_boost"]

            # ğŸ“Š 3ë‹¨ê³„: ê²°ê³¼ ê³„ì‚° ë° ì •ê·œí™” (ê¸°ì¡´ ë¡œì§)
            if emotion_scores:
                normalization_factor = max(len(text_words), 1)
                normalized_scores = {}

                for emotion, score in emotion_scores.items():
                    normalized_score = min(score / normalization_factor * 1.5, 1.0)
                    normalized_scores[emotion] = normalized_score

                primary_emotion = max(normalized_scores.keys(),
                                      key=lambda x: normalized_scores[x])
                confidence = normalized_scores[primary_emotion]

                confidence_boost = min(total_keywords_found * 0.1, 0.3)
                final_confidence = min(confidence + confidence_boost, 1.0)

                emotion_tags = self.emotion_to_tags.get(primary_emotion, ["#neutral"])

                return {
                    "success": True,
                    "method": "rule_based",
                    "primary_emotion": primary_emotion,
                    "confidence": round(final_confidence, 3),
                    "emotion_tags": emotion_tags,
                    "analysis_details": {
                        "raw_scores": emotion_scores,
                        "normalized_scores": normalized_scores,
                        "keyword_matches": keyword_matches,
                        "context_boost": context_boost,
                        "total_keywords": total_keywords_found,
                        "text_length": len(text),
                        "word_count": len(text_words)
                    },
                    "analyzed_at": datetime.now().isoformat(),
                    "analyzer_version": "rule_based_v1.3"
                }
            else:
                return self._create_neutral_result("no_emotion_keywords")

        except Exception as e:
            logger.error(f"âŒ ë£° ê¸°ë°˜ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return self._create_error_result("rule_analysis_error", str(e))

    # ê¸°ì¡´ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤ ìœ ì§€
    def _get_keyword_weight(self, keyword: str, emotion: str) -> float:
        """í‚¤ì›Œë“œ ì¤‘ìš”ë„ ê°€ì¤‘ì¹˜ ê³„ì‚° (ê¸°ì¡´ ë¡œì§)"""
        high_weight_keywords = {
            "ê¸°ì¨": ["ì¢‹ì•„", "í–‰ë³µ", "ì‚¬ë‘", "ì™„ë²½", "ìµœê³ "],
            "ë¶ˆì•ˆ": ["ë¶ˆì•ˆ", "ê±±ì •", "ë‘ë ¤ìš´", "ë¶€ë‹´"],
            "ë‹¹í™©": ["ë‹¹í™©", "ë†€ë€", "í˜¼ë€", "ì˜ì™¸"],
            "ë¶„ë…¸": ["í™”ê°€", "ì§œì¦", "ì‹«ì–´", "ìµœì•…"],
            "ìƒì²˜": ["ìƒì²˜", "ì•„í”ˆ", "ì‹¤ë§", "ê·¸ë¦¬ìš´"],
            "ìŠ¬í””": ["ìŠ¬í¼", "ëˆˆë¬¼", "ì™¸ë¡œìš´", "ì“¸ì“¸"],
            "ìš°ìš¸": ["ìš°ìš¸", "ì ˆë§", "ë¬´ê¸°ë ¥", "ì–´ë‘ "],
            "í¥ë¶„": ["í¥ë¶„", "ì‹ ë‚˜", "ì„¤ë ˜", "ì—ë„ˆì§€"]
        }

        if keyword in high_weight_keywords.get(emotion, []):
            return 1.5
        elif len(keyword) >= 3:
            return 1.2
        else:
            return 1.0

    def _analyze_perfume_context(self, text_lower: str, text_words: List[str]) -> Dict[str, float]:
        """í–¥ìˆ˜ ë„ë©”ì¸ íŠ¹í™” ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ (ê¸°ì¡´ ë¡œì§)"""
        context_boost = {}

        positive_quality_count = 0
        for keyword in self.perfume_context_keywords["positive_quality"]:
            positive_quality_count += text_lower.count(keyword)

        negative_quality_count = 0
        for keyword in self.perfume_context_keywords["negative_quality"]:
            negative_quality_count += text_lower.count(keyword)

        intensity_negative_count = 0
        for keyword in self.perfume_context_keywords["intensity_negative"]:
            intensity_negative_count += text_lower.count(keyword)

        if positive_quality_count > 0:
            boost_strength = min(positive_quality_count * 0.8, 2.0)
            context_boost["ê¸°ì¨"] = boost_strength

            if positive_quality_count >= 2:
                context_boost["í¥ë¶„"] = boost_strength * 0.6

        if negative_quality_count > 0:
            boost_strength = min(negative_quality_count * 0.7, 1.8)
            if negative_quality_count >= 2:
                context_boost["ë¶„ë…¸"] = boost_strength
            else:
                context_boost["ìƒì²˜"] = boost_strength * 0.8

        if intensity_negative_count > 0:
            boost_strength = min(intensity_negative_count * 0.6, 1.5)
            context_boost["ë¶ˆì•ˆ"] = boost_strength

        temporal_keywords = ["ì²˜ìŒ", "ì²«", "ë‚˜ì¤‘", "ì‹œê°„ì§€ë‚˜", "ë³€í™”"]
        temporal_count = sum(text_lower.count(kw) for kw in temporal_keywords)
        if temporal_count > 0:
            context_boost["ë‹¹í™©"] = min(temporal_count * 0.5, 1.0)

        return context_boost

    def _create_empty_result(self) -> Dict[str, Any]:
        """ë¹ˆ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ê²°ê³¼ (ê¸°ì¡´)"""
        return {
            "success": True,
            "method": "validation",
            "primary_emotion": "ì¤‘ë¦½",
            "confidence": 0.0,
            "emotion_tags": ["#neutral"],
            "reason": "empty_text",
            "message": "ë¶„ì„í•  í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.",
            "analyzed_at": datetime.now().isoformat()
        }

    def _create_neutral_result(self, reason: str) -> Dict[str, Any]:
        """ì¤‘ë¦½ ê°ì • ê²°ê³¼ (ê¸°ì¡´)"""
        return {
            "success": True,
            "method": "rule_based",
            "primary_emotion": "ì¤‘ë¦½",
            "confidence": 0.3,
            "emotion_tags": ["#neutral", "#calm"],
            "reason": reason,
            "message": "ëª…í™•í•œ ê°ì • ì‹ í˜¸ê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
            "analyzed_at": datetime.now().isoformat()
        }

    def _create_error_result(self, error_type: str, message: str) -> Dict[str, Any]:
        """ì—ëŸ¬ ê²°ê³¼ (ê¸°ì¡´)"""
        return {
            "success": False,
            "error_type": error_type,
            "message": message,
            "primary_emotion": "ì˜¤ë¥˜",
            "confidence": 0.0,
            "emotion_tags": ["#error"],
            "analyzed_at": datetime.now().isoformat()
        }

    def _update_performance_stats(self, result: Dict[str, Any], response_time: float):
        """ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸ (ìˆ˜ì •)"""
        self.performance_stats["total_analyses"] += 1

        if result.get("success"):
            self.performance_stats["successful_analyses"] += 1

            method = result.get("method", "unknown")
            if method in self.performance_stats["method_distribution"]:
                self.performance_stats["method_distribution"][method] += 1
            elif method == "vectorizer_based":  # ğŸ†• ìƒˆë¡œìš´ ë°©ë²• ì¶”ê°€
                self.performance_stats["method_distribution"]["vectorizer_based"] += 1

            confidence = result.get("confidence", 0.0)
            if confidence >= 0.7:
                self.performance_stats["confidence_distribution"]["high"] += 1
            elif confidence >= 0.4:
                self.performance_stats["confidence_distribution"]["medium"] += 1
            else:
                self.performance_stats["confidence_distribution"]["low"] += 1

        total_time = (self.performance_stats["average_response_time"] *
                      (self.performance_stats["total_analyses"] - 1) + response_time)
        self.performance_stats["average_response_time"] = total_time / self.performance_stats["total_analyses"]

    def get_analysis_stats(self) -> Dict[str, Any]:
        """ë¶„ì„ ì‹œìŠ¤í…œ ìƒíƒœ ì •ë³´ (ìˆ˜ì •)"""
        success_rate = 0.0
        if self.performance_stats["total_analyses"] > 0:
            success_rate = (self.performance_stats["successful_analyses"] /
                            self.performance_stats["total_analyses"] * 100)

        # ğŸ†• ë²¡í„°ë¼ì´ì € ìƒíƒœ ì¶”ê°€
        vectorizer_status = get_emotion_models_status()

        return {
            "model_loaded": self.vectorizer_loaded,
            "model_version": self.model_version,
            "supported_emotions": len(self.emotion_to_tags),
            "total_keywords": sum(len(keywords) for keywords in self.emotion_keywords.values()),
            "analysis_methods": (["vectorizer_based"] if self.vectorizer_loaded else []) + ["rule_based"],

            # ğŸ†• ë²¡í„°ë¼ì´ì € ì •ë³´
            "vectorizer_status": vectorizer_status,
            "emotion_model_available": self.emotion_model_loaded,

            "performance": {
                "total_analyses": self.performance_stats["total_analyses"],
                "successful_analyses": self.performance_stats["successful_analyses"],
                "success_rate": round(success_rate, 2),
                "average_response_time": round(self.performance_stats["average_response_time"], 3),
                "method_distribution": self.performance_stats["method_distribution"],
                "confidence_distribution": self.performance_stats["confidence_distribution"]
            },

            "emotion_list": list(self.emotion_to_tags.keys()),
            "emotion_tags_count": {emotion: len(tags) for emotion, tags in self.emotion_to_tags.items()},

            "system_info": {
                "max_text_length": 2000,
                "supported_languages": ["í•œêµ­ì–´"],
                "domain_specialization": "í–¥ìˆ˜_ë¦¬ë·°",
                "last_updated": datetime.now().isoformat()
            }
        }

    # ê¸°ì¡´ ë©”ì„œë“œë“¤ ìœ ì§€ (get_supported_emotions, get_emotion_tags, ë“±)
    def get_supported_emotions(self) -> List[str]:
        """ì§€ì›í•˜ëŠ” ê°ì • ëª©ë¡ ë°˜í™˜"""
        return list(self.emotion_to_tags.keys())

    def get_emotion_tags(self, emotion: str) -> List[str]:
        """íŠ¹ì • ê°ì •ì˜ íƒœê·¸ ëª©ë¡ ë°˜í™˜"""
        return self.emotion_to_tags.get(emotion, ["#neutral"])

    def reset_performance_stats(self):
        """ì„±ëŠ¥ í†µê³„ ë¦¬ì…‹"""
        logger.info("ğŸ”„ ì„±ëŠ¥ í†µê³„ ë¦¬ì…‹...")
        self.analysis_count = 0
        self.performance_stats = {
            "total_analyses": 0,
            "successful_analyses": 0,
            "average_response_time": 0.0,
            "method_distribution": {"vectorizer_based": 0, "rule_based": 0},
            "confidence_distribution": {"high": 0, "medium": 0, "low": 0}
        }
        logger.info("âœ… ì„±ëŠ¥ í†µê³„ ë¦¬ì…‹ ì™„ë£Œ")


# ğŸŒŸ ì „ì—­ ê°ì • ë¶„ì„ê¸° ì¸ìŠ¤í„´ìŠ¤
emotion_analyzer = EmotionAnalyzer()


# ğŸ§ª í…ŒìŠ¤íŠ¸ ë° ë””ë²„ê¹… í•¨ìˆ˜ë“¤
async def test_emotion_analyzer():
    """ê°ì • ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ (ë²¡í„°ë¼ì´ì € í¬í•¨)"""
    print("ğŸ§ª ê°ì • ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸ ì‹œì‘...\n")

    test_cases = [
        "ì´ í–¥ìˆ˜ ì •ë§ ì¢‹ì•„ìš”! ë‹¬ì½¤í•˜ê³  ìƒí¼í•´ì„œ ê¸°ë¶„ì´ ì¢‹ì•„ì ¸ìš”.",
        "í–¥ì´ ë„ˆë¬´ ì§„í•´ì„œ ë³„ë¡œì˜ˆìš”. ì¢€ ë¶€ë‹´ìŠ¤ëŸ½ë„¤ìš”.",
        "ì²˜ìŒ ë§¡ì•˜ì„ ë•Œ ë†€ëì–´ìš”. ì˜ˆìƒê³¼ ì™„ì „ ë‹¬ë¼ì„œ ë‹¹í™©ìŠ¤ëŸ¬ì› ì–´ìš”.",
        "ì´ í–¥ìˆ˜ë¥¼ ë§¡ìœ¼ë©´ ì˜›ë‚  ìƒê°ì´ ë‚˜ì„œ ìŠ¬í¼ì ¸ìš”.",
        "í–¥ìˆ˜ê°€ ë„ˆë¬´ ìê·¹ì ì´ì–´ì„œ í™”ê°€ ë‚˜ìš”. ìµœì•…ì´ì—ìš”.",
        "ìƒˆë¡œìš´ í–¥ìˆ˜ë¥¼ ë°œê²¬í•´ì„œ ë„ˆë¬´ ì‹ ë‚˜ìš”! ì—ë„ˆì§€ê°€ ë„˜ì³ìš”.",
        "í–¥ì´ ì€ì€í•˜ê³  ê¹”ë”í•´ì„œ ë§ˆìŒì— ë“¤ì–´ìš”.",
        ""  # ë¹ˆ í…ìŠ¤íŠ¸ í…ŒìŠ¤íŠ¸
    ]

    for i, text in enumerate(test_cases, 1):
        print(f"--- í…ŒìŠ¤íŠ¸ {i} ---")
        print(f"ì…ë ¥: {text if text else '(ë¹ˆ í…ìŠ¤íŠ¸)'}")

        # ë²¡í„°ë¼ì´ì € ê¸°ë°˜ í…ŒìŠ¤íŠ¸
        result_vec = await emotion_analyzer.analyze_emotion(text, use_vectorizer=True)
        print(f"ë²¡í„°ë¼ì´ì €: {result_vec['primary_emotion']} (ì‹ ë¢°ë„: {result_vec['confidence']:.3f})")

        # ë£° ê¸°ë°˜ í…ŒìŠ¤íŠ¸
        result_rule = await emotion_analyzer.analyze_emotion(text, use_vectorizer=False)
        print(f"ë£° ê¸°ë°˜: {result_rule['primary_emotion']} (ì‹ ë¢°ë„: {result_rule['confidence']:.3f})")
        print()

    # ì„±ëŠ¥ í†µê³„ ì¶œë ¥
    stats = emotion_analyzer.get_analysis_stats()
    print("ğŸ“Š ì„±ëŠ¥ í†µê³„:")
    print(f"  ì´ ë¶„ì„: {stats['performance']['total_analyses']}íšŒ")
    print(f"  ì„±ê³µë¥ : {stats['performance']['success_rate']}%")
    print(f"  í‰ê·  ì‘ë‹µì‹œê°„: {stats['performance']['average_response_time']:.3f}ì´ˆ")
    print(f"  ë°©ë²•ë³„ ë¶„í¬: {stats['performance']['method_distribution']}")
    print(f"  ë²¡í„°ë¼ì´ì € ìƒíƒœ: {stats['vectorizer_status']}")
    print()

    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    # ì§ì ‘ ì‹¤í–‰ ì‹œ í…ŒìŠ¤íŠ¸
    asyncio.run(test_emotion_analyzer())