# utils/emotion_analyzer.py - Google Drive ëª¨ë¸ ì§€ì› ì¶”ê°€ ë²„ì „

import re
import logging
import json
import os
import asyncio
import time
import hashlib
from typing import List, Dict, Tuple, Optional, Union, Any
from datetime import datetime
from collections import Counter
import numpy as np

logger = logging.getLogger(__name__)


class EmotionAnalyzer:
    """
    Whiff ì‹œí–¥ì¼ê¸° í…ìŠ¤íŠ¸ì˜ ê°ì • ë¶„ì„ ë° íƒœê·¸ ìƒì„± ì„œë¹„ìŠ¤ (Google Drive ëª¨ë¸ ì§€ì›)

    Features:
    - ë£° ê¸°ë°˜ ê°ì • ë¶„ì„ (í˜„ì¬ ë²„ì „)
    - AI ëª¨ë¸ v2 ì¤€ë¹„ êµ¬ì¡°
    - ğŸ†• Google Drive ëª¨ë¸ ìë™ ë‹¤ìš´ë¡œë“œ
    - 8ê°œ í•µì‹¬ ê°ì • ì§€ì›
    - í–¥ìˆ˜ ë„ë©”ì¸ íŠ¹í™” í‚¤ì›Œë“œ
    - í´ë°± ë©”ì»¤ë‹ˆì¦˜
    - ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
    """

    def __init__(self):
        """ê°ì • ë¶„ì„ê¸° ì´ˆê¸°í™”"""
        logger.info("ğŸ­ ê°ì • ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹œì‘...")

        # ğŸ¯ ê°ì •ë³„ íƒœê·¸ ë§¤í•‘ (ì´ˆê¸° ëª¨ë¸ ë²„ì „ - í™•ì¥ ê°€ëŠ¥)
        self.emotion_to_tags = {
            "ê¸°ì¨": ["#joyful", "#bright", "#citrus", "#happy", "#cheerful"],
            "ë¶ˆì•ˆ": ["#nervous", "#sharp", "#spicy", "#anxious", "#tense"],
            "ë‹¹í™©": ["#confused", "#mild", "#powdery", "#surprised", "#bewildered"],
            "ë¶„ë…¸": ["#angry", "#hot", "#burntwood", "#intense", "#fiery"],
            "ìƒì²˜": ["#hurt", "#cool", "#woody", "#sad", "#melancholy"],
            "ìŠ¬í””": ["#sad", "#deep", "#musk", "#blue", "#tearful"],
            "ìš°ìš¸": ["#depressed", "#dark", "#leather", "#gloomy", "#heavy"],
            "í¥ë¶„": ["#excited", "#fresh", "#green", "#energetic", "#vibrant"]
        }

        # ğŸ” ë£° ê¸°ë°˜ ê°ì • í‚¤ì›Œë“œ ì‚¬ì „ (í–¥ìˆ˜ ë„ë©”ì¸ íŠ¹í™”)
        self.emotion_keywords = {
            "ê¸°ì¨": [
                # ì§ì ‘ì  ê°ì • í‘œí˜„
                "ì¢‹ì•„", "í–‰ë³µ", "ê¸°ë»", "ì¦ê±°ì›Œ", "ë§Œì¡±", "ì™„ë²½", "ìµœê³ ", "ì‚¬ë‘",
                # í–¥ìˆ˜ ê´€ë ¨ ê¸ì • í‘œí˜„
                "ìƒì¾Œ", "ë°ì€", "í™”ì‚¬", "ì‹±ê·¸ëŸ¬ìš´", "ìƒí¼", "ë‹¬ì½¤", "í¬ê·¼", "ë”°ëœ»",
                "ì‚¬ë‘ìŠ¤ëŸ¬ìš´", "ì˜ˆìœ", "ê³ ê¸‰ìŠ¤ëŸ¬ìš´", "ìš°ì•„í•œ", "ì„¸ë ¨ëœ", "ë§¤ë ¥ì ",
                # ê°ê°ì  í‘œí˜„
                "ë¶€ë“œëŸ¬ìš´", "ì€ì€í•œ", "ê¹”ë”í•œ", "ê¹¨ë—í•œ", "ì²­ëŸ‰í•œ", "ì‹œì›í•œ"
            ],
            "ë¶ˆì•ˆ": [
                # ì§ì ‘ì  ê°ì • í‘œí˜„
                "ë¶ˆì•ˆ", "ê±±ì •", "ê¸´ì¥", "ë–¨ë ¤", "ë‘ë ¤ìš´", "ë¬´ì„œìš´", "ì¡°ë§ˆì¡°ë§ˆ",
                # í–¥ìˆ˜ ê´€ë ¨ ë¶€ì • í‘œí˜„
                "ì–´ìƒ‰", "ë¶€ë‹´", "ì••ë°•", "ìŠ¤íŠ¸ë ˆìŠ¤", "ë¶ˆí¸", "ì–´ìƒ‰í•´",
                "ì´ìƒí•´", "ì–´ìƒ‰í•œ", "ë‹µë‹µ", "ë¬´ê±°ìš´"
            ],
            "ë‹¹í™©": [
                # ì§ì ‘ì  ê°ì • í‘œí˜„
                "ë‹¹í™©", "ë†€ë€", "í˜¼ë€", "ì–´ë¦¬ë‘¥ì ˆ", "ë©í•œ", "ëª¨ë¥´ê² ë‹¤", "í—·ê°ˆë ¤",
                # ì˜ˆìƒê³¼ ë‹¤ë¥¸ ê²½í—˜
                "ì´ìƒ", "ì˜ˆìƒê³¼ ë‹¬ë¼", "ì˜ì™¸", "ì‹ ê¸°", "íŠ¹ì´", "ë…íŠ¹",
                "ì˜ˆìƒëª»í•œ", "ëœ»ë°–ì˜", "ê°‘ì‘ìŠ¤ëŸ¬ìš´"
            ],
            "ë¶„ë…¸": [
                # ì§ì ‘ì  ê°ì • í‘œí˜„
                "í™”ê°€", "ì§œì¦", "ì—´ë°›", "ë¶„ë…¸", "ê²©ì •", "ì‹«ì–´", "ë³„ë¡œ", "ìµœì•…",
                # í–¥ìˆ˜ ê´€ë ¨ ê°•í•œ ë¶€ì •
                "ìê·¹ì ", "ê°•ë ¬", "ê³¼í•´", "ë¶€ë‹´ìŠ¤ëŸ¬ì›Œ", "ë…í•´", "ì—­ê²¨ìš´",
                "ë”ì°", "ëª»ì°¸ê² ", "ê²¬ë”œìˆ˜ì—†", "ê·¹í˜"
            ],
            "ìƒì²˜": [
                # ì§ì ‘ì  ê°ì • í‘œí˜„
                "ìƒì²˜", "ì•„í”ˆ", "ì„œìš´", "ì‹¤ë§", "ì•„ì‰¬ì›Œ", "ìŠ¬í”ˆ", "í˜ë“ ",
                "ì„­ì„­", "ë§ˆìŒì•„í”ˆ", "ì“¸ì“¸",
                # ê·¸ë¦¬ì›€ê³¼ ì—°ê´€
                "ê·¸ë¦¬ìš´", "ê·¸ë¦½", "ì• í‹‹", "ì•ˆíƒ€ê¹Œìš´", "ì•„ë ¨í•œ"
            ],
            "ìŠ¬í””": [
                # ì§ì ‘ì  ê°ì • í‘œí˜„
                "ìŠ¬í¼", "ëˆˆë¬¼", "ì• ì ˆ", "ì²˜ëŸ‰", "ê³ ë…", "ì™¸ë¡œìš´", "ì“¸ì“¸",
                "ë¨¹ë¨¹", "ì°¡í•œ", "ìš¸ì»¥",
                # ê¹Šì€ ê°ì •
                "ì§„í•œ", "ê¹Šì€", "ì°¨ê°€ìš´", "ë¬´ê±°ìš´", "ì¹¨ìš¸í•œ", "ì•”ìš¸í•œ"
            ],
            "ìš°ìš¸": [
                # ì§ì ‘ì  ê°ì • í‘œí˜„
                "ìš°ìš¸", "ë‹µë‹µ", "ë¬´ê¸°ë ¥", "ì ˆë§", "ì–´ë‘ ", "ì¹¨ìš¸", "ë©œë‘ì½œë¦¬",
                "ë¸”ë£¨", "ê·¸ëŠ˜ì§„", "ì–´ë‘ìš´", "ë§‰ë§‰í•œ",
                # ê¹Šì€ ìš°ìš¸ê°
                "ì ˆë§ì ", "í¬ë§ì—†ëŠ”", "ì˜ìš•ì—†ëŠ”", "ê³µí—ˆí•œ"
            ],
            "í¥ë¶„": [
                # ì§ì ‘ì  ê°ì • í‘œí˜„
                "í¥ë¶„", "ì‹ ë‚˜", "ë‘ê·¼", "ì„¤ë ˜", "í™œê¸°", "ìƒë™ê°", "ì—ë„ˆì§€",
                "í™œë°œ", "ì—­ë™ì ", "í„ë–¡",
                # í–¥ìˆ˜ ê´€ë ¨ í™œê¸°
                "í†¡í†¡", "íŒ¡íŒ¡", "ìƒìƒí•œ", "í™œë ¥", "ì Šì€", "ë°œë„í•œ"
            ]
        }

        # ğŸŒ¸ í–¥ìˆ˜ ë„ë©”ì¸ íŠ¹í™” ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ
        self.perfume_context_keywords = {
            "positive_intensity": {
                "mild": ["ì€ì€", "ë¶€ë“œëŸ¬ìš´", "ê°€ë²¼ìš´", "ì‚´ì§", "ì€ê·¼"],
                "medium": ["ì ë‹¹", "ê´œì°®ì€", "ë¬´ë‚œí•œ", "ê· í˜•ì¡íŒ"],
                "strong": ["ì§„í•œ", "ê°•í•œ", "ê¹Šì€", "í’ë¶€í•œ", "ë†ì¶•ëœ"]
            },
            "positive_quality": [
                "ì¢‹ì•„", "ë§ˆìŒì— ë“¤ì–´", "í–¥ì´ ì¢‹ì•„", "ì˜ˆë»", "ê³ ê¸‰ìŠ¤ëŸ¬ì›Œ", "ìš°ì•„í•´",
                "ì„¸ë ¨ëœ", "ë§¤ë ¥ì ", "í¬ê·¼í•´", "ë”°ëœ»í•´", "ì‹œì›í•´", "ì²­ëŸ‰í•´", "ìƒí¼í•´",
                "ë‹¬ì½¤í•´", "ë¶€ë“œëŸ¬ì›Œ", "ì€ì€í•´", "ê¹”ë”í•´", "ê¹¨ë—í•´", "fresh", "nice"
            ],
            "negative_quality": [
                "ì‹«ì–´", "ë³„ë¡œ", "ì•„ì‰¬ì›Œ", "ì´ìƒí•´", "ì–´ìƒ‰í•´", "ë¶€ë‹´ìŠ¤ëŸ¬ì›Œ", "ê³¼í•´",
                "ë‹¨ì¡°ë¡œì›Œ", "ë°‹ë°‹í•´", "ë…í•´", "ìê·¹ì ", "ì—­ê²¨ìš´", "ë”ì°"
            ],
            "intensity_negative": [
                "ì§„í•´", "ì•½í•´", "ì„¸", "ê°•í•´", "ë…í•´", "ìê·¹ì ", "ì••ë„ì "
            ],
            "temporal_context": [
                "ì²˜ìŒ", "ì²«", "ë‚˜ì¤‘", "ì‹œê°„ì§€ë‚˜", "ì§€ì†", "ë³€í™”", "ë°”ë€Œ"
            ]
        }

        # ğŸ¯ ëª¨ë¸ ìƒíƒœ ê´€ë¦¬
        self.model_loaded = False
        self.model = None
        self.tokenizer = None
        self.model_version = "v2_dev"  # ê°œë°œ ì¤‘ ë²„ì „
        self.analysis_count = 0

        # ğŸ†• Google Drive ëª¨ë¸ ì„¤ì •
        self.google_drive_model_id = None  # ì„¤ì •ì—ì„œ ë¡œë“œ
        self.google_drive_enabled = False
        self.model_cache_dir = "./models/cache"
        self.model_hash_file = "./models/model_hash.txt"

        # ì„±ëŠ¥ í†µê³„
        self.performance_stats = {
            "total_analyses": 0,
            "successful_analyses": 0,
            "average_response_time": 0.0,
            "method_distribution": {"rule_based": 0, "ai_model": 0, "google_drive": 0},
            "confidence_distribution": {"high": 0, "medium": 0, "low": 0}
        }

        # ğŸ†• ì´ˆê¸°í™” ì‹œ Google Drive ì„¤ì • í™•ì¸
        self._load_google_drive_config()

        logger.info("âœ… ê°ì • ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"  - ì§€ì› ê°ì •: {list(self.emotion_to_tags.keys())}")
        logger.info(f"  - ëª¨ë¸ ë²„ì „: {self.model_version} (ê°œë°œ ì¤‘)")
        logger.info(f"  - ì´ í‚¤ì›Œë“œ: {sum(len(keywords) for keywords in self.emotion_keywords.values())}ê°œ")
        logger.info(f"  - Google Drive ì§€ì›: {'âœ…' if self.google_drive_enabled else 'âŒ'}")

    def _load_google_drive_config(self):
        """Google Drive ëª¨ë¸ ì„¤ì • ë¡œë“œ"""
        try:
            # í™˜ê²½ë³€ìˆ˜ì—ì„œ Google Drive ëª¨ë¸ ID ë¡œë“œ
            self.google_drive_model_id = os.getenv('GOOGLE_DRIVE_MODEL_ID')

            if self.google_drive_model_id:
                self.google_drive_enabled = True
                logger.info(f"ğŸŒ¤ï¸ Google Drive ëª¨ë¸ ID ì„¤ì •ë¨: {self.google_drive_model_id[:20]}...")

                # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
                os.makedirs(self.model_cache_dir, exist_ok=True)

            else:
                logger.info("ğŸ“‹ Google Drive ëª¨ë¸ ID ì—†ìŒ, ë¡œì»¬ ëª¨ë¸ë§Œ ì‚¬ìš©")

        except Exception as e:
            logger.warning(f"âš ï¸ Google Drive ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.google_drive_enabled = False

    def check_google_drive_model(self) -> bool:
        """Google Drive ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        if not self.google_drive_enabled:
            return False

        try:
            import gdown

            # ëª¨ë¸ íŒŒì¼ ì¡´ì¬ í™•ì¸
            cached_model_path = os.path.join(self.model_cache_dir, "emotion_model_gdrive.pkl")

            if os.path.exists(cached_model_path):
                # íŒŒì¼ í¬ê¸° í™•ì¸
                file_size = os.path.getsize(cached_model_path)
                if file_size > 1000:  # 1KB ì´ìƒ
                    logger.info(f"âœ… Google Drive ëª¨ë¸ ìºì‹œ ë°œê²¬: {file_size:,}B")
                    return True

            logger.info("ğŸ“¦ Google Drive ëª¨ë¸ ìºì‹œ ì—†ìŒ, ë‹¤ìš´ë¡œë“œ í•„ìš”")
            return False

        except ImportError:
            logger.warning("âš ï¸ gdown ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ, Google Drive ëª¨ë¸ ì‚¬ìš© ë¶ˆê°€")
            return False
        except Exception as e:
            logger.error(f"âŒ Google Drive ëª¨ë¸ í™•ì¸ ì‹¤íŒ¨: {e}")
            return False

    async def download_google_drive_model(self) -> bool:
        """Google Driveì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
        if not self.google_drive_enabled:
            logger.warning("âš ï¸ Google Drive ëª¨ë¸ì´ í™œì„±í™”ë˜ì§€ ì•ŠìŒ")
            return False

        try:
            import gdown

            logger.info("ğŸ“¥ Google Driveì—ì„œ ê°ì • ë¶„ì„ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘...")

            # ë‹¤ìš´ë¡œë“œ URL êµ¬ì„±
            download_url = f"https://drive.google.com/uc?id={self.google_drive_model_id}"
            output_path = os.path.join(self.model_cache_dir, "emotion_model_gdrive.pkl")

            # ê¸°ì¡´ íŒŒì¼ ì‚­ì œ (ìˆëŠ” ê²½ìš°)
            if os.path.exists(output_path):
                os.remove(output_path)
                logger.info("ğŸ—‘ï¸ ê¸°ì¡´ ìºì‹œ íŒŒì¼ ì‚­ì œ")

            # ë‹¤ìš´ë¡œë“œ ì‹¤í–‰
            start_time = time.time()
            gdown.download(download_url, output_path, quiet=False)
            download_time = time.time() - start_time

            # ë‹¤ìš´ë¡œë“œ ì„±ê³µ í™•ì¸
            if os.path.exists(output_path):
                file_size = os.path.getsize(output_path)
                logger.info(f"âœ… Google Drive ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
                logger.info(f"  - íŒŒì¼ í¬ê¸°: {file_size:,}B ({file_size / 1024:.1f}KB)")
                logger.info(f"  - ë‹¤ìš´ë¡œë“œ ì‹œê°„: {download_time:.2f}ì´ˆ")

                # íŒŒì¼ í•´ì‹œ ì €ì¥ (ëª¨ë¸ ë¬´ê²°ì„± í™•ì¸ìš©)
                model_hash = self._calculate_file_hash(output_path)
                with open(self.model_hash_file, 'w') as f:
                    f.write(model_hash)
                logger.info(f"ğŸ” ëª¨ë¸ í•´ì‹œ ì €ì¥: {model_hash[:8]}...")

                return True
            else:
                logger.error("âŒ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ í›„ íŒŒì¼ì´ ì—†ìŒ")
                return False

        except ImportError:
            logger.error("âŒ gdown ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
            return False
        except Exception as e:
            logger.error(f"âŒ Google Drive ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    def _calculate_file_hash(self, file_path: str) -> str:
        """íŒŒì¼ í•´ì‹œ ê³„ì‚° (ë¬´ê²°ì„± í™•ì¸ìš©)"""
        try:
            hash_sha256 = hashlib.sha256()
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_sha256.update(chunk)
            return hash_sha256.hexdigest()
        except Exception as e:
            logger.warning(f"âš ï¸ íŒŒì¼ í•´ì‹œ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return "unknown"

    async def load_google_drive_model(self):
        """Google Drive ëª¨ë¸ ë¡œë”©"""
        if not self.google_drive_enabled:
            return False

        try:
            cached_model_path = os.path.join(self.model_cache_dir, "emotion_model_gdrive.pkl")

            # ìºì‹œëœ ëª¨ë¸ í™•ì¸
            if not os.path.exists(cached_model_path):
                logger.info("ğŸ“¦ Google Drive ëª¨ë¸ ìºì‹œ ì—†ìŒ, ë‹¤ìš´ë¡œë“œ ì‹œë„")
                download_success = await self.download_google_drive_model()
                if not download_success:
                    return False

            # ëª¨ë¸ ë¡œë”©
            logger.info("ğŸ“¦ Google Drive ëª¨ë¸ ë¡œë”© ì‹œì‘...")

            import pickle
            with open(cached_model_path, 'rb') as f:
                model_data = pickle.load(f)

            # ëª¨ë¸ êµ¬ì¡° í™•ì¸
            if isinstance(model_data, dict):
                self.model = model_data.get('model')
                self.tokenizer = model_data.get('tokenizer')
                self.model_version = model_data.get('version', 'gdrive_v1')
                logger.info(f"âœ… Google Drive ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {self.model_version}")
            else:
                # ë‹¨ì¼ ëª¨ë¸ì¸ ê²½ìš°
                self.model = model_data
                self.model_version = "gdrive_v1"
                logger.info("âœ… Google Drive ë‹¨ì¼ ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

            self.model_loaded = True
            return True

        except Exception as e:
            logger.error(f"âŒ Google Drive ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            self.model_loaded = False
            return False

    async def analyze_emotion(self, text: str, use_model: bool = True) -> Dict[str, Any]:
        """
        í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„ (Google Drive ëª¨ë¸ ìš°ì„ , ì‹¤íŒ¨ ì‹œ ë£° ê¸°ë°˜ í´ë°±)

        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸
            use_model: AI ëª¨ë¸ ì‚¬ìš© ì—¬ë¶€ (Falseë©´ ë£° ê¸°ë°˜ë§Œ ì‚¬ìš©)

        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        start_time = time.time()
        self.analysis_count += 1

        logger.info(f"ğŸ­ ê°ì • ë¶„ì„ ì‹œì‘ (#{self.analysis_count})")
        logger.info(f"  - í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)}ì")
        logger.info(f"  - ëª¨ë¸ ì‚¬ìš©: {'âœ…' if use_model else 'âŒ'}")

        # ì…ë ¥ ê²€ì¦
        if not text or not text.strip():
            return self._create_empty_result()

        if len(text) > 2000:
            logger.warning(f"âš ï¸ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤: {len(text)}ì")
            return self._create_error_result("text_too_long", "í…ìŠ¤íŠ¸ê°€ 2000ìë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤.")

        try:
            # ğŸŒ¤ï¸ Google Drive ëª¨ë¸ ë¶„ì„ ì‹œë„ (ìš°ì„ ìˆœìœ„ 1)
            if use_model and self.google_drive_enabled:
                try:
                    if not self.model_loaded:
                        logger.info("ğŸŒ¤ï¸ Google Drive ëª¨ë¸ ë¡œë”© ì‹œë„...")
                        await self.load_google_drive_model()

                    if self.model_loaded:
                        logger.info(f"ğŸŒ¤ï¸ Google Drive ëª¨ë¸ v{self.model_version} ë¶„ì„ ì‹œì‘...")
                        gdrive_result = await self._analyze_with_google_drive_model(text)

                        if gdrive_result.get("success"):
                            response_time = time.time() - start_time
                            gdrive_result["method"] = "google_drive_model"
                            self._update_performance_stats(gdrive_result, response_time)

                            logger.info(f"âœ… Google Drive ëª¨ë¸ ë¶„ì„ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {response_time:.3f}ì´ˆ)")
                            logger.info(f"  - ê°ì •: {gdrive_result.get('primary_emotion')}")
                            logger.info(f"  - ì‹ ë¢°ë„: {gdrive_result.get('confidence', 0):.3f}")

                            return gdrive_result
                        else:
                            logger.warning("âš ï¸ Google Drive ëª¨ë¸ ë¶„ì„ ì‹¤íŒ¨, ë£° ê¸°ë°˜ìœ¼ë¡œ í´ë°±")

                except Exception as e:
                    logger.error(f"âŒ Google Drive ëª¨ë¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")

            # ğŸ¤– ë¡œì»¬ AI ëª¨ë¸ ë¶„ì„ ì‹œë„ (ìš°ì„ ìˆœìœ„ 2)
            if use_model and self._is_local_model_available():
                try:
                    logger.info(f"ğŸ¤– ë¡œì»¬ AI ëª¨ë¸ v{self.model_version} ë¶„ì„ ì‹œì‘...")
                    model_result = await self._analyze_with_local_model(text)

                    if model_result.get("success"):
                        response_time = time.time() - start_time
                        self._update_performance_stats(model_result, response_time)

                        logger.info(f"âœ… ë¡œì»¬ AI ëª¨ë¸ ë¶„ì„ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {response_time:.3f}ì´ˆ)")
                        logger.info(f"  - ê°ì •: {model_result.get('primary_emotion')}")
                        logger.info(f"  - ì‹ ë¢°ë„: {model_result.get('confidence', 0):.3f}")

                        return model_result
                    else:
                        logger.warning("âš ï¸ ë¡œì»¬ AI ëª¨ë¸ ë¶„ì„ ì‹¤íŒ¨, ë£° ê¸°ë°˜ìœ¼ë¡œ í´ë°±")

                except Exception as e:
                    logger.error(f"âŒ ë¡œì»¬ AI ëª¨ë¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")

            # ğŸ“‹ ë£° ê¸°ë°˜ ë¶„ì„ (í´ë°± ë˜ëŠ” ê¸°ë³¸)
            logger.info(f"ğŸ“‹ ë£° ê¸°ë°˜ ê°ì • ë¶„ì„ ì‹œì‘...")
            rule_result = await self._analyze_with_rules(text)

            response_time = time.time() - start_time
            self._update_performance_stats(rule_result, response_time)

            logger.info(f"âœ… ë£° ê¸°ë°˜ ë¶„ì„ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {response_time:.3f}ì´ˆ)")
            logger.info(f"  - ê°ì •: {rule_result.get('primary_emotion')}")
            logger.info(f"  - ì‹ ë¢°ë„: {rule_result.get('confidence', 0):.3f}")

            return rule_result

        except Exception as e:
            logger.error(f"âŒ ê°ì • ë¶„ì„ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            return self._create_error_result("analysis_exception", str(e))

    def _is_local_model_available(self) -> bool:
        """ë¡œì»¬ AI ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        # ğŸš§ í˜„ì¬ëŠ” ëª¨ë¸ ê°œë°œ ì¤‘ì´ë¯€ë¡œ False ë°˜í™˜
        # ëª¨ë¸ ì™„ì„± í›„ ì‹¤ì œ ë¡œë”© ë¡œì§ìœ¼ë¡œ êµì²´
        return False

    async def _analyze_with_google_drive_model(self, text: str) -> Dict[str, Any]:
        """Google Drive ëª¨ë¸ì„ ì‚¬ìš©í•œ ê°ì • ë¶„ì„"""
        try:
            if not self.model_loaded or not self.model:
                return {"success": False, "reason": "model_not_loaded"}

            # ğŸš§ ì‹¤ì œ Google Drive ëª¨ë¸ ì¶”ë¡  ë¡œì§ êµ¬í˜„ í•„ìš”
            # í˜„ì¬ëŠ” ê°œë°œ ì¤‘ ìƒíƒœë¡œ ì‹œë®¬ë ˆì´ì…˜
            await asyncio.sleep(0.1)  # ëª¨ë¸ ì¶”ë¡  ì‹œë®¬ë ˆì´ì…˜

            # ì„ì‹œ ì‘ë‹µ (ì‹¤ì œ ëª¨ë¸ êµ¬í˜„ í›„ êµì²´)
            return {
                "success": True,
                "method": "google_drive_model",
                "primary_emotion": "ê¸°ì¨",
                "confidence": 0.85,
                "emotion_tags": self.emotion_to_tags.get("ê¸°ì¨", ["#neutral"]),
                "analysis_details": {
                    "model_version": self.model_version,
                    "processing_method": "google_drive_ai"
                },
                "analyzed_at": datetime.now().isoformat(),
                "analyzer_version": "gdrive_v1"
            }

        except Exception as e:
            logger.error(f"âŒ Google Drive ëª¨ë¸ ì¶”ë¡  ì¤‘ ì˜¤ë¥˜: {e}")
            return {"success": False, "reason": "gdrive_model_error", "message": str(e)}

    async def _analyze_with_local_model(self, text: str) -> Dict[str, Any]:
        """ë¡œì»¬ AI ëª¨ë¸ì„ ì‚¬ìš©í•œ ê°ì • ë¶„ì„ (ê¸°ì¡´ ë¡œì§)"""
        try:
            # ğŸš§ ëª¨ë¸ ê°œë°œ ì™„ë£Œ í›„ êµ¬í˜„ ì˜ˆì •
            await asyncio.sleep(0.05)  # ëª¨ë¸ ì¶”ë¡  ì‹œë®¬ë ˆì´ì…˜

            return {
                "success": False,
                "reason": "model_under_development",
                "message": f"ë¡œì»¬ AI ëª¨ë¸ v{self.model_version}ëŠ” í˜„ì¬ ê°œë°œ ì¤‘ì…ë‹ˆë‹¤."
            }

        except Exception as e:
            logger.error(f"âŒ ë¡œì»¬ ëª¨ë¸ ì¶”ë¡  ì¤‘ ì˜¤ë¥˜: {e}")
            return {"success": False, "reason": "local_model_error", "message": str(e)}

    async def _analyze_with_rules(self, text: str) -> Dict[str, Any]:
        """ë£° ê¸°ë°˜ ê°ì • ë¶„ì„ (í–¥ìˆ˜ ë„ë©”ì¸ íŠ¹í™”) - ê¸°ì¡´ ë¡œì§ ìœ ì§€"""
        try:
            text_lower = text.lower().strip()
            text_words = text.split()

            # ğŸ” 1ë‹¨ê³„: ê¸°ë³¸ ê°ì • í‚¤ì›Œë“œ ë§¤ì¹­
            emotion_scores = {}
            keyword_matches = {}
            total_keywords_found = 0

            for emotion, keywords in self.emotion_keywords.items():
                score = 0
                matched_keywords = []

                for keyword in keywords:
                    # ì •í™•í•œ ë‹¨ì–´ ë§¤ì¹­ (ë¶€ë¶„ ë¬¸ìì—´ì´ ì•„ë‹Œ)
                    pattern = r'\b' + re.escape(keyword) + r'\b'
                    matches = len(re.findall(pattern, text_lower))

                    if matches > 0:
                        # í‚¤ì›Œë“œ ì¤‘ìš”ë„ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ ì ìš©
                        weight = self._get_keyword_weight(keyword, emotion)
                        score += matches * weight
                        matched_keywords.extend([keyword] * matches)
                        total_keywords_found += matches

                if score > 0:
                    emotion_scores[emotion] = score
                    keyword_matches[emotion] = matched_keywords

            # ğŸŒ¸ 2ë‹¨ê³„: í–¥ìˆ˜ ë„ë©”ì¸ ì»¨í…ìŠ¤íŠ¸ ë³´ì •
            context_boost = self._analyze_perfume_context(text_lower, text_words)

            # ì»¨í…ìŠ¤íŠ¸ ë³´ì • ì ìš©
            for emotion, boost in context_boost.items():
                if emotion in emotion_scores:
                    emotion_scores[emotion] += boost
                elif boost > 0.5:  # ì¶©ë¶„íˆ ê°•í•œ ì»¨í…ìŠ¤íŠ¸ ì‹ í˜¸
                    emotion_scores[emotion] = boost
                    keyword_matches[emotion] = ["context_boost"]

            # ğŸ“Š 3ë‹¨ê³„: ê²°ê³¼ ê³„ì‚° ë° ì •ê·œí™”
            if emotion_scores:
                # í…ìŠ¤íŠ¸ ê¸¸ì´ì— ë”°ë¥¸ ì •ê·œí™”
                normalization_factor = max(len(text_words), 1)
                normalized_scores = {}

                for emotion, score in emotion_scores.items():
                    # ì •ê·œí™” ì ìˆ˜ ê³„ì‚° (0.0 ~ 1.0)
                    normalized_score = min(score / normalization_factor * 1.5, 1.0)
                    normalized_scores[emotion] = normalized_score

                # ìµœê³  ì ìˆ˜ ê°ì • ì„ íƒ
                primary_emotion = max(normalized_scores.keys(),
                                      key=lambda x: normalized_scores[x])
                confidence = normalized_scores[primary_emotion]

                # ì‹ ë¢°ë„ ë³´ì • (í‚¤ì›Œë“œ ë§¤ì¹­ì´ ë§ì„ìˆ˜ë¡ ì‹ ë¢°ë„ ì¦ê°€)
                confidence_boost = min(total_keywords_found * 0.1, 0.3)
                final_confidence = min(confidence + confidence_boost, 1.0)

                # ê°ì • íƒœê·¸ ìƒì„±
                emotion_tags = self.emotion_to_tags.get(primary_emotion, ["#neutral"])

                return {
                    "success": True,
                    "method": "rule_based",
                    "primary_emotion": primary_emotion,
                    "confidence": round(final_confidence, 3),
                    "emotion_tags": emotion_tags,
                    "analysis_details": {
                        "raw_scores": emotion_scores,
                        "normalized_scores": normalized_scores,
                        "keyword_matches": keyword_matches,
                        "context_boost": context_boost,
                        "total_keywords": total_keywords_found,
                        "text_length": len(text),
                        "word_count": len(text_words)
                    },
                    "analyzed_at": datetime.now().isoformat(),
                    "analyzer_version": "rule_based_v1.2"
                }
            else:
                # í‚¤ì›Œë“œ ë§¤ì¹­ì´ ì—†ëŠ” ê²½ìš°
                return self._create_neutral_result("no_emotion_keywords")

        except Exception as e:
            logger.error(f"âŒ ë£° ê¸°ë°˜ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return self._create_error_result("rule_analysis_error", str(e))

    def _get_keyword_weight(self, keyword: str, emotion: str) -> float:
        """í‚¤ì›Œë“œ ì¤‘ìš”ë„ ê°€ì¤‘ì¹˜ ê³„ì‚° - ê¸°ì¡´ ë¡œì§ ìœ ì§€"""
        # ê°ì •ë³„ í•µì‹¬ í‚¤ì›Œë“œì— ë†’ì€ ê°€ì¤‘ì¹˜
        high_weight_keywords = {
            "ê¸°ì¨": ["ì¢‹ì•„", "í–‰ë³µ", "ì‚¬ë‘", "ì™„ë²½", "ìµœê³ "],
            "ë¶ˆì•ˆ": ["ë¶ˆì•ˆ", "ê±±ì •", "ë‘ë ¤ìš´", "ë¶€ë‹´"],
            "ë‹¹í™©": ["ë‹¹í™©", "ë†€ë€", "í˜¼ë€", "ì˜ì™¸"],
            "ë¶„ë…¸": ["í™”ê°€", "ì§œì¦", "ì‹«ì–´", "ìµœì•…"],
            "ìƒì²˜": ["ìƒì²˜", "ì•„í”ˆ", "ì‹¤ë§", "ê·¸ë¦¬ìš´"],
            "ìŠ¬í””": ["ìŠ¬í¼", "ëˆˆë¬¼", "ì™¸ë¡œìš´", "ì“¸ì“¸"],
            "ìš°ìš¸": ["ìš°ìš¸", "ì ˆë§", "ë¬´ê¸°ë ¥", "ì–´ë‘ "],
            "í¥ë¶„": ["í¥ë¶„", "ì‹ ë‚˜", "ì„¤ë ˜", "ì—ë„ˆì§€"]
        }

        if keyword in high_weight_keywords.get(emotion, []):
            return 1.5  # í•µì‹¬ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜
        elif len(keyword) >= 3:
            return 1.2  # ê¸´ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜
        else:
            return 1.0  # ê¸°ë³¸ ê°€ì¤‘ì¹˜

    def _analyze_perfume_context(self, text_lower: str, text_words: List[str]) -> Dict[str, float]:
        """í–¥ìˆ˜ ë„ë©”ì¸ íŠ¹í™” ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ - ê¸°ì¡´ ë¡œì§ ìœ ì§€"""
        context_boost = {}

        # ê¸ì •ì  í’ˆì§ˆ í‘œí˜„ ê°ì§€
        positive_quality_count = 0
        for keyword in self.perfume_context_keywords["positive_quality"]:
            positive_quality_count += text_lower.count(keyword)

        # ë¶€ì •ì  í’ˆì§ˆ í‘œí˜„ ê°ì§€
        negative_quality_count = 0
        for keyword in self.perfume_context_keywords["negative_quality"]:
            negative_quality_count += text_lower.count(keyword)

        # ê°•ë„ ê´€ë ¨ ë¶€ì • í‘œí˜„
        intensity_negative_count = 0
        for keyword in self.perfume_context_keywords["intensity_negative"]:
            intensity_negative_count += text_lower.count(keyword)

        # ğŸŒ¸ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ê°ì • ë³´ì •
        if positive_quality_count > 0:
            boost_strength = min(positive_quality_count * 0.8, 2.0)
            context_boost["ê¸°ì¨"] = boost_strength

            # ê¸ì •ì  í‘œí˜„ì´ ë§¤ìš° ê°•í•œ ê²½ìš° í¥ë¶„ë„ ì¶”ê°€
            if positive_quality_count >= 2:
                context_boost["í¥ë¶„"] = boost_strength * 0.6

        if negative_quality_count > 0:
            boost_strength = min(negative_quality_count * 0.7, 1.8)
            # ë¶€ì •ì  í‘œí˜„ì˜ ê°•ë„ì— ë”°ë¼ ë‹¤ë¥¸ ê°ì • ë°°ì •
            if negative_quality_count >= 2:
                context_boost["ë¶„ë…¸"] = boost_strength
            else:
                context_boost["ìƒì²˜"] = boost_strength * 0.8

        if intensity_negative_count > 0:
            boost_strength = min(intensity_negative_count * 0.6, 1.5)
            context_boost["ë¶ˆì•ˆ"] = boost_strength

        # ì‹œê°„ì  ë§¥ë½ ë¶„ì„ (ë³€í™” í‘œí˜„)
        temporal_keywords = ["ì²˜ìŒ", "ì²«", "ë‚˜ì¤‘", "ì‹œê°„ì§€ë‚˜", "ë³€í™”"]
        temporal_count = sum(text_lower.count(kw) for kw in temporal_keywords)
        if temporal_count > 0:
            context_boost["ë‹¹í™©"] = min(temporal_count * 0.5, 1.0)

        return context_boost

    def _create_empty_result(self) -> Dict[str, Any]:
        """ë¹ˆ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ê²°ê³¼ - ê¸°ì¡´ ë¡œì§ ìœ ì§€"""
        return {
            "success": True,
            "method": "validation",
            "primary_emotion": "ì¤‘ë¦½",
            "confidence": 0.0,
            "emotion_tags": ["#neutral"],
            "reason": "empty_text",
            "message": "ë¶„ì„í•  í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.",
            "analyzed_at": datetime.now().isoformat()
        }

    def _create_neutral_result(self, reason: str) -> Dict[str, Any]:
        """ì¤‘ë¦½ ê°ì • ê²°ê³¼ - ê¸°ì¡´ ë¡œì§ ìœ ì§€"""
        return {
            "success": True,
            "method": "rule_based",
            "primary_emotion": "ì¤‘ë¦½",
            "confidence": 0.3,
            "emotion_tags": ["#neutral", "#calm"],
            "reason": reason,
            "message": "ëª…í™•í•œ ê°ì • ì‹ í˜¸ê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
            "analyzed_at": datetime.now().isoformat()
        }

    def _create_error_result(self, error_type: str, message: str) -> Dict[str, Any]:
        """ì—ëŸ¬ ê²°ê³¼ - ê¸°ì¡´ ë¡œì§ ìœ ì§€"""
        return {
            "success": False,
            "error_type": error_type,
            "message": message,
            "primary_emotion": "ì˜¤ë¥˜",
            "confidence": 0.0,
            "emotion_tags": ["#error"],
            "analyzed_at": datetime.now().isoformat()
        }

    def _update_performance_stats(self, result: Dict[str, Any], response_time: float):
        """ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸ - Google Drive ë©”ì†Œë“œ ì¶”ê°€"""
        self.performance_stats["total_analyses"] += 1

        if result.get("success"):
            self.performance_stats["successful_analyses"] += 1

            # ë°©ë²•ë³„ ë¶„í¬ ì—…ë°ì´íŠ¸
            method = result.get("method", "unknown")
            if method in self.performance_stats["method_distribution"]:
                self.performance_stats["method_distribution"][method] += 1

            # ì‹ ë¢°ë„ ë¶„í¬ ì—…ë°ì´íŠ¸
            confidence = result.get("confidence", 0.0)
            if confidence >= 0.7:
                self.performance_stats["confidence_distribution"]["high"] += 1
            elif confidence >= 0.4:
                self.performance_stats["confidence_distribution"]["medium"] += 1
            else:
                self.performance_stats["confidence_distribution"]["low"] += 1

        # í‰ê·  ì‘ë‹µ ì‹œê°„ ì—…ë°ì´íŠ¸
        total_time = (self.performance_stats["average_response_time"] *
                      (self.performance_stats["total_analyses"] - 1) + response_time)
        self.performance_stats["average_response_time"] = total_time / self.performance_stats["total_analyses"]

    # ğŸ†• Google Drive ê´€ë ¨ ë©”ì†Œë“œë“¤
    async def force_download_google_drive_model(self) -> bool:
        """Google Drive ëª¨ë¸ ê°•ì œ ë‹¤ìš´ë¡œë“œ"""
        if not self.google_drive_enabled:
            return False

        # ê¸°ì¡´ ìºì‹œ ì‚­ì œ
        cached_model_path = os.path.join(self.model_cache_dir, "emotion_model_gdrive.pkl")
        if os.path.exists(cached_model_path):
            os.remove(cached_model_path)
            logger.info("ğŸ—‘ï¸ ê¸°ì¡´ Google Drive ëª¨ë¸ ìºì‹œ ì‚­ì œ")

        # ê°•ì œ ë‹¤ìš´ë¡œë“œ
        return await self.download_google_drive_model()

    def get_google_drive_model_info(self) -> Dict[str, Any]:
        """Google Drive ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        info = {
            "enabled": self.google_drive_enabled,
            "model_id": self.google_drive_model_id[:20] + "..." if self.google_drive_model_id else None,
            "cache_dir": self.model_cache_dir,
            "model_loaded": self.model_loaded,
            "model_version": self.model_version if self.model_loaded else None
        }

        # ìºì‹œëœ ëª¨ë¸ íŒŒì¼ ì •ë³´
        cached_model_path = os.path.join(self.model_cache_dir, "emotion_model_gdrive.pkl")
        if os.path.exists(cached_model_path):
            file_size = os.path.getsize(cached_model_path)
            file_mtime = datetime.fromtimestamp(os.path.getmtime(cached_model_path))
            info["cached_model"] = {
                "exists": True,
                "size_bytes": file_size,
                "size_kb": round(file_size / 1024, 1),
                "last_modified": file_mtime.isoformat()
            }
        else:
            info["cached_model"] = {"exists": False}

        return info

    # ê¸°ì¡´ ë©”ì†Œë“œë“¤ ìœ ì§€
    def get_supported_emotions(self) -> List[str]:
        """ì§€ì›í•˜ëŠ” ê°ì • ëª©ë¡ ë°˜í™˜"""
        return list(self.emotion_to_tags.keys())

    def get_emotion_tags(self, emotion: str) -> List[str]:
        """íŠ¹ì • ê°ì •ì˜ íƒœê·¸ ëª©ë¡ ë°˜í™˜"""
        return self.emotion_to_tags.get(emotion, ["#neutral"])

    def update_emotion_mapping(self, new_mapping: Dict[str, List[str]]):
        """ê°ì •-íƒœê·¸ ë§¤í•‘ ì—…ë°ì´íŠ¸ (ëª¨ë¸ ì—…ë°ì´íŠ¸ ì‹œ ì‚¬ìš©)"""
        logger.info(f"ğŸ”„ ê°ì • íƒœê·¸ ë§¤í•‘ ì—…ë°ì´íŠ¸...")
        old_count = len(self.emotion_to_tags)
        self.emotion_to_tags.update(new_mapping)
        new_count = len(self.emotion_to_tags)
        logger.info(f"âœ… ê°ì • íƒœê·¸ ë§¤í•‘ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {old_count} â†’ {new_count}ê°œ")

    def add_custom_keywords(self, emotion: str, keywords: List[str]):
        """íŠ¹ì • ê°ì •ì— ì»¤ìŠ¤í…€ í‚¤ì›Œë“œ ì¶”ê°€"""
        if emotion in self.emotion_keywords:
            old_count = len(self.emotion_keywords[emotion])
            self.emotion_keywords[emotion].extend(keywords)
            # ì¤‘ë³µ ì œê±°
            self.emotion_keywords[emotion] = list(set(self.emotion_keywords[emotion]))
            new_count = len(self.emotion_keywords[emotion])
            logger.info(f"ğŸ“ {emotion} í‚¤ì›Œë“œ ì—…ë°ì´íŠ¸: {old_count} â†’ {new_count}ê°œ")
        else:
            logger.warning(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” ê°ì •: {emotion}")

    async def load_model(self, model_path: str = "./models/emotion_model_v2.pkl"):
        """ë¡œì»¬ AI ëª¨ë¸ ë¡œë”© (ëª¨ë¸ ì™„ì„± í›„ êµ¬í˜„)"""
        try:
            logger.info(f"ğŸ¤– ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë”© ì‹œì‘...")
            logger.info(f"  - ëª¨ë¸ ê²½ë¡œ: {model_path}")

            # TODO: ì‹¤ì œ ëª¨ë¸ ë¡œë”© ë¡œì§
            if not os.path.exists(model_path):
                logger.warning(f"âš ï¸ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {model_path}")
                self.model_loaded = False
                return False

            # í˜„ì¬ëŠ” ê°œë°œ ì¤‘ì´ë¯€ë¡œ False
            logger.warning(f"âš ï¸ ëª¨ë¸ì´ ì•„ì§ ê°œë°œ ì¤‘ì…ë‹ˆë‹¤ (v{self.model_version})")
            self.model_loaded = False
            return False

        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            self.model_loaded = False
            return False

    def get_analysis_stats(self) -> Dict[str, Any]:
        """ë¶„ì„ ì‹œìŠ¤í…œ ìƒíƒœ ì •ë³´ - Google Drive ì •ë³´ ì¶”ê°€"""
        success_rate = 0.0
        if self.performance_stats["total_analyses"] > 0:
            success_rate = (self.performance_stats["successful_analyses"] /
                            self.performance_stats["total_analyses"] * 100)

        return {
            # ê¸°ë³¸ ì •ë³´
            "model_loaded": self.model_loaded,
            "model_version": self.model_version,
            "supported_emotions": len(self.emotion_to_tags),
            "total_keywords": sum(len(keywords) for keywords in self.emotion_keywords.values()),
            "analysis_methods": ["rule_based"] + (["google_drive"] if self.google_drive_enabled else []) + (
                ["ai_model"] if self.model_loaded else []),

            # ğŸ†• Google Drive ì •ë³´
            "google_drive": self.get_google_drive_model_info(),

            # ì„±ëŠ¥ í†µê³„
            "performance": {
                "total_analyses": self.performance_stats["total_analyses"],
                "successful_analyses": self.performance_stats["successful_analyses"],
                "success_rate": round(success_rate, 2),
                "average_response_time": round(self.performance_stats["average_response_time"], 3),
                "method_distribution": self.performance_stats["method_distribution"],
                "confidence_distribution": self.performance_stats["confidence_distribution"]
            },

            # ê°ì • ëª©ë¡
            "emotion_list": list(self.emotion_to_tags.keys()),
            "emotion_tags_count": {emotion: len(tags) for emotion, tags in self.emotion_to_tags.items()},

            # ì‹œìŠ¤í…œ ì •ë³´
            "system_info": {
                "max_text_length": 2000,
                "supported_languages": ["í•œêµ­ì–´"],
                "domain_specialization": "í–¥ìˆ˜_ë¦¬ë·°",
                "last_updated": datetime.now().isoformat()
            }
        }

    def get_performance_report(self) -> Dict[str, Any]:
        """ìƒì„¸ ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„± - Google Drive ì •ë³´ í¬í•¨"""
        stats = self.get_analysis_stats()

        report = {
            "report_generated_at": datetime.now().isoformat(),
            "system_overview": {
                "status": "operational" if stats["performance"]["success_rate"] > 80 else "degraded",
                "total_analyses": stats["performance"]["total_analyses"],
                "success_rate": stats["performance"]["success_rate"],
                "average_response_time": stats["performance"]["average_response_time"]
            },
            "performance_analysis": stats["performance"],
            "google_drive_status": stats["google_drive"],  # ğŸ†• ì¶”ê°€
            "recommendations": []
        }

        # ì„±ëŠ¥ ê°œì„  ê¶Œì¥ì‚¬í•­
        perf = stats["performance"]
        if perf["success_rate"] < 90:
            report["recommendations"].append("ì„±ê³µë¥ ì´ ë‚®ìŠµë‹ˆë‹¤. ì—ëŸ¬ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

        if perf["average_response_time"] > 2.0:
            report["recommendations"].append("ì‘ë‹µ ì‹œê°„ì´ ëŠë¦½ë‹ˆë‹¤. í‚¤ì›Œë“œ ìµœì í™”ë¥¼ ê³ ë ¤í•˜ì„¸ìš”.")

        confidence_dist = perf["confidence_distribution"]
        total_confident = confidence_dist["high"] + confidence_dist["medium"]
        if total_confident < confidence_dist["low"]:
            report["recommendations"].append("ì‹ ë¢°ë„ê°€ ë‚®ì€ ë¶„ì„ì´ ë§ìŠµë‹ˆë‹¤. í‚¤ì›Œë“œë¥¼ í™•ì¥í•˜ì„¸ìš”.")

        if not self.model_loaded and self.google_drive_enabled:
            report["recommendations"].append("Google Drive ëª¨ë¸ì„ ë¡œë”©í•˜ë©´ ì„±ëŠ¥ì´ ê°œì„ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        elif not self.google_drive_enabled:
            report["recommendations"].append("Google Drive ëª¨ë¸ì„ í™œì„±í™”í•˜ë©´ ë” ì •í™•í•œ ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")

        return report

    def reset_performance_stats(self):
        """ì„±ëŠ¥ í†µê³„ ë¦¬ì…‹"""
        logger.info("ğŸ”„ ì„±ëŠ¥ í†µê³„ ë¦¬ì…‹...")
        self.analysis_count = 0
        self.performance_stats = {
            "total_analyses": 0,
            "successful_analyses": 0,
            "average_response_time": 0.0,
            "method_distribution": {"rule_based": 0, "ai_model": 0, "google_drive": 0},
            "confidence_distribution": {"high": 0, "medium": 0, "low": 0}
        }
        logger.info("âœ… ì„±ëŠ¥ í†µê³„ ë¦¬ì…‹ ì™„ë£Œ")


# ğŸŒŸ ì „ì—­ ê°ì • ë¶„ì„ê¸° ì¸ìŠ¤í„´ìŠ¤
emotion_analyzer = EmotionAnalyzer()


# ğŸ§ª í…ŒìŠ¤íŠ¸ ë° ë””ë²„ê¹… í•¨ìˆ˜ë“¤
async def test_emotion_analyzer():
    """ê°ì • ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ - Google Drive í¬í•¨"""
    print("ğŸ§ª ê°ì • ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸ ì‹œì‘...\n")

    test_cases = [
        "ì´ í–¥ìˆ˜ ì •ë§ ì¢‹ì•„ìš”! ë‹¬ì½¤í•˜ê³  ìƒí¼í•´ì„œ ê¸°ë¶„ì´ ì¢‹ì•„ì ¸ìš”.",
        "í–¥ì´ ë„ˆë¬´ ì§„í•´ì„œ ë³„ë¡œì˜ˆìš”. ì¢€ ë¶€ë‹´ìŠ¤ëŸ½ë„¤ìš”.",
        "ì²˜ìŒ ë§¡ì•˜ì„ ë•Œ ë†€ëì–´ìš”. ì˜ˆìƒê³¼ ì™„ì „ ë‹¬ë¼ì„œ ë‹¹í™©ìŠ¤ëŸ¬ì› ì–´ìš”.",
        "ì´ í–¥ìˆ˜ë¥¼ ë§¡ìœ¼ë©´ ì˜›ë‚  ìƒê°ì´ ë‚˜ì„œ ìŠ¬í¼ì ¸ìš”.",
        "í–¥ìˆ˜ê°€ ë„ˆë¬´ ìê·¹ì ì´ì–´ì„œ í™”ê°€ ë‚˜ìš”. ìµœì•…ì´ì—ìš”.",
        "ìƒˆë¡œìš´ í–¥ìˆ˜ë¥¼ ë°œê²¬í•´ì„œ ë„ˆë¬´ ì‹ ë‚˜ìš”! ì—ë„ˆì§€ê°€ ë„˜ì³ìš”.",
        "í–¥ì´ ì€ì€í•˜ê³  ê¹”ë”í•´ì„œ ë§ˆìŒì— ë“¤ì–´ìš”.",
        ""  # ë¹ˆ í…ìŠ¤íŠ¸ í…ŒìŠ¤íŠ¸
    ]

    for i, text in enumerate(test_cases, 1):
        print(f"--- í…ŒìŠ¤íŠ¸ {i} ---")
        print(f"ì…ë ¥: {text if text else '(ë¹ˆ í…ìŠ¤íŠ¸)'}")

        result = await emotion_analyzer.analyze_emotion(text)

        print(f"ê²°ê³¼: {result['primary_emotion']} (ì‹ ë¢°ë„: {result['confidence']:.3f})")
        print(f"íƒœê·¸: {result['emotion_tags']}")
        print(f"ë°©ë²•: {result['method']}")
        print()

    # ì„±ëŠ¥ í†µê³„ ì¶œë ¥
    stats = emotion_analyzer.get_analysis_stats()
    print("ğŸ“Š ì„±ëŠ¥ í†µê³„:")
    print(f"  ì´ ë¶„ì„: {stats['performance']['total_analyses']}íšŒ")
    print(f"  ì„±ê³µë¥ : {stats['performance']['success_rate']}%")
    print(f"  í‰ê·  ì‘ë‹µì‹œê°„: {stats['performance']['average_response_time']:.3f}ì´ˆ")
    print(f"  Google Drive: {'ì‚¬ìš© ê°€ëŠ¥' if stats['google_drive']['enabled'] else 'ì‚¬ìš© ë¶ˆê°€'}")
    print()

    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    # ì§ì ‘ ì‹¤í–‰ ì‹œ í…ŒìŠ¤íŠ¸
    asyncio.run(test_emotion_analyzer())