
# ğŸ­ ê°ì • íƒœê¹… ëª¨ë¸ ë¡œë” - ì‹œí–¥ì¼ê¸° ìë™ íƒœê¹…ìš©

import os
import pickle
import requests
import logging
from pathlib import Path
import gdown
from typing import Optional, Tuple, Any, List
import numpy as np

logger = logging.getLogger(__name__)

# ğŸ”— Google Drive íŒŒì¼ ID ì„¤ì •
EMOTION_MODEL_FILE_ID = "1JYUJvKVb44p63ctWe3c1G_qmdcDr-Xix"  # ê°ì • íƒœê¹… ëª¨ë¸ íŒŒì¼ ID

# ë¡œì»¬ ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
BASE_DIR = Path(__file__).parent.parent
MODELS_DIR = BASE_DIR / "emotion_models"
EMOTION_MODEL_PATH = MODELS_DIR / "scent_emotion_model_v6.keras"
VECTORIZER_PATH = MODELS_DIR / "vectorizer.pkl"

# ğŸ­ ê°ì • íƒœê·¸ ë§¤í•‘ (ë°ì´í„°ì…‹ ê¸°ì¤€)
EMOTION_TAGS = {
    0: "ê¸°ì¨",
    1: "ë¶ˆì•ˆ",
    2: "ë‹¹í™©",
    3: "ë¶„ë…¸",
    4: "ìƒì²˜",
    5: "ìŠ¬í””",
    6: "ìš°ìš¸",
    7: "í¥ë¶„"
}

EMOTION_LABELS = {v: k for k, v in EMOTION_TAGS.items()}  # ì—­ë§¤í•‘

# ì „ì—­ ë³€ìˆ˜ë¡œ ëª¨ë¸ê³¼ ë²¡í„°ë¼ì´ì € ì €ì¥
_emotion_model = None
_vectorizer = None
_model_loaded = False


def create_models_directory():
    """ëª¨ë¸ ë””ë ‰í† ë¦¬ ìƒì„±"""
    MODELS_DIR.mkdir(parents=True, exist_ok=True)
    logger.info(f"ğŸ“ ê°ì • íƒœê¹… ëª¨ë¸ ë””ë ‰í† ë¦¬ í™•ì¸: {MODELS_DIR}")


def download_from_google_drive_gdown(file_id: str, output_path: str) -> bool:
    """gdown ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•œ Google Drive ë‹¤ìš´ë¡œë“œ"""
    try:
        logger.info(f"ğŸ“¥ Google Driveì—ì„œ ê°ì • íƒœê¹… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘ (gdown): {file_id}")

        url = f"https://drive.google.com/uc?id={file_id}"
        output = gdown.download(url, output_path, quiet=False)

        if output and os.path.exists(output_path):
            file_size = os.path.getsize(output_path)
            logger.info(f"âœ… ê°ì • íƒœê¹… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {output_path} ({file_size:,} bytes)")
            return True
        else:
            logger.error("âŒ ê°ì • íƒœê¹… ëª¨ë¸ gdown ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
            return False

    except Exception as e:
        logger.error(f"âŒ ê°ì • íƒœê¹… ëª¨ë¸ gdown ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
        return False


def download_from_google_drive_requests(file_id: str, destination: str) -> bool:
    """requestsë¥¼ ì‚¬ìš©í•œ Google Drive ë‹¤ìš´ë¡œë“œ (í´ë°±)"""
    try:
        logger.info(f"ğŸ“¥ Google Driveì—ì„œ ê°ì • íƒœê¹… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘ (requests): {file_id}")

        url = f"https://drive.google.com/uc?export=download&id={file_id}"
        session = requests.Session()
        response = session.get(url, stream=True)

        if response.status_code == 200:
            if "virus scan warning" in response.text.lower() or "download_warning" in response.text:
                logger.info("ğŸ” ëŒ€ìš©ëŸ‰ íŒŒì¼ í™•ì¸ í† í° ì²˜ë¦¬ ì¤‘...")
                confirm_url = f"https://drive.google.com/uc?export=download&confirm=t&id={file_id}"
                response = session.get(confirm_url, stream=True)

        if response.status_code == 200:
            os.makedirs(os.path.dirname(destination), exist_ok=True)

            total_size = 0
            with open(destination, 'wb') as f:
                for chunk in response.iter_content(chunk_size=32768):
                    if chunk:
                        f.write(chunk)
                        total_size += len(chunk)

                        if total_size % (10 * 1024 * 1024) == 0:
                            logger.info(f"ğŸ“¥ ë‹¤ìš´ë¡œë“œ ì§„í–‰: {total_size / 1024 / 1024:.1f}MB")

            file_size = os.path.getsize(destination)
            logger.info(f"âœ… ê°ì • íƒœê¹… ëª¨ë¸ requests ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {destination} ({file_size:,} bytes)")
            return True
        else:
            logger.error(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: HTTP {response.status_code}")
            return False

    except Exception as e:
        logger.error(f"âŒ ê°ì • íƒœê¹… ëª¨ë¸ requests ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
        return False


def download_model_file(file_id: str, output_path: str) -> bool:
    """ê°ì • íƒœê¹… ëª¨ë¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)"""
    if not file_id:
        logger.warning("âš ï¸ ê°ì • íƒœê¹… ëª¨ë¸ íŒŒì¼ IDê°€ ì œê³µë˜ì§€ ì•ŠìŒ")
        return False

    create_models_directory()

    if os.path.exists(output_path):
        file_size = os.path.getsize(output_path)
        if file_size > 1024:
            logger.info(f"âœ… ê°ì • íƒœê¹… ëª¨ë¸ì´ ì´ë¯¸ ì¡´ì¬: {output_path} ({file_size:,} bytes)")
            return True

    # ë°©ë²• 1: gdown ì‹œë„
    try:
        import gdown
        if download_from_google_drive_gdown(file_id, output_path):
            return True
    except ImportError:
        logger.warning("âš ï¸ gdown ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŒ. requestsë¡œ ì‹œë„í•©ë‹ˆë‹¤.")
    except Exception as e:
        logger.warning(f"âš ï¸ gdown ë°©ë²• ì‹¤íŒ¨: {e}")

    # ë°©ë²• 2: requests ì‹œë„
    if download_from_google_drive_requests(file_id, output_path):
        return True

    logger.error("âŒ ëª¨ë“  ê°ì • íƒœê¹… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë°©ë²• ì‹¤íŒ¨")
    return False


def verify_model_file(file_path: str) -> bool:
    """ê°ì • íƒœê¹… ëª¨ë¸ íŒŒì¼ ìœ íš¨ì„± ê²€ì¦"""
    try:
        if not os.path.exists(file_path):
            return False

        file_size = os.path.getsize(file_path)
        if file_size < 1024:
            logger.warning(f"âš ï¸ ê°ì • íƒœê¹… ëª¨ë¸ íŒŒì¼ í¬ê¸°ê°€ ë„ˆë¬´ ì‘ìŒ: {file_size} bytes")
            return False

        if file_path.endswith('.keras'):
            try:
                import tensorflow as tf
                with open(file_path, 'rb') as f:
                    header = f.read(100)
                    if b'keras' in header.lower() or b'tensorflow' in header.lower():
                        logger.info(f"âœ… ê°ì • íƒœê¹… Keras ëª¨ë¸ ê²€ì¦ ì™„ë£Œ: {file_size:,} bytes")
                        return True
            except Exception as e:
                logger.warning(f"âš ï¸ ê°ì • íƒœê¹… Keras ëª¨ë¸ ê²€ì¦ ì‹¤íŒ¨: {e}")

        elif file_path.endswith('.pkl'):
            try:
                with open(file_path, 'rb') as f:
                    header = f.read(10)
                    if header.startswith(b'\x80'):
                        logger.info(f"âœ… ê°ì • íƒœê¹… Pickle íŒŒì¼ ê²€ì¦ ì™„ë£Œ: {file_size:,} bytes")
                        return True
            except Exception as e:
                logger.warning(f"âš ï¸ ê°ì • íƒœê¹… Pickle íŒŒì¼ ê²€ì¦ ì‹¤íŒ¨: {e}")

        logger.info(f"âœ… ê°ì • íƒœê¹… ëª¨ë¸ ê¸°ë³¸ ê²€ì¦ ì™„ë£Œ: {file_size:,} bytes")
        return True

    except Exception as e:
        logger.error(f"âŒ ê°ì • íƒœê¹… ëª¨ë¸ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
        return False


def load_emotion_tagging_model():
    """ê°ì • íƒœê¹… ëª¨ë¸ ë¡œë”©"""
    global _emotion_model

    if _emotion_model is not None:
        return _emotion_model

    try:
        # ëª¨ë¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ (í•„ìš”í•œ ê²½ìš°)
        if not verify_model_file(str(EMOTION_MODEL_PATH)):
            logger.info("ğŸ“¥ ê°ì • íƒœê¹… ëª¨ë¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘...")
            if not download_model_file(EMOTION_MODEL_FILE_ID, str(EMOTION_MODEL_PATH)):
                raise Exception("ê°ì • íƒœê¹… ëª¨ë¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")

        # ëª¨ë¸ ë¡œë”©
        logger.info(f"ğŸ­ ê°ì • íƒœê¹… ëª¨ë¸ ë¡œë”© ì‹œì‘: {EMOTION_MODEL_PATH}")

        import tensorflow as tf

        # Keras ëª¨ë¸ ë¡œë”©
        _emotion_model = tf.keras.models.load_model(str(EMOTION_MODEL_PATH), compile=False)

        logger.info(f"âœ… ê°ì • íƒœê¹… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        logger.info(f"  - ì…ë ¥ shape: {_emotion_model.input_shape}")
        logger.info(f"  - ì¶œë ¥ shape: {_emotion_model.output_shape}")
        logger.info(f"  - ì˜ˆìƒ ì¶œë ¥: 8ê°œ ê°ì • íƒœê·¸ í™•ë¥ ")

        return _emotion_model

    except Exception as e:
        logger.error(f"âŒ ê°ì • íƒœê¹… ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        _emotion_model = None
        return None


def load_vectorizer():
    """ë²¡í„°ë¼ì´ì € ë¡œë”© (ë¡œì»¬ íŒŒì¼ ì‚¬ìš©)"""
    global _vectorizer

    if _vectorizer is not None:
        return _vectorizer

    try:
        if os.path.exists(VECTORIZER_PATH):
            logger.info(f"ğŸ“Š ê°ì • íƒœê¹… ë²¡í„°ë¼ì´ì € ë¡œë”© ì‹œì‘ (ë¡œì»¬ íŒŒì¼): {VECTORIZER_PATH}")

            file_size = os.path.getsize(VECTORIZER_PATH)
            logger.info(f"ğŸ“Š ë²¡í„°ë¼ì´ì € íŒŒì¼ í¬ê¸°: {file_size:,} bytes")

            with open(VECTORIZER_PATH, 'rb') as f:
                _vectorizer = pickle.load(f)

            logger.info("âœ… ê°ì • íƒœê¹… ë²¡í„°ë¼ì´ì € ë¡œë”© ì™„ë£Œ (ë¡œì»¬ íŒŒì¼)")
            return _vectorizer
        else:
            logger.warning("âš ï¸ ê°ì • íƒœê¹… ë²¡í„°ë¼ì´ì € íŒŒì¼ì´ ì—†ìŒ - ëª¨ë¸ ë‚´ì¥ ì „ì²˜ë¦¬ ì‚¬ìš©")
            logger.warning(f"  ì˜ˆìƒ ê²½ë¡œ: {VECTORIZER_PATH}")
            return None

    except Exception as e:
        logger.error(f"âŒ ê°ì • íƒœê¹… ë²¡í„°ë¼ì´ì € ë¡œë”© ì‹¤íŒ¨: {e}")
        _vectorizer = None
        return None


def preprocess_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (ê°„ë‹¨í•œ ì •ì œ)"""
    if not text:
        return ""

    # ê¸°ë³¸ ì •ì œ
    text = text.strip()
    # ì¶”ê°€ì ì¸ ì „ì²˜ë¦¬ê°€ í•„ìš”í•˜ë©´ ì—¬ê¸°ì— ì¶”ê°€

    return text


def predict_emotion_tags(text: str) -> dict:
    """
    ì‹œí–¥ì¼ê¸° í…ìŠ¤íŠ¸ì—ì„œ ê°ì • íƒœê·¸ ì˜ˆì¸¡

    Args:
        text: ì‹œí–¥ì¼ê¸° í…ìŠ¤íŠ¸

    Returns:
        {
            "success": bool,
            "predicted_emotion": str,
            "confidence": float,
            "all_probabilities": dict,
            "method": str
        }
    """
    try:
        logger.info(f"ğŸ­ ê°ì • íƒœê¹… ì˜ˆì¸¡ ì‹œì‘: '{text[:50]}...'")

        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        processed_text = preprocess_text(text)
        if not processed_text:
            return {
                "success": False,
                "error": "ë¹ˆ í…ìŠ¤íŠ¸",
                "method": "validation"
            }

        # ëª¨ë¸ê³¼ ë²¡í„°ë¼ì´ì € ë¡œë”©
        model = load_emotion_tagging_model()
        vectorizer = load_vectorizer()

        if model is None:
            # ëª¨ë¸ì´ ì—†ìœ¼ë©´ ë£° ê¸°ë°˜ í´ë°±
            return _rule_based_emotion_tagging(processed_text)

        if vectorizer is None:
            logger.warning("âš ï¸ ë²¡í„°ë¼ì´ì €ê°€ ì—†ì–´ ê°„ë‹¨í•œ ì „ì²˜ë¦¬ ì‚¬ìš©")
            # ê°„ë‹¨í•œ ì „ì²˜ë¦¬ ë¡œì§ (ì‹¤ì œ ëª¨ë¸ì— ë”°ë¼ ì¡°ì • í•„ìš”)
            return _simple_model_prediction(model, processed_text)

        # ë²¡í„°ë¼ì´ì €ë¥¼ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ë³€í™˜
        try:
            text_vector = vectorizer.transform([processed_text])

            # ëª¨ë¸ ì˜ˆì¸¡
            predictions = model.predict(text_vector, verbose=0)
            probabilities = predictions[0]  # ì²« ë²ˆì§¸ ìƒ˜í”Œì˜ í™•ë¥ ë“¤

            # ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ ê°ì • íƒœê·¸
            predicted_label = int(np.argmax(probabilities))
            predicted_emotion = EMOTION_TAGS[predicted_label]
            confidence = float(probabilities[predicted_label])

            # ëª¨ë“  ê°ì •ë³„ í™•ë¥ 
            all_probabilities = {
                EMOTION_TAGS[i]: float(prob)
                for i, prob in enumerate(probabilities)
            }

            logger.info(f"âœ… ê°ì • íƒœê¹… ì˜ˆì¸¡ ì™„ë£Œ: {predicted_emotion} (ì‹ ë¢°ë„: {confidence:.3f})")

            return {
                "success": True,
                "predicted_emotion": predicted_emotion,
                "predicted_label": predicted_label,
                "confidence": confidence,
                "all_probabilities": all_probabilities,
                "method": "AI ëª¨ë¸",
                "processed_text": processed_text
            }

        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜: {e}")
            return _rule_based_emotion_tagging(processed_text)

    except Exception as e:
        logger.error(f"âŒ ê°ì • íƒœê¹… ì˜ˆì¸¡ ì¤‘ ì˜ˆì™¸: {e}")
        return {
            "success": False,
            "error": str(e),
            "method": "exception"
        }


def _simple_model_prediction(model, text: str) -> dict:
    """ë²¡í„°ë¼ì´ì € ì—†ì´ ê°„ë‹¨í•œ ëª¨ë¸ ì˜ˆì¸¡ (ì‹¤í—˜ì )"""
    try:
        # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ì¸ì½”ë”© (ì‹¤ì œ ëª¨ë¸ ì…ë ¥ì— ë§ê²Œ ì¡°ì • í•„ìš”)
        # ì—¬ê¸°ì„œëŠ” í´ë°±ìœ¼ë¡œ ë£° ê¸°ë°˜ ì‚¬ìš©
        logger.warning("âš ï¸ ë²¡í„°ë¼ì´ì € ì—†ì´ëŠ” ëª¨ë¸ ì‚¬ìš© ë¶ˆê°€, ë£° ê¸°ë°˜ìœ¼ë¡œ ì „í™˜")
        return _rule_based_emotion_tagging(text)

    except Exception as e:
        logger.error(f"âŒ ê°„ë‹¨í•œ ëª¨ë¸ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
        return _rule_based_emotion_tagging(text)


def _rule_based_emotion_tagging(text: str) -> dict:
    """ë£° ê¸°ë°˜ ê°ì • íƒœê¹… (í´ë°±)"""
    try:
        logger.info(f"ğŸ“‹ ë£° ê¸°ë°˜ ê°ì • íƒœê¹… ì‹œì‘: '{text[:30]}...'")

        text_lower = text.lower()

        # ê°ì •ë³„ í‚¤ì›Œë“œ ë§¤ì¹­
        emotion_scores = {}

        # 8ê°œ ê°ì •ë³„ í‚¤ì›Œë“œ (ë°ì´í„°ì…‹ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±)
        emotion_keywords = {
            "ê¸°ì¨": ["ì¢‹", "í–‰ë³µ", "ê¸°ë»", "ì¦ê±°", "ë§Œì¡±", "ì™„ë²½", "ì‚¬ë‘", "ë”°ëœ»", "í¬ê·¼", "ë°", "ìƒì¾Œ", "ë‹¬ì½¤"],
            "ë¶ˆì•ˆ": ["ë¶ˆì•ˆ", "ê±±ì •", "ê¸´ì¥", "ë–¨", "ë‘ë ¤", "ë¬´ì„œ", "ì¡°ë§ˆì¡°ë§ˆ", "ì–´ìƒ‰", "ë¶€ë‹´", "ìŠ¤íŠ¸ë ˆìŠ¤"],
            "ë‹¹í™©": ["ë‹¹í™©", "ë†€", "í˜¼ë€", "ì–´ë¦¬ë‘¥ì ˆ", "ë©", "ëª¨ë¥´ê² ", "í—·ê°ˆ", "ì´ìƒ", "ì˜ì™¸", "ì‹ ê¸°"],
            "ë¶„ë…¸": ["í™”", "ì§œì¦", "ì—´ë°›", "ë¶„ë…¸", "ì‹«", "ë³„ë¡œ", "ìµœì•…", "ìê·¹ì ", "ê°•ë ¬", "ê³¼í•´"],
            "ìƒì²˜": ["ìƒì²˜", "ì•„í”ˆ", "ì„œìš´", "ì‹¤ë§", "ì•„ì‰¬", "í˜ë“ ", "ì„­ì„­", "ê·¸ë¦¬ìš´", "ì• í‹‹"],
            "ìŠ¬í””": ["ìŠ¬", "ëˆˆë¬¼", "ì• ì ˆ", "ì²˜ëŸ‰", "ì™¸ë¡œ", "ì“¸ì“¸", "ë¨¹ë¨¹", "ì°¡", "ìš¸ì»¥", "ì§„í•œ"],
            "ìš°ìš¸": ["ìš°ìš¸", "ë‹µë‹µ", "ë¬´ê¸°ë ¥", "ì ˆë§", "ì–´ë‘ ", "ì¹¨ìš¸", "ë©œë‘ì½œë¦¬", "ë¸”ë£¨", "ë§‰ë§‰"],
            "í¥ë¶„": ["í¥ë¶„", "ì‹ ë‚˜", "ë‘ê·¼", "ì„¤ë ˜", "í™œê¸°", "ìƒë™ê°", "ì—ë„ˆì§€", "í™œë°œ", "í†¡í†¡", "ìƒìƒ"]
        }

        for emotion, keywords in emotion_keywords.items():
            score = 0
            for keyword in keywords:
                if keyword in text_lower:
                    score += text_lower.count(keyword)
            emotion_scores[emotion] = score

        # ìµœê³  ì ìˆ˜ ê°ì • ì„ íƒ
        if any(score > 0 for score in emotion_scores.values()):
            predicted_emotion = max(emotion_scores.keys(), key=lambda x: emotion_scores[x])
            max_score = emotion_scores[predicted_emotion]
            confidence = min(max_score / len(text.split()) * 2, 1.0)  # ì •ê·œí™”
        else:
            # ê¸°ë³¸ê°’: ì¤‘ë¦½ì  ê°ì •
            predicted_emotion = "ê¸°ì¨"  # ê¸°ë³¸ê°’
            confidence = 0.3

        predicted_label = EMOTION_LABELS[predicted_emotion]

        # ëª¨ë“  ê°ì •ë³„ ì •ê·œí™”ëœ í™•ë¥ 
        total_score = sum(emotion_scores.values()) or 1
        all_probabilities = {
            emotion: score / total_score
            for emotion, score in emotion_scores.items()
        }

        logger.info(f"âœ… ë£° ê¸°ë°˜ ê°ì • íƒœê¹… ì™„ë£Œ: {predicted_emotion} (ì‹ ë¢°ë„: {confidence:.3f})")

        return {
            "success": True,
            "predicted_emotion": predicted_emotion,
            "predicted_label": predicted_label,
            "confidence": confidence,
            "all_probabilities": all_probabilities,
            "method": "ë£° ê¸°ë°˜",
            "keyword_scores": emotion_scores
        }

    except Exception as e:
        logger.error(f"âŒ ë£° ê¸°ë°˜ ê°ì • íƒœê¹… ì‹¤íŒ¨: {e}")
        return {
            "success": False,
            "error": str(e),
            "method": "rule_based_error"
        }


def initialize_emotion_tagging_models() -> Tuple[bool, str]:
    """ê°ì • íƒœê¹… ëª¨ë¸ ì´ˆê¸°í™”"""
    global _model_loaded

    try:
        logger.info("ğŸ­ ê°ì • íƒœê¹… ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘...")

        # ëª¨ë¸ íŒŒì¼ í™•ì¸ (ê°ì • ëª¨ë¸ì€ ë‹¤ìš´ë¡œë“œ, ë²¡í„°ë¼ì´ì €ëŠ” ë¡œì»¬)
        model_available = verify_model_file(str(EMOTION_MODEL_PATH))
        vectorizer_available = os.path.exists(VECTORIZER_PATH)

        logger.info(f"ğŸ“‹ ê°ì • íƒœê¹… ëª¨ë¸ íŒŒì¼ ìƒíƒœ:")
        logger.info(f"  - ê°ì • íƒœê¹… ëª¨ë¸: {'âœ… ì¡´ì¬' if model_available else 'âŒ ë‹¤ìš´ë¡œë“œ í•„ìš”'}")
        logger.info(f"  - ë²¡í„°ë¼ì´ì €: {'âœ… ì¡´ì¬ (ë¡œì»¬)' if vectorizer_available else 'âŒ ì—†ìŒ (ë¡œì»¬)'}")

        # ê°ì • ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (í•„ìš”í•œ ê²½ìš°)
        if not model_available:
            logger.info("ğŸ“¥ ê°ì • íƒœê¹… ëª¨ë¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œë„...")
            if EMOTION_MODEL_FILE_ID:
                if download_model_file(EMOTION_MODEL_FILE_ID, str(EMOTION_MODEL_PATH)):
                    model_available = True
                    logger.info("âœ… ê°ì • íƒœê¹… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
                else:
                    return False, "ê°ì • íƒœê¹… ëª¨ë¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨"
            else:
                return False, "ê°ì • íƒœê¹… ëª¨ë¸ íŒŒì¼ IDê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ"

        # ë²¡í„°ë¼ì´ì € í™•ì¸ (ë¡œì»¬ íŒŒì¼)
        if not vectorizer_available:
            logger.warning(f"âš ï¸ ê°ì • íƒœê¹… ë²¡í„°ë¼ì´ì € íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {VECTORIZER_PATH}")
            logger.warning("  ëª¨ë¸ ë‚´ì¥ ì „ì²˜ë¦¬ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ ë£° ê¸°ë°˜ìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤")

        # ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸
        if model_available:
            model = load_emotion_tagging_model()
            vectorizer = load_vectorizer()  # ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰

            if model is not None:
                logger.info("âœ… ê°ì • íƒœê¹… ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
                logger.info(f"  - ê°ì • íƒœê¹… ëª¨ë¸: âœ… ë¡œë“œë¨")
                logger.info(f"  - ë²¡í„°ë¼ì´ì €: {'âœ… ë¡œë“œë¨' if vectorizer is not None else 'âŒ ì—†ìŒ (ë£° ê¸°ë°˜ ì‚¬ìš©)'}")
                logger.info(f"  - ì§€ì› ê°ì •: {list(EMOTION_TAGS.values())}")
                _model_loaded = True
                return True, "ê°ì • íƒœê¹… ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ"
            else:
                return False, "ê°ì • íƒœê¹… ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨"
        else:
            return False, "ê°ì • íƒœê¹… ëª¨ë¸ íŒŒì¼ ì¤€ë¹„ ì‹¤íŒ¨"

    except Exception as e:
        logger.error(f"âŒ ê°ì • íƒœê¹… ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return False, f"ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {str(e)}"


def is_model_available() -> bool:
    """ê°ì • íƒœê¹… ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
    return _model_loaded and _emotion_model is not None


def get_model_status() -> dict:
    """ê°ì • íƒœê¹… ëª¨ë¸ ìƒíƒœ ì •ë³´ ë°˜í™˜"""
    return {
        "model_loaded": _model_loaded,
        "emotion_model_available": _emotion_model is not None,
        "vectorizer_available": _vectorizer is not None,
        "emotion_model_path": str(EMOTION_MODEL_PATH),
        "vectorizer_path": str(VECTORIZER_PATH),
        "emotion_model_exists": os.path.exists(EMOTION_MODEL_PATH),
        "vectorizer_exists": os.path.exists(VECTORIZER_PATH),
        "emotion_model_file_id": EMOTION_MODEL_FILE_ID,
        "vectorizer_source": "ë¡œì»¬ íŒŒì¼ (Git í¬í•¨)",
        "emotion_model_source": "Google Drive ë‹¤ìš´ë¡œë“œ",
        "supported_emotions": list(EMOTION_TAGS.values()),
        "total_emotion_count": len(EMOTION_TAGS),
        "emotion_label_mapping": EMOTION_LABELS
    }


def get_supported_emotions() -> List[str]:
    """ì§€ì›í•˜ëŠ” ê°ì • íƒœê·¸ ëª©ë¡ ë°˜í™˜"""
    return list(EMOTION_TAGS.values())


def get_emotion_label_mapping() -> dict:
    """ê°ì •-ë¼ë²¨ ë§¤í•‘ ë°˜í™˜"""
    return EMOTION_LABELS.copy()


# ğŸ§ª í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_emotion_tagging(test_texts: List[str] = None):
    """ê°ì • íƒœê¹… í…ŒìŠ¤íŠ¸"""
    if test_texts is None:
        test_texts = [
            "í–¥ê¸°ë¥¼ ë§¡ìœ¼ë‹ˆ ë‚´ ì•ˆì— ë”°ëœ»í•¨ì´ ë²ˆì¡Œë‹¤.",
            "ë‚¯ì„  ê³µê°„ì—ì„œ ì´ í–¥ì€ ì§€ë‚˜ì¹˜ê²Œ ë„ë“œë¼ì¡Œë‹¤.",
            "ë‚´ê°€ ë¿Œë ¸ì§€ë§Œ ë‚´ê°€ ë‹¹í™©í•œ í–¥ì´ì—ˆë‹¤.",
            "ë‚˜ë¥¼ ìœ„í•œ ê³µê°„ì´ í–¥ í•˜ë‚˜ë¡œ ë‚¯ì„¤ì–´ì¡Œë‹¤.",
            "ê·¸ë‚ ì˜ ë§ì´ í–¥ì²˜ëŸ¼ ë‹¤ì‹œ í¼ì ¸ë‚˜ê°”ë‹¤."
        ]

    logger.info("ğŸ§ª ê°ì • íƒœê¹… í…ŒìŠ¤íŠ¸ ì‹œì‘...")

    for i, text in enumerate(test_texts, 1):
        logger.info(f"\n--- í…ŒìŠ¤íŠ¸ {i} ---")
        logger.info(f"ì…ë ¥: {text}")

        result = predict_emotion_tags(text)

        if result.get("success"):
            logger.info(f"ì˜ˆì¸¡ ê°ì •: {result['predicted_emotion']}")
            logger.info(f"ì‹ ë¢°ë„: {result['confidence']:.3f}")
            logger.info(f"ë°©ë²•: {result['method']}")
        else:
            logger.error(f"ì˜ˆì¸¡ ì‹¤íŒ¨: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")

    logger.info("âœ… ê°ì • íƒœê¹… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    # ì§ì ‘ ì‹¤í–‰ ì‹œ í…ŒìŠ¤íŠ¸
    test_emotion_tagging()