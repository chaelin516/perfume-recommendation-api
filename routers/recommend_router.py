import os
import pickle
import logging
import random
import sys
import numpy as np
from datetime import datetime
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import List, Literal, Optional, Dict, Any
import pandas as pd
from sklearn.preprocessing import OneHotEncoder

# â”€â”€â”€ ë¡œê±° ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger("recommend_router")

# â”€â”€â”€ 1. perfume_final_dataset.csv ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DATA_PATH = os.path.join(os.path.dirname(__file__), "../data/perfume_final_dataset.csv")
try:
    df = pd.read_csv(DATA_PATH)
    df.fillna("", inplace=True)
    logger.info(f"âœ… Perfume dataset loaded: {df.shape[0]} rows")
    logger.info(f"ğŸ“‹ Available columns: {list(df.columns)}")

    # âœ… ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    required_columns = ['name', 'brand', 'image_url', 'notes']
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        logger.error(f"âŒ Missing required columns: {missing_columns}")
        raise RuntimeError(f"Missing required columns: {missing_columns}")

    # âœ… emotion ê´€ë ¨ ì»¬ëŸ¼ í™•ì¸
    if 'desired_impression' in df.columns:
        logger.info("âœ… Using 'desired_impression' column for emotion data")
    if 'emotion_cluster' in df.columns:
        logger.info("âœ… Using 'emotion_cluster' column for cluster data")
        # emotion_cluster ì»¬ëŸ¼ ì •ìˆ˜í˜•ìœ¼ë¡œ ë³€í™˜
        df['emotion_cluster'] = pd.to_numeric(df['emotion_cluster'], errors='coerce').fillna(0).astype(int)
        logger.info(f"ğŸ“Š Emotion clusters: {sorted(df['emotion_cluster'].unique())}")
    else:
        logger.warning("âš ï¸ No emotion_cluster column found")

    # ğŸ“Š ë°ì´í„° ìƒ˜í”Œ ë¡œê·¸
    if len(df) > 0:
        sample_row = df.iloc[0]
        logger.info(f"ğŸ“ Sample data: {sample_row['name']} by {sample_row['brand']}")

except Exception as e:
    logger.error(f"âŒ perfume_final_dataset.csv ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
    raise RuntimeError(f"perfume_final_dataset.csv ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")

# â”€â”€â”€ 2. ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE_DIR = os.path.dirname(__file__)
MODEL_PATH = os.path.join(BASE_DIR, "../models/final_model.keras")
ENCODER_PATH = os.path.join(BASE_DIR, "../models/encoder.pkl")

# â”€â”€â”€ 3. ì „ì—­ ë³€ìˆ˜ ë° ìƒíƒœ ê´€ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_model = None
_encoder = None
_model_available = False
_model_availability_reason = "ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ"
_fallback_encoder = None

# â”€â”€â”€ 4. ê°ì • í´ëŸ¬ìŠ¤í„° ë§¤í•‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
EMOTION_CLUSTER_MAP = {
    0: "ì°¨ë¶„í•œ, í¸ì•ˆí•œ",
    1: "ìì‹ ê°, ì‹ ì„ í•¨",
    2: "ìš°ì•„í•¨, ì¹œê·¼í•¨",
    3: "ìˆœìˆ˜í•¨, ì¹œê·¼í•¨",
    4: "ì‹ ë¹„ë¡œìš´, ë§¤ë ¥ì ",
    5: "í™œê¸°ì°¬, ì—ë„ˆì§€"
}


# â”€â”€â”€ 5. í–¥ìƒëœ ëª¨ë¸ ê°€ìš©ì„± í™•ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def check_model_availability():
    """ëª¨ë¸ íŒŒì¼ë“¤ì˜ ê°€ìš©ì„±ì„ ìƒì„¸íˆ í™•ì¸í•©ë‹ˆë‹¤."""
    global _model_available, _model_availability_reason

    logger.info("ğŸ” ëª¨ë¸ íŒŒì¼ ìƒì„¸ ì§„ë‹¨ ì‹œì‘...")
    logger.info(f"ğŸ“‚ í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}")
    logger.info(f"ğŸ“‚ ë¼ìš°í„° íŒŒì¼ ìœ„ì¹˜: {BASE_DIR}")

    # ìƒìœ„ ë””ë ‰í† ë¦¬ êµ¬ì¡° í™•ì¸
    parent_dir = os.path.dirname(BASE_DIR)
    logger.info(f"ğŸ“‚ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬: {parent_dir}")

    # models ë””ë ‰í† ë¦¬ í™•ì¸
    models_dir = os.path.join(parent_dir, "models")
    logger.info(f"ğŸ“‚ ëª¨ë¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ: {models_dir}")
    logger.info(f"ğŸ“‚ ëª¨ë¸ ë””ë ‰í† ë¦¬ ì¡´ì¬ ì—¬ë¶€: {os.path.exists(models_dir)}")

    if os.path.exists(models_dir):
        try:
            files_in_models = os.listdir(models_dir)
            logger.info(f"ğŸ“‹ models ë””ë ‰í† ë¦¬ ë‚´ íŒŒì¼ ëª©ë¡: {files_in_models}")
        except PermissionError:
            logger.error(f"âŒ models ë””ë ‰í† ë¦¬ ì½ê¸° ê¶Œí•œ ì—†ìŒ: {models_dir}")
        except Exception as e:
            logger.error(f"âŒ models ë””ë ‰í† ë¦¬ ì½ê¸° ì˜¤ë¥˜: {e}")
    else:
        logger.warning(f"âš ï¸ models ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {models_dir}")

    try:
        # === 1. ëª¨ë¸ íŒŒì¼ ìƒì„¸ í™•ì¸ ===
        logger.info(f"ğŸ” ëª¨ë¸ íŒŒì¼ í™•ì¸: {MODEL_PATH}")

        model_exists = os.path.exists(MODEL_PATH)
        logger.info(f"ğŸ“„ ëª¨ë¸ íŒŒì¼ ì¡´ì¬: {'âœ…' if model_exists else 'âŒ'}")

        if model_exists:
            try:
                model_size = os.path.getsize(MODEL_PATH)
                model_size_mb = model_size / (1024 * 1024)
                logger.info(f"ğŸ“ ëª¨ë¸ íŒŒì¼ í¬ê¸°: {model_size:,} bytes ({model_size_mb:.2f} MB)")

                # íŒŒì¼ ê¶Œí•œ í™•ì¸
                readable = os.access(MODEL_PATH, os.R_OK)
                logger.info(f"ğŸ” ëª¨ë¸ íŒŒì¼ ì½ê¸° ê¶Œí•œ: {'âœ…' if readable else 'âŒ'}")

                # íŒŒì¼ í¬ê¸° ìœ íš¨ì„±
                MIN_MODEL_SIZE = 100000  # 100KB
                model_size_valid = model_size > MIN_MODEL_SIZE
                logger.info(f"ğŸ“ ëª¨ë¸ íŒŒì¼ í¬ê¸° ìœ íš¨ì„± (>{MIN_MODEL_SIZE:,}B): {'âœ…' if model_size_valid else 'âŒ'}")

                if not model_size_valid:
                    logger.warning(f"âš ï¸ ëª¨ë¸ íŒŒì¼ì´ ë„ˆë¬´ ì‘ìŠµë‹ˆë‹¤. ì˜ˆìƒ ìµœì†Œ í¬ê¸°: {MIN_MODEL_SIZE:,}B, ì‹¤ì œ: {model_size:,}B")

                if not readable:
                    logger.error(f"âŒ ëª¨ë¸ íŒŒì¼ ì½ê¸° ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤: {MODEL_PATH}")

            except OSError as e:
                logger.error(f"âŒ ëª¨ë¸ íŒŒì¼ ì •ë³´ ì½ê¸° ì‹¤íŒ¨: {e}")
                model_size_valid = False
                readable = False
        else:
            logger.warning(f"âš ï¸ ëª¨ë¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {MODEL_PATH}")
            model_size_valid = False
            readable = False

        # === 2. ì¸ì½”ë” íŒŒì¼ ìƒì„¸ í™•ì¸ ===
        logger.info(f"ğŸ” ì¸ì½”ë” íŒŒì¼ í™•ì¸: {ENCODER_PATH}")

        encoder_exists = os.path.exists(ENCODER_PATH)
        logger.info(f"ğŸ“„ ì¸ì½”ë” íŒŒì¼ ì¡´ì¬: {'âœ…' if encoder_exists else 'âŒ'}")

        if encoder_exists:
            try:
                encoder_size = os.path.getsize(ENCODER_PATH)
                encoder_size_kb = encoder_size / 1024
                logger.info(f"ğŸ“ ì¸ì½”ë” íŒŒì¼ í¬ê¸°: {encoder_size:,} bytes ({encoder_size_kb:.2f} KB)")

                # íŒŒì¼ ê¶Œí•œ í™•ì¸
                encoder_readable = os.access(ENCODER_PATH, os.R_OK)
                logger.info(f"ğŸ” ì¸ì½”ë” íŒŒì¼ ì½ê¸° ê¶Œí•œ: {'âœ…' if encoder_readable else 'âŒ'}")

                # íŒŒì¼ í¬ê¸° ìœ íš¨ì„±
                MIN_ENCODER_SIZE = 100  # 100B
                encoder_size_valid = encoder_size > MIN_ENCODER_SIZE
                logger.info(f"ğŸ“ ì¸ì½”ë” íŒŒì¼ í¬ê¸° ìœ íš¨ì„± (>{MIN_ENCODER_SIZE}B): {'âœ…' if encoder_size_valid else 'âŒ'}")

                if not encoder_size_valid:
                    logger.warning(f"âš ï¸ ì¸ì½”ë” íŒŒì¼ì´ ë„ˆë¬´ ì‘ìŠµë‹ˆë‹¤. ì˜ˆìƒ ìµœì†Œ í¬ê¸°: {MIN_ENCODER_SIZE}B, ì‹¤ì œ: {encoder_size}B")

                if not encoder_readable:
                    logger.error(f"âŒ ì¸ì½”ë” íŒŒì¼ ì½ê¸° ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤: {ENCODER_PATH}")

            except OSError as e:
                logger.error(f"âŒ ì¸ì½”ë” íŒŒì¼ ì •ë³´ ì½ê¸° ì‹¤íŒ¨: {e}")
                encoder_size_valid = False
                encoder_readable = False
        else:
            logger.warning(f"âš ï¸ ì¸ì½”ë” íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {ENCODER_PATH}")
            encoder_size_valid = False
            encoder_readable = False

        # === 3. TensorFlow ê°€ìš©ì„± í™•ì¸ ===
        logger.info(f"ğŸ” TensorFlow ê°€ìš©ì„± í™•ì¸...")
        tf_available = False
        tf_version = "Unknown"
        tf_error = None

        try:
            import tensorflow as tf
            tf_available = True
            tf_version = tf.__version__
            logger.info(f"âœ… TensorFlow ì‚¬ìš© ê°€ëŠ¥: v{tf_version}")
        except ImportError as e:
            tf_error = f"TensorFlow ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}"
            logger.error(f"âŒ {tf_error}")
        except Exception as e:
            tf_error = f"TensorFlow í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}"
            logger.error(f"âŒ {tf_error}")

        # === 4. ìµœì¢… íŒì • ===
        model_valid = model_exists and model_size_valid and readable
        encoder_valid = encoder_exists and encoder_size_valid and encoder_readable

        _model_available = model_valid and encoder_valid and tf_available

        # ìƒì„¸í•œ ì‹¤íŒ¨ ì‚¬ìœ  ì„¤ì •
        if not _model_available:
            reasons = []
            if not model_exists:
                reasons.append("ëª¨ë¸ íŒŒì¼ ì—†ìŒ")
            elif not model_size_valid:
                reasons.append("ëª¨ë¸ íŒŒì¼ í¬ê¸° ë¶€ì¡±")
            elif not readable:
                reasons.append("ëª¨ë¸ íŒŒì¼ ì½ê¸° ê¶Œí•œ ì—†ìŒ")

            if not encoder_exists:
                reasons.append("ì¸ì½”ë” íŒŒì¼ ì—†ìŒ")
            elif not encoder_size_valid:
                reasons.append("ì¸ì½”ë” íŒŒì¼ í¬ê¸° ë¶€ì¡±")
            elif not encoder_readable:
                reasons.append("ì¸ì½”ë” íŒŒì¼ ì½ê¸° ê¶Œí•œ ì—†ìŒ")

            if not tf_available:
                reasons.append(f"TensorFlow ì‚¬ìš© ë¶ˆê°€ ({tf_error})")

            _model_availability_reason = "; ".join(reasons)
        else:
            _model_availability_reason = "ëª¨ë“  ì¡°ê±´ ë§Œì¡±"

        # === 5. ìµœì¢… ê²°ê³¼ ë¡œê·¸ ===
        logger.info(f"ğŸ¯ ëª¨ë¸ ê°€ìš©ì„± ìµœì¢… íŒì •: {'âœ… ì‚¬ìš© ê°€ëŠ¥' if _model_available else 'âŒ ì‚¬ìš© ë¶ˆê°€'}")
        logger.info(f"ğŸ“‹ íŒì • ì‚¬ìœ : {_model_availability_reason}")

        if _model_available:
            logger.info(f"ğŸ¤– AI ê°ì • í´ëŸ¬ìŠ¤í„° ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤")
        else:
            logger.info(f"ğŸ“‹ ë£° ê¸°ë°˜ ì¶”ì²œ ì‹œìŠ¤í…œì„ ì‚¬ìš©í•©ë‹ˆë‹¤")

        # === 6. ìš”ì•½ ì •ë³´ ===
        logger.info(f"ğŸ“Š ëª¨ë¸ íŒŒì¼ ìƒíƒœ ìš”ì•½:")
        logger.info(f"   - ëª¨ë¸ íŒŒì¼: {MODEL_PATH}")
        logger.info(
            f"     ì¡´ì¬: {'âœ…' if model_exists else 'âŒ'} | í¬ê¸° ìœ íš¨: {'âœ…' if model_size_valid else 'âŒ'} | ì½ê¸° ê°€ëŠ¥: {'âœ…' if readable else 'âŒ'}")
        logger.info(f"   - ì¸ì½”ë” íŒŒì¼: {ENCODER_PATH}")
        logger.info(
            f"     ì¡´ì¬: {'âœ…' if encoder_exists else 'âŒ'} | í¬ê¸° ìœ íš¨: {'âœ…' if encoder_size_valid else 'âŒ'} | ì½ê¸° ê°€ëŠ¥: {'âœ…' if encoder_readable else 'âŒ'}")
        logger.info(f"   - TensorFlow: {'âœ…' if tf_available else 'âŒ'} (v{tf_version})")

        return _model_available

    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ê°€ìš©ì„± í™•ì¸ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        logger.error(f"ğŸ” ì˜ˆì™¸ ìƒì„¸: {type(e).__name__}: {str(e)}")
        _model_available = False
        _model_availability_reason = f"í™•ì¸ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}"
        return False


def get_model_availability_info():
    """í˜„ì¬ ëª¨ë¸ ê°€ìš©ì„± ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return {
        "available": _model_available,
        "reason": _model_availability_reason,
        "model_path": MODEL_PATH,
        "encoder_path": ENCODER_PATH,
        "model_exists": os.path.exists(MODEL_PATH),
        "encoder_exists": os.path.exists(ENCODER_PATH),
        "current_dir": os.getcwd(),
        "router_dir": BASE_DIR
    }


# â”€â”€â”€ 6. ëª¨ë¸ ë¡œë”© í•¨ìˆ˜ë“¤ (í–¥ìƒëœ ë¡œê¹…) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_model():
    """Keras ê°ì • í´ëŸ¬ìŠ¤í„° ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    global _model

    if _model is None:
        logger.info(f"ğŸ¤– Keras ëª¨ë¸ ë¡œë”© ì‹œë„...")
        logger.info(f"ğŸ“‚ ëª¨ë¸ íŒŒì¼ ê²½ë¡œ: {MODEL_PATH}")

        try:
            if not os.path.exists(MODEL_PATH):
                logger.error(f"âŒ ëª¨ë¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {MODEL_PATH}")
                return None

            # íŒŒì¼ í¬ê¸° ì¬í™•ì¸
            model_size = os.path.getsize(MODEL_PATH)
            if model_size < 100000:  # 100KB ë¯¸ë§Œ
                logger.error(f"âŒ ëª¨ë¸ íŒŒì¼ì´ ë„ˆë¬´ ì‘ìŠµë‹ˆë‹¤: {model_size} bytes")
                return None

            # TensorFlow ë™ì  ì„í¬íŠ¸
            try:
                tf_start = datetime.now()
                from tensorflow.keras.models import load_model
                tf_load_time = (datetime.now() - tf_start).total_seconds()
                logger.info(f"ğŸ“¦ TensorFlow ë¡œë”© ì™„ë£Œ (ì†Œìš”ì‹œê°„: {tf_load_time:.3f}ì´ˆ)")

                model_start = datetime.now()
                _model = load_model(MODEL_PATH, compile=False)  # compile=Falseë¡œ ë¹ ë¥¸ ë¡œë”©
                model_load_time = (datetime.now() - model_start).total_seconds()

                # ëª¨ë¸ êµ¬ì¡° í™•ì¸
                logger.info(f"âœ… Keras ëª¨ë¸ ë¡œë“œ ì„±ê³µ (ì†Œìš”ì‹œê°„: {model_load_time:.3f}ì´ˆ)")
                logger.info(f"ğŸ“Š ëª¨ë¸ ì…ë ¥ shape: {_model.input_shape}")
                logger.info(f"ğŸ“Š ëª¨ë¸ ì¶œë ¥ shape: {_model.output_shape}")
                logger.info(f"ğŸ“Š ëª¨ë¸ ë ˆì´ì–´ ìˆ˜: {len(_model.layers)}")
                logger.info(f"ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {_model.count_params():,}")

                # ì¶œë ¥ í¬ê¸° í™•ì¸
                output_size = _model.output_shape[-1]
                if output_size == 6:
                    logger.info("ğŸ¯ ê°ì • í´ëŸ¬ìŠ¤í„° ë¶„ë¥˜ ëª¨ë¸ë¡œ ì¸ì‹ë¨")
                else:
                    logger.warning(f"âš ï¸ ì˜ˆìƒê³¼ ë‹¤ë¥¸ ì¶œë ¥ í¬ê¸°: {output_size} (ì˜ˆìƒ: 6)")

            except ImportError as e:
                logger.error(f"âŒ TensorFlowë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
                return None
            except Exception as e:
                logger.error(f"âŒ Keras ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                logger.error(f"ğŸ” ëª¨ë¸ ë¡œë“œ ì˜ˆì™¸ ìƒì„¸: {type(e).__name__}: {str(e)}")
                return None

        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì¤‘ ì˜ˆì™¸: {e}")
            logger.error(f"ğŸ” ë¡œë”© ì˜ˆì™¸ ìƒì„¸: {type(e).__name__}: {str(e)}")
            return None

    return _model


def get_saved_encoder():
    """ì €ì¥ëœ encoder.pklì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    global _encoder

    if _encoder is None:
        logger.info(f"ğŸ“¦ ì¸ì½”ë” ë¡œë”© ì‹œë„...")
        logger.info(f"ğŸ“‚ ì¸ì½”ë” íŒŒì¼ ê²½ë¡œ: {ENCODER_PATH}")

        try:
            if not os.path.exists(ENCODER_PATH):
                logger.error(f"âŒ ì¸ì½”ë” íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {ENCODER_PATH}")
                return None

            # íŒŒì¼ í¬ê¸° í™•ì¸
            encoder_size = os.path.getsize(ENCODER_PATH)
            logger.info(f"ğŸ“ ì¸ì½”ë” íŒŒì¼ í¬ê¸°: {encoder_size:,} bytes ({encoder_size / 1024:.2f} KB)")

            encoder_start = datetime.now()
            with open(ENCODER_PATH, "rb") as f:
                _encoder = pickle.load(f)
            encoder_load_time = (datetime.now() - encoder_start).total_seconds()

            logger.info(f"âœ… encoder.pkl ë¡œë“œ ì„±ê³µ (ì†Œìš”ì‹œê°„: {encoder_load_time:.3f}ì´ˆ)")

            # ì¸ì½”ë” êµ¬ì¡° í™•ì¸
            try:
                if hasattr(_encoder, 'categories_'):
                    categories_count = len(_encoder.categories_)
                    logger.info(f"ğŸ“Š ì¸ì½”ë” ì¹´í…Œê³ ë¦¬ ìˆ˜: {categories_count}")
                    for i, cat in enumerate(_encoder.categories_):
                        logger.info(f"   ì¹´í…Œê³ ë¦¬ {i}: {len(cat)}ê°œ ({list(cat)})")

                if hasattr(_encoder, 'feature_names_in_'):
                    logger.info(f"ğŸ“Š ì¸ì½”ë” í”¼ì²˜ëª…: {_encoder.feature_names_in_}")

            except Exception as e:
                logger.warning(f"âš ï¸ ì¸ì½”ë” êµ¬ì¡° ì •ë³´ ì½ê¸° ì‹¤íŒ¨: {e}")

        except Exception as e:
            logger.error(f"âŒ encoder.pkl ë¡œë“œ ì‹¤íŒ¨: {e}")
            logger.error(f"ğŸ” ì¸ì½”ë” ë¡œë“œ ì˜ˆì™¸ ìƒì„¸: {type(e).__name__}: {str(e)}")
            return None

    return _encoder


def get_fallback_encoder():
    """Fallback OneHotEncoderë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    global _fallback_encoder

    if _fallback_encoder is None:
        try:
            logger.info("ğŸ”§ Fallback OneHotEncoder ìƒì„± ì¤‘...")

            CATEGORIES = [
                ["women", "men", "unisex"],  # gender
                ["spring", "summer", "fall", "winter"],  # season
                ["day", "night"],  # time
                ["confident", "elegant", "pure", "friendly", "mysterious", "fresh"],  # impression
                ["casual", "work", "date"],  # activity
                ["hot", "cold", "rainy", "any"]  # weather
            ]

            _fallback_encoder = OneHotEncoder(
                categories=CATEGORIES,
                handle_unknown="ignore",
                sparse=False
            )

            # ë”ë¯¸ ë°ì´í„°ë¡œ fit
            dummy_data = [
                ["women", "spring", "day", "confident", "casual", "hot"],
                ["men", "summer", "night", "elegant", "work", "cold"],
                ["unisex", "fall", "day", "pure", "date", "rainy"],
                ["women", "winter", "night", "friendly", "casual", "any"],
                ["men", "spring", "day", "mysterious", "work", "hot"],
                ["unisex", "summer", "night", "fresh", "date", "cold"]
            ]

            fallback_start = datetime.now()
            _fallback_encoder.fit(dummy_data)
            fallback_time = (datetime.now() - fallback_start).total_seconds()

            logger.info(f"âœ… Fallback OneHotEncoder ìƒì„± ì™„ë£Œ (ì†Œìš”ì‹œê°„: {fallback_time:.3f}ì´ˆ)")
            logger.info(f"ğŸ“Š Fallback ì¸ì½”ë” ì¹´í…Œê³ ë¦¬ ìˆ˜: {len(CATEGORIES)}")
            logger.info(f"ğŸ“Š Fallback ì¸ì½”ë” ì¶œë ¥ ì°¨ì›: {_fallback_encoder.transform([dummy_data[0]]).shape[1]}")

        except Exception as e:
            logger.error(f"âŒ Fallback encoder ìƒì„± ì‹¤íŒ¨: {e}")
            logger.error(f"ğŸ” Fallback ìƒì„± ì˜ˆì™¸ ìƒì„¸: {type(e).__name__}: {str(e)}")
            return None

    return _fallback_encoder


# â”€â”€â”€ 7. AI ê°ì • í´ëŸ¬ìŠ¤í„° ëª¨ë¸ ì¶”ì²œ (í–¥ìƒëœ ë¡œê¹…) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def predict_with_emotion_cluster_model(request_dict: dict) -> pd.DataFrame:
    """ê°ì • í´ëŸ¬ìŠ¤í„° ëª¨ë¸ì„ ì‚¬ìš©í•œ AI ì¶”ì²œ"""

    try:
        logger.info(f"ğŸ¤– AI ê°ì • í´ëŸ¬ìŠ¤í„° ëª¨ë¸ ì¶”ì²œ ì‹œì‘...")

        # ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
        model_start = datetime.now()
        model = get_model()
        model_load_time = (datetime.now() - model_start).total_seconds()

        if model is None:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ (ì†Œìš”ì‹œê°„: {model_load_time:.3f}ì´ˆ)")
            raise Exception("ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
        else:
            logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ (ì†Œìš”ì‹œê°„: {model_load_time:.3f}ì´ˆ)")

        # ì¸ì½”ë”ë¡œ ì…ë ¥ ë°ì´í„° ë³€í™˜
        raw_features = [
            request_dict["gender"],
            request_dict["season"],
            request_dict["time"],
            request_dict["impression"],
            request_dict["activity"],
            request_dict["weather"]
        ]

        logger.info(f"ğŸ“ ì›ì‹œ ì…ë ¥ íŠ¹ì„±: {raw_features}")

        # ì¸ì½”ë” ì‚¬ìš©
        encoder_start = datetime.now()
        encoder = get_saved_encoder()
        if encoder:
            try:
                x_input = encoder.transform([raw_features])
                encoder_method = "ì €ì¥ëœ ì¸ì½”ë”"
                encoder_time = (datetime.now() - encoder_start).total_seconds()
                logger.info(f"âœ… ì €ì¥ëœ ì¸ì½”ë” ì‚¬ìš© ì„±ê³µ (ì†Œìš”ì‹œê°„: {encoder_time:.3f}ì´ˆ)")
            except Exception as e:
                encoder_time = (datetime.now() - encoder_start).total_seconds()
                logger.warning(f"âš ï¸ ì €ì¥ëœ ì¸ì½”ë” ì‹¤íŒ¨ (ì†Œìš”ì‹œê°„: {encoder_time:.3f}ì´ˆ): {e}")
                logger.info("ğŸ”§ Fallback ì¸ì½”ë”ë¡œ ì „í™˜...")

                fallback_start = datetime.now()
                fallback_encoder = get_fallback_encoder()
                if fallback_encoder:
                    x_input = fallback_encoder.transform([raw_features])
                    encoder_method = "Fallback ì¸ì½”ë”"
                    fallback_time = (datetime.now() - fallback_start).total_seconds()
                    logger.info(f"âœ… Fallback ì¸ì½”ë” ì‚¬ìš© ì„±ê³µ (ì†Œìš”ì‹œê°„: {fallback_time:.3f}ì´ˆ)")
                else:
                    raise Exception("Fallback encoder ìƒì„± ì‹¤íŒ¨")
        else:
            logger.info("ğŸ”§ ì €ì¥ëœ ì¸ì½”ë”ê°€ ì—†ì–´ Fallback ì¸ì½”ë” ì‚¬ìš©...")
            fallback_start = datetime.now()
            fallback_encoder = get_fallback_encoder()
            if fallback_encoder:
                x_input = fallback_encoder.transform([raw_features])
                encoder_method = "Fallback ì¸ì½”ë”"
                fallback_time = (datetime.now() - fallback_start).total_seconds()
                logger.info(f"âœ… Fallback ì¸ì½”ë” ì‚¬ìš© ì„±ê³µ (ì†Œìš”ì‹œê°„: {fallback_time:.3f}ì´ˆ)")
            else:
                raise Exception("Fallback encoder ìƒì„± ì‹¤íŒ¨")

        logger.info(f"ğŸ”® ê°ì • í´ëŸ¬ìŠ¤í„° ì˜ˆì¸¡ ì‹œì‘ (ì…ë ¥ shape: {x_input.shape}, ì¸ì½”ë”: {encoder_method})")
        logger.info(f"ğŸ“Š ì¸ì½”ë”©ëœ ì…ë ¥ ë²¡í„° í¬ê¸°: {x_input.shape}")

        # ëª¨ë¸ ì˜ˆì¸¡ (ê°ì • í´ëŸ¬ìŠ¤í„°)
        predict_start = datetime.now()
        preds = model.predict(x_input, verbose=0)  # (1, 6) ì¶œë ¥
        predict_time = (datetime.now() - predict_start).total_seconds()

        cluster_probabilities = preds[0]  # [0.1, 0.8, 0.05, 0.02, 0.02, 0.01]
        predicted_cluster = int(np.argmax(cluster_probabilities))  # ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í´ëŸ¬ìŠ¤í„°
        confidence = float(cluster_probabilities[predicted_cluster])

        cluster_name = EMOTION_CLUSTER_MAP.get(predicted_cluster, f"í´ëŸ¬ìŠ¤í„° {predicted_cluster}")
        logger.info(f"âœ… ëª¨ë¸ ì˜ˆì¸¡ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {predict_time:.3f}ì´ˆ)")
        logger.info(f"ğŸ¯ ì˜ˆì¸¡ëœ ê°ì • í´ëŸ¬ìŠ¤í„°: {predicted_cluster} ({cluster_name}) - ì‹ ë¢°ë„: {confidence:.3f}")
        logger.info(f"ğŸ“Š ì „ì²´ í´ëŸ¬ìŠ¤í„° í™•ë¥ : {[f'{i}:{p:.3f}' for i, p in enumerate(cluster_probabilities)]}")

        # ê°ì • í´ëŸ¬ìŠ¤í„°ì— í•´ë‹¹í•˜ëŠ” í–¥ìˆ˜ í•„í„°ë§
        if 'emotion_cluster' in df.columns:
            cluster_perfumes = df[df['emotion_cluster'] == predicted_cluster].copy()
            logger.info(f"ğŸ“‹ í´ëŸ¬ìŠ¤í„° {predicted_cluster} í–¥ìˆ˜ ê°œìˆ˜: {len(cluster_perfumes)}ê°œ")
        else:
            logger.warning("âš ï¸ emotion_cluster ì»¬ëŸ¼ì´ ì—†ì–´ ì „ì²´ ë°ì´í„° ì‚¬ìš©")
            cluster_perfumes = df.copy()

        # í´ëŸ¬ìŠ¤í„°ì— í•´ë‹¹í•˜ëŠ” í–¥ìˆ˜ê°€ ì—†ìœ¼ë©´ ëŒ€ì²´ í´ëŸ¬ìŠ¤í„° ì‚¬ìš©
        if cluster_perfumes.empty:
            logger.warning(f"âš ï¸ í´ëŸ¬ìŠ¤í„° {predicted_cluster}ì— í•´ë‹¹í•˜ëŠ” í–¥ìˆ˜ê°€ ì—†ìŒ")
            # ë‘ ë²ˆì§¸ë¡œ ë†’ì€ í™•ë¥ ì˜ í´ëŸ¬ìŠ¤í„° ì°¾ê¸°
            second_best = int(np.argsort(cluster_probabilities)[-2])
            cluster_perfumes = df[df['emotion_cluster'] == second_best].copy()
            original_cluster = predicted_cluster
            predicted_cluster = second_best
            confidence = float(cluster_probabilities[second_best])
            logger.info(f"ğŸ“‹ ëŒ€ì²´ í´ëŸ¬ìŠ¤í„° {second_best} ì‚¬ìš© (ì›ë˜: {original_cluster}): {len(cluster_perfumes)}ê°œ")

        # ì¶”ê°€ í•„í„°ë§ (ì„±ë³„, ê³„ì ˆ ë“±)
        original_count = len(cluster_perfumes)
        logger.info(f"ğŸ” ì¶”ê°€ í•„í„°ë§ ì‹œì‘ (í˜„ì¬ í›„ë³´: {original_count}ê°œ)")

        # ì„±ë³„ í•„í„°ë§
        if 'gender' in cluster_perfumes.columns:
            gender_filtered = cluster_perfumes[
                cluster_perfumes['gender'] == request_dict["gender"]
                ]
            if not gender_filtered.empty:
                cluster_perfumes = gender_filtered
                logger.info(f"  ì„±ë³„ '{request_dict['gender']}' í•„í„°ë§: {original_count} â†’ {len(cluster_perfumes)}ê°œ")

        # ê³„ì ˆ í•„í„°ë§
        if 'season_tags' in cluster_perfumes.columns:
            season_filtered = cluster_perfumes[
                cluster_perfumes['season_tags'].str.contains(
                    request_dict["season"], na=False, case=False
                )
            ]
            if not season_filtered.empty:
                cluster_perfumes = season_filtered
                logger.info(f"  ê³„ì ˆ '{request_dict['season']}' í•„í„°ë§: â†’ {len(cluster_perfumes)}ê°œ")

        # ì‹œê°„ í•„í„°ë§
        if 'time_tags' in cluster_perfumes.columns:
            time_filtered = cluster_perfumes[
                cluster_perfumes['time_tags'].str.contains(
                    request_dict["time"], na=False, case=False
                )
            ]
            if not time_filtered.empty:
                cluster_perfumes = time_filtered
                logger.info(f"  ì‹œê°„ '{request_dict['time']}' í•„í„°ë§: â†’ {len(cluster_perfumes)}ê°œ")

        # AI ì‹ ë¢°ë„ ê¸°ë°˜ ì ìˆ˜ í• ë‹¹
        cluster_perfumes = cluster_perfumes.copy()

        # í´ëŸ¬ìŠ¤í„° ì‹ ë¢°ë„ë¥¼ ê¸°ë³¸ ì ìˆ˜ë¡œ ì‚¬ìš©
        base_score = confidence * 0.8  # AI ì‹ ë¢°ë„ì˜ 80%ë¥¼ ê¸°ë³¸ ì ìˆ˜ë¡œ
        logger.info(f"ğŸ“Š ê¸°ë³¸ ì ìˆ˜ (AI ì‹ ë¢°ë„ ê¸°ë°˜): {base_score:.3f}")

        scores = []
        for idx, (_, row) in enumerate(cluster_perfumes.iterrows()):
            score = base_score

            # ì¶”ê°€ ì¡°ê±´ ì¼ì¹˜ ë³´ë„ˆìŠ¤
            if 'season_tags' in row and request_dict["season"].lower() in str(row['season_tags']).lower():
                score += 0.08
            if 'time_tags' in row and request_dict["time"].lower() in str(row['time_tags']).lower():
                score += 0.06
            if 'desired_impression' in row and request_dict["impression"].lower() in str(
                    row['desired_impression']).lower():
                score += 0.05

            # ë¸Œëœë“œ ì¸ê¸°ë„ ë³´ë„ˆìŠ¤
            popular_brands = ['Creed', 'Tom Ford', 'Chanel', 'Dior', 'Jo Malone', 'Diptyque']
            if any(popular in str(row.get('brand', '')) for popular in popular_brands):
                score += 0.03

            # ë‹¤ì–‘ì„±ì„ ìœ„í•œ ìœ„ì¹˜ ê¸°ë°˜ ì ìˆ˜ (ì•ìª½ì¼ìˆ˜ë¡ ì•½ê°„ ë†’ì€ ì ìˆ˜)
            position_bonus = (len(cluster_perfumes) - idx) / len(cluster_perfumes) * 0.05
            score += position_bonus

            # ëœë¤ ìš”ì†Œ (ë‹¤ì–‘ì„± í™•ë³´)
            score += random.uniform(-0.03, 0.05)

            # ì •ê·œí™” (0.4 ~ 0.95 ë²”ìœ„)
            score = max(0.4, min(0.95, score))
            scores.append(score)

        cluster_perfumes['score'] = scores

        # ìƒìœ„ 10ê°œ ì„ íƒ
        top_10 = cluster_perfumes.nlargest(10, 'score')

        logger.info(f"âœ… AI í´ëŸ¬ìŠ¤í„° ëª¨ë¸ ì¶”ì²œ ì™„ë£Œ: {len(top_10)}ê°œ")
        if not top_10.empty:
            logger.info(f"ğŸ“Š ì ìˆ˜ ë²”ìœ„: {top_10['score'].min():.3f} ~ {top_10['score'].max():.3f}")
            logger.info(f"ğŸ“Š í‰ê·  ì ìˆ˜: {top_10['score'].mean():.3f}")
            logger.info(f"ğŸ“Š ì„ íƒëœ í–¥ìˆ˜ë“¤: {list(top_10['name'].head(3))[:3]}...")

        return top_10

    except Exception as e:
        logger.error(f"âŒ AI í´ëŸ¬ìŠ¤í„° ëª¨ë¸ ì¶”ì²œ ì‹¤íŒ¨: {e}")
        logger.error(f"ğŸ” AI ì¶”ì²œ ì˜ˆì™¸ ìƒì„¸: {type(e).__name__}: {str(e)}")
        raise e


# â”€â”€â”€ 8. ë£° ê¸°ë°˜ ì¶”ì²œ ì‹œìŠ¤í…œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def rule_based_recommendation(request_data: dict, top_k: int = 10) -> List[dict]:
    """ë£° ê¸°ë°˜ í–¥ìˆ˜ ì¶”ì²œ ì‹œìŠ¤í…œ (AI ëª¨ë¸ ëŒ€ì²´)"""
    logger.info("ğŸ¯ ë£° ê¸°ë°˜ ì¶”ì²œ ì‹œìŠ¤í…œ ì‹œì‘")

    try:
        # í•„í„°ë§ ì¡°ê±´
        gender = request_data["gender"]
        season = request_data["season"]
        time = request_data["time"]
        impression = request_data["impression"]
        activity = request_data["activity"]
        weather = request_data["weather"]

        logger.info(f"ğŸ” í•„í„°ë§ ì¡°ê±´: gender={gender}, season={season}, time={time}, "
                    f"impression={impression}, activity={activity}, weather={weather}")

        # ì„±ë³„ ë§¤í•‘
        gender_map = {"women": "women", "men": "men", "unisex": "unisex"}
        mapped_gender = gender_map.get(gender, "unisex")

        # 1ë‹¨ê³„: ê¸°ë³¸ í•„í„°ë§
        candidates = df.copy()
        original_count = len(candidates)

        # ì„±ë³„ í•„í„°ë§
        if 'gender' in df.columns:
            gender_filtered = candidates[candidates['gender'] == mapped_gender]
            if not gender_filtered.empty:
                candidates = gender_filtered
                logger.info(f"  ì„±ë³„ '{mapped_gender}' í•„í„°ë§: {original_count} â†’ {len(candidates)}ê°œ")

        # ê³„ì ˆ í•„í„°ë§
        if 'season_tags' in df.columns:
            season_filtered = candidates[
                candidates['season_tags'].str.contains(season, na=False, case=False)
            ]
            if not season_filtered.empty:
                candidates = season_filtered
                logger.info(f"  ê³„ì ˆ '{season}' í•„í„°ë§: â†’ {len(candidates)}ê°œ")

        # ì‹œê°„ í•„í„°ë§
        if 'time_tags' in df.columns:
            time_filtered = candidates[
                candidates['time_tags'].str.contains(time, na=False, case=False)
            ]
            if not time_filtered.empty:
                candidates = time_filtered
                logger.info(f"  ì‹œê°„ '{time}' í•„í„°ë§: â†’ {len(candidates)}ê°œ")

        # ì¸ìƒ í•„í„°ë§
        if 'desired_impression' in df.columns:
            impression_filtered = candidates[
                candidates['desired_impression'].str.contains(impression, na=False, case=False)
            ]
            if not impression_filtered.empty:
                candidates = impression_filtered
                logger.info(f"  ì¸ìƒ '{impression}' í•„í„°ë§: â†’ {len(candidates)}ê°œ")

        # í™œë™ í•„í„°ë§ (ìˆëŠ” ê²½ìš°)
        if 'activity' in df.columns:
            activity_filtered = candidates[
                candidates['activity'].str.contains(activity, na=False, case=False)
            ]
            if not activity_filtered.empty:
                candidates = activity_filtered
                logger.info(f"  í™œë™ '{activity}' í•„í„°ë§: â†’ {len(candidates)}ê°œ")

        # ë‚ ì”¨ í•„í„°ë§ (ìˆëŠ” ê²½ìš°)
        if 'weather' in df.columns and weather != 'any':
            weather_filtered = candidates[
                candidates['weather'].str.contains(weather, na=False, case=False)
            ]
            if not weather_filtered.empty:
                candidates = weather_filtered
                logger.info(f"  ë‚ ì”¨ '{weather}' í•„í„°ë§: â†’ {len(candidates)}ê°œ")

        # 2ë‹¨ê³„: ìŠ¤ì½”ì–´ë§
        if candidates.empty:
            logger.warning("âš ï¸ í•„í„°ë§ ê²°ê³¼ê°€ ì—†ì–´ ì „ì²´ ë°ì´í„°ì—ì„œ ë‹¤ì–‘ì„± ê¸°ë°˜ ì„ íƒ")
            # ë‹¤ì–‘í•œ ë¸Œëœë“œì—ì„œ ê³ ë¥´ê²Œ ì„ íƒ
            if 'brand' in df.columns:
                unique_brands = df['brand'].unique()
                candidates_list = []
                per_brand = max(1, top_k // len(unique_brands))

                for brand in unique_brands:
                    brand_perfumes = df[df['brand'] == brand].sample(
                        n=min(per_brand, len(df[df['brand'] == brand])),
                        random_state=42
                    )
                    candidates_list.append(brand_perfumes)

                candidates = pd.concat(candidates_list).head(top_k)
            else:
                candidates = df.sample(n=min(top_k, len(df)), random_state=42)

        # ì ìˆ˜ ê³„ì‚°
        candidates = candidates.copy()
        scores = []

        # ë¸Œëœë“œë³„ ê°€ì¤‘ì¹˜ (ì¸ê¸° ë¸Œëœë“œ ì˜ˆì‹œ)
        popular_brands = ['Creed', 'Tom Ford', 'Chanel', 'Dior', 'Jo Malone', 'Diptyque']

        for idx, (_, row) in enumerate(candidates.iterrows()):
            score = 0.3  # ê¸°ë³¸ ì ìˆ˜

            # 1. ì¡°ê±´ ì¼ì¹˜ë„ ì ìˆ˜
            brand_name = str(row.get('brand', ''))
            notes_text = str(row.get('notes', ''))

            # ë¸Œëœë“œ ì¸ê¸°ë„ ë³´ë„ˆìŠ¤
            if any(popular in brand_name for popular in popular_brands):
                score += 0.15

            # ë…¸íŠ¸ ë³µì¡ì„± (ë” ë§ì€ ë…¸íŠ¸ = ë” ë³µì¡í•œ í–¥ìˆ˜)
            note_count = len([n.strip() for n in notes_text.split(',') if n.strip()])
            if note_count >= 8:
                score += 0.10
            elif note_count >= 5:
                score += 0.05

            # í…ìŠ¤íŠ¸ ë§¤ì¹­ ì •í™•ë„
            impression_match_count = 0
            if 'desired_impression' in row:
                impressions = str(row['desired_impression']).lower().split(',')
                impression_match_count = sum(1 for imp in impressions if impression.lower() in imp.strip())
                score += impression_match_count * 0.08

            # ê³„ì ˆ/ì‹œê°„ ë§¤ì¹­ ì •í™•ë„
            if 'season_tags' in row:
                season_tags = str(row['season_tags']).lower()
                if season.lower() in season_tags:
                    score += 0.12 if f' {season.lower()} ' in f' {season_tags} ' else 0.08

            if 'time_tags' in row:
                time_tags = str(row['time_tags']).lower()
                if time.lower() in time_tags:
                    score += 0.12 if f' {time.lower()} ' in f' {time_tags} ' else 0.08

            # í™œë™ ë§¤ì¹­
            if 'activity' in row and activity.lower() in str(row['activity']).lower():
                score += 0.08

            # ë‚ ì”¨ ë§¤ì¹­
            if 'weather' in row and weather != 'any':
                if weather.lower() in str(row['weather']).lower():
                    score += 0.06
            elif weather == 'any':
                score += 0.03

            # ë‹¤ì–‘ì„±ì„ ìœ„í•œ ìœ„ì¹˜ ê¸°ë°˜ ì ìˆ˜
            position_bonus = (len(candidates) - idx) / len(candidates) * 0.05
            score += position_bonus

            # ëœë¤ ìš”ì†Œ (ë‹¤ì–‘ì„± í™•ë³´)
            score += random.uniform(-0.15, 0.15)

            # ì ìˆ˜ ì •ê·œí™” (0.2 ~ 0.95 ë²”ìœ„)
            score = max(0.2, min(0.95, score))
            scores.append(score)

        candidates['score'] = scores

        # ìƒìœ„ Kê°œ ì„ íƒ
        top_candidates = candidates.nlargest(top_k, 'score')

        logger.info(f"âœ… ë£° ê¸°ë°˜ ì¶”ì²œ ì™„ë£Œ: ìµœì¢… {len(top_candidates)}ê°œ ì„ íƒ")
        if not top_candidates.empty:
            logger.info(f"ğŸ“Š ì ìˆ˜ ë²”ìœ„: {top_candidates['score'].min():.3f} ~ {top_candidates['score'].max():.3f}")
            logger.info(f"ğŸ“Š í‰ê·  ì ìˆ˜: {top_candidates['score'].mean():.3f}")
            logger.info(f"ğŸ“Š ì„ íƒëœ í–¥ìˆ˜ë“¤: {list(top_candidates['name'].head(3))[:3]}...")

        return top_candidates.to_dict('records')

    except Exception as e:
        logger.error(f"âŒ ë£° ê¸°ë°˜ ì¶”ì²œ ì¤‘ ì˜¤ë¥˜: {e}")
        logger.error(f"ğŸ” ë£° ê¸°ë°˜ ì˜ˆì™¸ ìƒì„¸: {type(e).__name__}: {str(e)}")
        # ìµœì¢… ì•ˆì „ì¥ì¹˜: ì™„ì „ ëœë¤ ì¶”ì²œ
        logger.info("ğŸ² ì™„ì „ ëœë¤ ì¶”ì²œìœ¼ë¡œ ëŒ€ì²´")
        random_sample = df.sample(n=min(top_k, len(df)), random_state=42)
        random_sample = random_sample.copy()
        random_sample['score'] = [random.uniform(0.4, 0.7) for _ in range(len(random_sample))]
        return random_sample.to_dict('records')


# â”€â”€â”€ 9. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_emotion_text(row):
    """ê°ì • ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    # 1ìˆœìœ„: desired_impression
    if 'desired_impression' in df.columns and pd.notna(row.get('desired_impression')):
        return str(row['desired_impression'])

    # 2ìˆœìœ„: emotion_clusterë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    if 'emotion_cluster' in df.columns and pd.notna(row.get('emotion_cluster')):
        cluster_id = int(row['emotion_cluster']) if str(row['emotion_cluster']).isdigit() else 0
        return EMOTION_CLUSTER_MAP.get(cluster_id, "ê· í˜•ì¡íŒ")

    return "ë‹¤ì–‘í•œ ê°ì •"


def get_recommendation_reason(score: float, method: str) -> str:
    """ì ìˆ˜ì™€ ë°©ë²•ì— ë”°ë¥¸ ì¶”ì²œ ì´ìœ  ìƒì„±"""

    if method.startswith("AI"):
        if score >= 0.9:
            return f"ğŸ¤– AIê°€ {score:.1%} í™•ë¥ ë¡œ ë‹¹ì‹ ì˜ ì™„ë²½í•œ í–¥ìˆ˜ë¼ê³  ë¶„ì„í–ˆìŠµë‹ˆë‹¤!"
        elif score >= 0.8:
            return f"ğŸ¤– AIê°€ {score:.1%} í™•ë¥ ë¡œ ë‹¹ì‹ ì—ê²Œ ì˜ ë§ì„ ê²ƒì´ë¼ ì˜ˆì¸¡í–ˆìŠµë‹ˆë‹¤."
        elif score >= 0.6:
            return f"ğŸ¤– AIê°€ {score:.1%} í™•ë¥ ë¡œ ìƒˆë¡œìš´ ì‹œë„í•´ë³¼ ë§Œí•œ í–¥ìˆ˜ë¡œ ì¶”ì²œí–ˆìŠµë‹ˆë‹¤."
        else:
            return f"ğŸ¤– AIê°€ {score:.1%} í™•ë¥ ë¡œ ìƒ‰ë‹¤ë¥¸ ë§¤ë ¥ì„ ì œì•ˆí•©ë‹ˆë‹¤."
    else:
        # ë£° ê¸°ë°˜ - ë” ë‹¤ì–‘í•œ ë©”ì‹œì§€
        if score >= 0.9:
            return f"ğŸ¯ ì¡°ê±´ ì™„ë²½ ì¼ì¹˜ (ì¼ì¹˜ë„ {score:.1%}) - ì´ë³´ë‹¤ ì™„ë²½í•  ìˆœ ì—†ì–´ìš”!"
        elif score >= 0.8:
            return f"â­ ì¡°ê±´ ë†’ì€ ì¼ì¹˜ (ì¼ì¹˜ë„ {score:.1%}) - ê°•ë ¥ ì¶”ì²œ!"
        elif score >= 0.6:
            return f"âœ¨ ì¡°ê±´ ì í•© (ì¼ì¹˜ë„ {score:.1%}) - ê³ ë ¤í•´ë³´ì„¸ìš”!"
        elif score >= 0.4:
            return f"ğŸ” ë¶€ë¶„ ì¼ì¹˜ (ì¼ì¹˜ë„ {score:.1%}) - ìƒˆë¡œìš´ ë°œê²¬ì´ ë  ìˆ˜ë„!"
        else:
            return f"ğŸ² ìƒˆë¡œìš´ ìŠ¤íƒ€ì¼ ì œì•ˆ (ì¼ì¹˜ë„ {score:.1%}) - ë„ì „í•´ë³´ì„¸ìš”!"


# â”€â”€â”€ 10. ìŠ¤í‚¤ë§ˆ ì •ì˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class RecommendRequest(BaseModel):
    gender: Literal["women", "men", "unisex"]
    season: Literal["spring", "summer", "fall", "winter"]
    time: Literal["day", "night"]
    impression: Literal["confident", "elegant", "pure", "friendly", "mysterious", "fresh"]
    activity: Literal["casual", "work", "date"]
    weather: Literal["hot", "cold", "rainy", "any"]


class PerfumeRecommendItem(BaseModel):
    name: str
    brand: str
    image_url: str
    notes: str
    emotions: str
    reason: str
    score: Optional[float] = None
    method: Optional[str] = None


# â”€â”€â”€ 11. ë¼ìš°í„° ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
router = APIRouter(prefix="/perfumes", tags=["Perfume"])

# ì‹œì‘ ì‹œ ëª¨ë¸ ê°€ìš©ì„± í™•ì¸
logger.info("ğŸš€ ì¶”ì²œ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘...")
check_model_availability()
logger.info("âœ… ì¶”ì²œ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
logger.info(f"ğŸ¯ ìµœì¢… ì¶”ì²œ ë°©ì‹: {'ğŸ¤– AI ê°ì • í´ëŸ¬ìŠ¤í„° ëª¨ë¸' if _model_available else 'ğŸ“‹ ë£° ê¸°ë°˜ ì‹œìŠ¤í…œ'}")
logger.info(f"ğŸ“‹ ëª¨ë¸ ìƒíƒœ: {_model_availability_reason}")


# â”€â”€â”€ 12. API ì—”ë“œí¬ì¸íŠ¸ë“¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@router.post(
    "/recommend",
    response_model=List[PerfumeRecommendItem],
    summary="í–¥ìˆ˜ ì¶”ì²œ (AI ê°ì • í´ëŸ¬ìŠ¤í„° ëª¨ë¸ + ë£° ê¸°ë°˜ Fallback)",
    description=(
            "ì‚¬ìš©ìì˜ ì„ í˜¸ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í–¥ìˆ˜ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤.\n\n"
            "**ğŸ¤– ì¶”ì²œ ë°©ì‹:**\n"
            "1. **AI ê°ì • í´ëŸ¬ìŠ¤í„° ëª¨ë¸**: 6ê°œ ì…ë ¥ â†’ 6ê°œ ê°ì • í´ëŸ¬ìŠ¤í„° ë¶„ë¥˜ â†’ í•´ë‹¹ í´ëŸ¬ìŠ¤í„° í–¥ìˆ˜ ì¶”ì²œ\n"
            "2. **ë£° ê¸°ë°˜ Fallback**: ì¡°ê±´ë¶€ í•„í„°ë§ + ìŠ¤ì½”ì–´ë§ (ëª¨ë¸ì´ ì—†ê±°ë‚˜ ì‹¤íŒ¨í•œ ê²½ìš°)\n"
            "3. **ë‹¤ì–‘ì„± ë³´ì¥**: ë¸Œëœë“œë³„ ê· í˜• ì¡íŒ ì¶”ì²œ\n\n"
            "**ğŸ“‹ ì…ë ¥ íŒŒë¼ë¯¸í„°:**\n"
            "- `gender`: ì„±ë³„ (women/men/unisex)\n"
            "- `season`: ê³„ì ˆ (spring/summer/fall/winter)\n"
            "- `time`: ì‹œê°„ëŒ€ (day/night)\n"
            "- `impression`: ì›í•˜ëŠ” ì¸ìƒ (confident/elegant/pure/friendly/mysterious/fresh)\n"
            "- `activity`: í™œë™ (casual/work/date)\n"
            "- `weather`: ë‚ ì”¨ (hot/cold/rainy/any)\n\n"
            "**ğŸ§  AI ëª¨ë¸ ì„¸ë¶€ì‚¬í•­:**\n"
            "- ëª¨ë¸ êµ¬ì¡°: Sequential (Dense 256 â†’ 128 â†’ 64 â†’ 6)\n"
            "- ì¶œë ¥: 6ê°œ ê°ì • í´ëŸ¬ìŠ¤í„° í™•ë¥  (softmax)\n"
            "- í•™ìŠµ ë°ì´í„°: 2025-05-26 ì €ì¥\n"
            "- Keras ë²„ì „: 2.13.1\n\n"
            "**âœ¨ íŠ¹ì§•:**\n"
            "- ì‹¤ì œ ëª¨ë¸ íŒŒì¼ í¬ê¸° ê¸°ë°˜ ìœ íš¨ì„± ê²€ì¦\n"
            "- ê²¬ê³ í•œ ì—ëŸ¬ í•¸ë“¤ë§\n"
            "- ìƒì„¸í•œ ì¶”ì²œ ì´ìœ  ì œê³µ"
    )
)
def recommend_perfumes(request: RecommendRequest):
    request_start_time = datetime.now()
    logger.info(f"ğŸ¯ í–¥ìˆ˜ ì¶”ì²œ ìš”ì²­ ì‹œì‘: {request}")
    logger.info(f"ğŸ’¡ í˜„ì¬ ëª¨ë¸ ê°€ìš©ì„±: {'âœ… ì‚¬ìš© ê°€ëŠ¥' if _model_available else 'âŒ ì‚¬ìš© ë¶ˆê°€'} ({_model_availability_reason})")

    # ìš”ì²­ ë°ì´í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
    request_dict = request.dict()

    method_used = "ì•Œ ìˆ˜ ì—†ìŒ"

    # 1) AI ëª¨ë¸ ì‹œë„
    if _model_available:
        model_start_time = datetime.now()
        try:
            logger.info("ğŸ¤– AI ê°ì • í´ëŸ¬ìŠ¤í„° ëª¨ë¸ ì¶”ì²œ ì‹œë„")

            # ê°ì • í´ëŸ¬ìŠ¤í„° ëª¨ë¸ë¡œ ì¶”ì²œ
            top_10 = predict_with_emotion_cluster_model(request_dict)
            method_used = "AI ê°ì • í´ëŸ¬ìŠ¤í„° ëª¨ë¸"

            model_time = (datetime.now() - model_start_time).total_seconds()
            logger.info(f"âœ… AI ëª¨ë¸ ì¶”ì²œ ì„±ê³µ (ë°©ë²•: {method_used}, ì†Œìš”ì‹œê°„: {model_time:.3f}ì´ˆ)")

        except Exception as e:
            model_time = (datetime.now() - model_start_time).total_seconds()
            logger.warning(f"âš ï¸ AI ëª¨ë¸ ì¶”ì²œ ì‹¤íŒ¨ (ì†Œìš”ì‹œê°„: {model_time:.3f}ì´ˆ): {e}")
            logger.warning(f"ğŸ” AI ëª¨ë¸ ì‹¤íŒ¨ ìƒì„¸: {type(e).__name__}: {str(e)}")
            logger.info("ğŸ“‹ ë£° ê¸°ë°˜ ì¶”ì²œìœ¼ë¡œ ì „í™˜")

            rule_start_time = datetime.now()
            rule_results = rule_based_recommendation(request_dict, 10)
            top_10 = pd.DataFrame(rule_results)
            rule_time = (datetime.now() - rule_start_time).total_seconds()
            method_used = "ë£° ê¸°ë°˜ (AI ëª¨ë¸ ì‹¤íŒ¨)"
            logger.info(f"ğŸ“‹ ë£° ê¸°ë°˜ ì¶”ì²œ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {rule_time:.3f}ì´ˆ)")
    else:
        logger.info(f"ğŸ“‹ ë£° ê¸°ë°˜ ì¶”ì²œ ì‚¬ìš© (ëª¨ë¸ ì‚¬ìš© ë¶ˆê°€: {_model_availability_reason})")
        rule_start_time = datetime.now()
        rule_results = rule_based_recommendation(request_dict, 10)
        top_10 = pd.DataFrame(rule_results)
        rule_time = (datetime.now() - rule_start_time).total_seconds()
        method_used = f"ë£° ê¸°ë°˜ (ëª¨ë¸ ì‚¬ìš© ë¶ˆê°€)"
        logger.info(f"ğŸ“‹ ë£° ê¸°ë°˜ ì¶”ì²œ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {rule_time:.3f}ì´ˆ)")

    # 2) ê²°ê³¼ ê°€ê³µ
    response_list: List[PerfumeRecommendItem] = []
    for idx, (_, row) in enumerate(top_10.iterrows(), 1):
        emotions_text = get_emotion_text(row)
        score = float(row.get('score', 0.0))

        # ì¶”ì²œ ì´ìœ  ìƒì„±
        reason = get_recommendation_reason(score, method_used)

        response_list.append(
            PerfumeRecommendItem(
                name=str(row["name"]),
                brand=str(row["brand"]),
                image_url=str(row["image_url"]),
                notes=str(row["notes"]),
                emotions=emotions_text,
                reason=reason,
                score=score,
                method=method_used
            )
        )

    # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
    total_processing_time = (datetime.now() - request_start_time).total_seconds()

    logger.info(f"âœ… í–¥ìˆ˜ ì¶”ì²œ ì™„ë£Œ: {len(response_list)}ê°œ ({method_used})")
    logger.info(f"â±ï¸ ì´ ì²˜ë¦¬ ì‹œê°„: {total_processing_time:.3f}ì´ˆ")
    if response_list:
        logger.info(
            f"ğŸ“Š ì ìˆ˜ ë²”ìœ„: {min(item.score for item in response_list):.3f} ~ {max(item.score for item in response_list):.3f}")
        logger.info(f"ğŸ“Š í‰ê·  ì ìˆ˜: {sum(item.score for item in response_list) / len(response_list):.3f}")

    return response_list


@router.get(
    "/model-status",
    summary="ëª¨ë¸ ìƒíƒœ í™•ì¸",
    description="AI ëª¨ë¸ê³¼ ê´€ë ¨ íŒŒì¼ë“¤ì˜ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤."
)
def get_model_status():
    """ëª¨ë¸ ë° ì‹œìŠ¤í…œ ìƒíƒœë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""

    # í˜„ì¬ ìƒíƒœ ì¬í™•ì¸
    current_availability = check_model_availability()
    availability_info = get_model_availability_info()

    # íŒŒì¼ ìƒíƒœ í™•ì¸
    model_exists = os.path.exists(MODEL_PATH)
    encoder_exists = os.path.exists(ENCODER_PATH)

    model_size = os.path.getsize(MODEL_PATH) if model_exists else 0
    encoder_size = os.path.getsize(ENCODER_PATH) if encoder_exists else 0

    # ëª¨ë¸ êµ¬ì¡° ì •ë³´ (ëª¨ë¸ì´ ë¡œë“œëœ ê²½ìš°)
    model_structure = None
    if _model is not None:
        try:
            model_structure = {
                "input_shape": str(_model.input_shape),
                "output_shape": str(_model.output_shape),
                "total_params": _model.count_params(),
                "layers": len(_model.layers)
            }
        except:
            model_structure = "ëª¨ë¸ ì •ë³´ ì½ê¸° ì‹¤íŒ¨"

    # TensorFlow ìƒíƒœ í™•ì¸
    tf_status = "Unknown"
    try:
        import tensorflow as tf
        tf_status = f"ì‚¬ìš© ê°€ëŠ¥ (v{tf.__version__})"
    except ImportError:
        tf_status = "ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨"
    except Exception as e:
        tf_status = f"ì˜¤ë¥˜: {e}"

    return {
        "timestamp": datetime.now().isoformat(),
        "model_available": _model_available,
        "availability_reason": _model_availability_reason,
        "availability_info": availability_info,
        "current_availability_check": current_availability,
        "files": {
            "keras_model": {
                "path": MODEL_PATH,
                "exists": model_exists,
                "size_bytes": model_size,
                "size_mb": round(model_size / (1024 * 1024), 2),
                "valid": model_size > 100000,
                "readable": os.access(MODEL_PATH, os.R_OK) if model_exists else False
            },
            "encoder": {
                "path": ENCODER_PATH,
                "exists": encoder_exists,
                "size_bytes": encoder_size,
                "size_kb": round(encoder_size / 1024, 2),
                "valid": encoder_size > 100,
                "readable": os.access(ENCODER_PATH, os.R_OK) if encoder_exists else False
            }
        },
        "tensorflow_status": tf_status,
        "model_structure": model_structure,
        "emotion_clusters": EMOTION_CLUSTER_MAP,
        "recommendation_method": "AI ê°ì • í´ëŸ¬ìŠ¤í„° ëª¨ë¸" if _model_available else "ë£° ê¸°ë°˜",
        "fallback_encoder_ready": _fallback_encoder is not None,
        "system": {
            "python_version": sys.version.split()[0],
            "current_directory": os.getcwd(),
            "router_location": BASE_DIR,
            "dataset_loaded": len(df) > 0,
            "dataset_size": len(df)
        },
        "dataset_info": {
            "total_perfumes": len(df),
            "columns": list(df.columns),
            "sample_brands": df['brand'].unique()[:5].tolist() if 'brand' in df.columns else [],
            "emotion_cluster_distribution": dict(
                df['emotion_cluster'].value_counts()) if 'emotion_cluster' in df.columns else None
        }
    }


@router.get(
    "/health",
    summary="ì¶”ì²œ ì‹œìŠ¤í…œ í—¬ìŠ¤ ì²´í¬",
    description="ì¶”ì²œ ì‹œìŠ¤í…œì˜ ì „ë°˜ì ì¸ ê±´ê°• ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤."
)
def health_check():
    """ì¶”ì²œ ì‹œìŠ¤í…œ í—¬ìŠ¤ ì²´í¬"""

    health_status = {
        "timestamp": datetime.now().isoformat(),
        "status": "healthy",
        "checks": {}
    }

    # ë°ì´í„°ì…‹ í™•ì¸
    try:
        health_status["checks"]["dataset"] = {
            "status": "ok" if len(df) > 0 else "error",
            "perfume_count": len(df),
            "columns_available": len(df.columns)
        }
    except Exception as e:
        health_status["checks"]["dataset"] = {
            "status": "error",
            "error": str(e)
        }

    # ëª¨ë¸ íŒŒì¼ í™•ì¸
    try:
        model_exists = os.path.exists(MODEL_PATH)
        encoder_exists = os.path.exists(ENCODER_PATH)
        model_size = os.path.getsize(MODEL_PATH) if model_exists else 0
        encoder_size = os.path.getsize(ENCODER_PATH) if encoder_exists else 0

        model_valid = model_exists and model_size > 100000
        encoder_valid = encoder_exists and encoder_size > 100

        health_status["checks"]["model_files"] = {
            "status": "ok" if model_valid and encoder_valid else "warning",
            "model_available": model_valid,
            "encoder_available": encoder_valid,
            "model_size_mb": round(model_size / (1024 * 1024), 2),
            "encoder_size_kb": round(encoder_size / 1024, 2),
            "fallback_ready": _fallback_encoder is not None,
            "availability_reason": _model_availability_reason
        }
    except Exception as e:
        health_status["checks"]["model_files"] = {
            "status": "error",
            "error": str(e)
        }

    # ì¶”ì²œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
    try:
        test_request = {
            "gender": "women",
            "season": "spring",
            "time": "day",
            "impression": "fresh",
            "activity": "casual",
            "weather": "any"
        }

        start_time = datetime.now()
        if _model_available:
            try:
                test_results = predict_with_emotion_cluster_model(test_request)
                method = "AI ê°ì • í´ëŸ¬ìŠ¤í„° ëª¨ë¸"
            except:
                rule_results = rule_based_recommendation(test_request, 3)
                test_results = pd.DataFrame(rule_results)
                method = "ë£° ê¸°ë°˜ (AI ì‹¤íŒ¨)"
        else:
            rule_results = rule_based_recommendation(test_request, 3)
            test_results = pd.DataFrame(rule_results)
            method = "ë£° ê¸°ë°˜ (ëª¨ë¸ ì—†ìŒ)"

        processing_time = (datetime.now() - start_time).total_seconds()

        health_status["checks"]["recommendation_system"] = {
            "status": "ok" if len(test_results) > 0 else "error",
            "test_result_count": len(test_results),
            "processing_time_seconds": round(processing_time, 3),
            "method": method
        }
    except Exception as e:
        health_status["checks"]["recommendation_system"] = {
            "status": "error",
            "error": str(e)
        }

    # ì „ì²´ ìƒíƒœ ê²°ì •
    all_checks = health_status["checks"].values()
    if any(check.get("status") == "error" for check in all_checks):
        health_status["status"] = "unhealthy"
    elif any(check.get("status") == "warning" for check in all_checks):
        health_status["status"] = "degraded"

    return health_status


@router.post(
    "/test-recommendation",
    summary="ì¶”ì²œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ (ê°œë°œìš©)",
    description="ë‹¤ì–‘í•œ ì¡°ê±´ìœ¼ë¡œ ì¶”ì²œ ì‹œìŠ¤í…œì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."
)
def test_recommendation_system():
    """ì¶”ì²œ ì‹œìŠ¤í…œì„ ë‹¤ì–‘í•œ ì¡°ê±´ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""

    test_cases = [
        {
            "name": "ì—¬ì„±ìš© ë´„ ë°ì´ í–¥ìˆ˜",
            "request": {
                "gender": "women",
                "season": "spring",
                "time": "day",
                "impression": "fresh",
                "activity": "casual",
                "weather": "any"
            }
        },
        {
            "name": "ë‚¨ì„±ìš© ê²¨ìš¸ ë‚˜ì´íŠ¸ í–¥ìˆ˜",
            "request": {
                "gender": "men",
                "season": "winter",
                "time": "night",
                "impression": "confident",
                "activity": "date",
                "weather": "cold"
            }
        },
        {
            "name": "ìœ ë‹ˆì„¹ìŠ¤ ì—¬ë¦„ í–¥ìˆ˜",
            "request": {
                "gender": "unisex",
                "season": "summer",
                "time": "day",
                "impression": "mysterious",
                "activity": "work",
                "weather": "hot"
            }
        }
    ]

    results = []

    for test_case in test_cases:
        try:
            start_time = datetime.now()

            # AI ëª¨ë¸ ë˜ëŠ” ë£° ê¸°ë°˜ ì¶”ì²œ í…ŒìŠ¤íŠ¸
            if _model_available:
                try:
                    ai_results = predict_with_emotion_cluster_model(test_case["request"])
                    result_data = ai_results.to_dict('records')[:3]
                    method = "AI ê°ì • í´ëŸ¬ìŠ¤í„° ëª¨ë¸"
                except:
                    rule_results = rule_based_recommendation(test_case["request"], 5)
                    result_data = rule_results[:3]
                    method = "ë£° ê¸°ë°˜ (AI ì‹¤íŒ¨)"
            else:
                rule_results = rule_based_recommendation(test_case["request"], 5)
                result_data = rule_results[:3]
                method = "ë£° ê¸°ë°˜ (ëª¨ë¸ ì—†ìŒ)"

            processing_time = (datetime.now() - start_time).total_seconds()

            results.append({
                "test_name": test_case["name"],
                "request": test_case["request"],
                "success": True,
                "method": method,
                "result_count": len(result_data),
                "processing_time_seconds": round(processing_time, 3),
                "sample_results": [
                    {
                        "name": r.get("name", ""),
                        "brand": r.get("brand", ""),
                        "score": round(r.get("score", 0), 3)
                    } for r in result_data
                ]
            })

        except Exception as e:
            results.append({
                "test_name": test_case["name"],
                "request": test_case["request"],
                "success": False,
                "error": str(e),
                "processing_time_seconds": 0
            })

    return {
        "timestamp": datetime.now().isoformat(),
        "model_available": _model_available,
        "availability_reason": _model_availability_reason,
        "fallback_encoder_available": _fallback_encoder is not None,
        "dataset_size": len(df),
        "emotion_clusters": EMOTION_CLUSTER_MAP,
        "test_results": results,
        "summary": {
            "total_tests": len(test_cases),
            "successful_tests": sum(1 for r in results if r["success"]),
            "average_processing_time": round(
                sum(r.get("processing_time_seconds", 0) for r in results) / len(results), 3
            )
        }
    }