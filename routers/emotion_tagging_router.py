# routers/emotion_tagging_router.py
# ğŸ¯ ê°ì • íƒœê¹… AI ëª¨ë¸ API ë¼ìš°í„° (scent_emotion_model_v6.keras ì—°ë™)

import os
import logging
import numpy as np
import pickle
from datetime import datetime
from typing import Dict, List, Optional, Any
from fastapi import APIRouter, HTTPException, Depends
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field, validator

# TensorFlow ë™ì  ì„í¬íŠ¸ (vectorizer.pkl ë°›ê¸° ì „ê¹Œì§€ëŠ” ì£¼ì„ ì²˜ë¦¬)
try:
    import tensorflow as tf
    from tensorflow.keras.models import load_model

    TENSORFLOW_AVAILABLE = True
    logger = logging.getLogger("emotion_tagging")
    logger.info("âœ… TensorFlow ì‚¬ìš© ê°€ëŠ¥")
except ImportError as e:
    TENSORFLOW_AVAILABLE = False
    logger = logging.getLogger("emotion_tagging")
    logger.warning(f"âš ï¸ TensorFlow ì—†ìŒ: {e}")

# scikit-learn ë™ì  ì„í¬íŠ¸
try:
    from sklearn.feature_extraction.text import TfidfVectorizer

    SKLEARN_AVAILABLE = True
    logger.info("âœ… scikit-learn ì‚¬ìš© ê°€ëŠ¥")
except ImportError as e:
    SKLEARN_AVAILABLE = False
    logger.warning(f"âš ï¸ scikit-learn ì—†ìŒ: {e}")

# ê¸°ì¡´ ê°ì • ë¶„ì„ê¸° ì„í¬íŠ¸ (ë£° ê¸°ë°˜ í´ë°±ìš©)
from utils.emotion_analyzer import emotion_analyzer


# â”€â”€â”€ 1. ìŠ¤í‚¤ë§ˆ ì •ì˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class EmotionPredictRequest(BaseModel):
    text: str = Field(..., min_length=1, max_length=2000, description="ê°ì •ì„ ë¶„ì„í•  í…ìŠ¤íŠ¸")
    include_probabilities: bool = Field(False, description="ì „ì²´ ê°ì • í™•ë¥  ë¶„í¬ í¬í•¨ ì—¬ë¶€")

    @validator('text')
    def validate_text(cls, v):
        if not v.strip():
            raise ValueError('í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.')
        return v.strip()


class EmotionPredictResponse(BaseModel):
    emotion: str = Field(..., description="ì˜ˆì¸¡ëœ ê°ì •")
    confidence: float = Field(..., ge=0.0, le=1.0, description="ì‹ ë¢°ë„ (0.0-1.0)")
    label: int = Field(..., ge=0, le=7, description="ê°ì • ë¼ë²¨ (0-7)")
    method: str = Field(..., description="ì‚¬ìš©ëœ ë¶„ì„ ë°©ë²•")
    processing_time: float = Field(..., description="ì²˜ë¦¬ ì‹œê°„ (ì´ˆ)")
    probabilities: Optional[Dict[str, float]] = Field(None, description="ì „ì²´ ê°ì •ë³„ í™•ë¥  ë¶„í¬")


class EmotionHealthResponse(BaseModel):
    ai_model_available: bool = Field(..., description="AI ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€")
    vectorizer_available: bool = Field(..., description="ë²¡í„°ë¼ì´ì € ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€")
    fallback_available: bool = Field(..., description="ë£° ê¸°ë°˜ í´ë°± ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€")
    supported_emotions: List[str] = Field(..., description="ì§€ì›í•˜ëŠ” ê°ì • ëª©ë¡")
    model_info: Dict[str, Any] = Field(..., description="ëª¨ë¸ ì •ë³´")


# â”€â”€â”€ 2. ê°ì • íƒœê¹… í´ë˜ìŠ¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class WhiffEmotionTagger:
    """Whiff ì „ìš© ê°ì • íƒœê¹… í´ë˜ìŠ¤"""

    def __init__(self):
        # 8ê°€ì§€ ê°ì • ë§¤í•‘ (ë°ì´í„°ì…‹ê³¼ ë™ì¼)
        self.emotion_labels = {
            0: "ê¸°ì¨", 1: "ë¶ˆì•ˆ", 2: "ë‹¹í™©", 3: "ë¶„ë…¸",
            4: "ìƒì²˜", 5: "ìŠ¬í””", 6: "ìš°ìš¸", 7: "í¥ë¶„"
        }

        # íŒŒì¼ ê²½ë¡œ ì„¤ì • (ê¸°ì¡´ modelsì™€ ë¶„ë¦¬)
        self.model_path = "emotion_models/scent_emotion_model_v6.keras"
        self.vectorizer_path = "emotion_models/vectorizer.pkl"

        # ëª¨ë¸ ìƒíƒœ
        self.model = None
        self.vectorizer = None
        self.model_loaded = False
        self.vectorizer_loaded = False

        # ì´ˆê¸°í™” ì‹œë„
        self._initialize_model()

        logger.info("ğŸ­ Whiff ê°ì • íƒœê¹… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")

    def _initialize_model(self):
        """ëª¨ë¸ ë° ë²¡í„°ë¼ì´ì € ì´ˆê¸°í™”"""
        try:
            # 1. Keras ëª¨ë¸ ë¡œë“œ ì‹œë„
            if TENSORFLOW_AVAILABLE and os.path.exists(self.model_path):
                logger.info(f"ğŸ¤– Keras ëª¨ë¸ ë¡œë”© ì‹œë„: {self.model_path}")

                model_size = os.path.getsize(self.model_path)
                logger.info(f"ğŸ“„ ëª¨ë¸ íŒŒì¼ í¬ê¸°: {model_size:,}B ({model_size / 1024 / 1024:.1f}MB)")

                self.model = load_model(self.model_path, compile=False)
                self.model_loaded = True

                logger.info(f"âœ… Keras ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
                logger.info(f"ğŸ“Š ëª¨ë¸ ì…ë ¥ shape: {self.model.input_shape}")
                logger.info(f"ğŸ“Š ëª¨ë¸ ì¶œë ¥ shape: {self.model.output_shape}")
            else:
                logger.warning(f"âš ï¸ Keras ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {self.model_path}")

            # 2. ë²¡í„°ë¼ì´ì € ë¡œë“œ ì‹œë„
            if SKLEARN_AVAILABLE and os.path.exists(self.vectorizer_path):
                logger.info(f"ğŸ“¦ ë²¡í„°ë¼ì´ì € ë¡œë”© ì‹œë„: {self.vectorizer_path}")

                with open(self.vectorizer_path, 'rb') as f:
                    self.vectorizer = pickle.load(f)
                self.vectorizer_loaded = True

                logger.info(f"âœ… ë²¡í„°ë¼ì´ì € ë¡œë“œ ì„±ê³µ")
                if hasattr(self.vectorizer, 'vocabulary_'):
                    logger.info(f"ğŸ“Š ì–´íœ˜ í¬ê¸°: {len(self.vectorizer.vocabulary_)}")
            else:
                logger.warning(f"âš ï¸ ë²¡í„°ë¼ì´ì € íŒŒì¼ ì—†ìŒ: {self.vectorizer_path}")

                # ğŸ”§ ì„ì‹œ ë²¡í„°ë¼ì´ì € ìƒì„± (ê°œë°œìš©)
                if SKLEARN_AVAILABLE:
                    logger.info("ğŸ”§ ê°œë°œìš© ì„ì‹œ ë²¡í„°ë¼ì´ì € ìƒì„± ì¤‘...")
                    self._create_temporary_vectorizer()

        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    def _create_temporary_vectorizer(self):
        """ê°œë°œìš© ì„ì‹œ ë²¡í„°ë¼ì´ì € ìƒì„± (vectorizer.pkl ë°›ê¸° ì „ê¹Œì§€)"""
        try:
            # ìƒ˜í”Œ í…ìŠ¤íŠ¸ë¡œ ì„ì‹œ ë²¡í„°ë¼ì´ì € í›ˆë ¨
            sample_texts = [
                "í–¥ê¸°ë¥¼ ë§¡ìœ¼ë‹ˆ ë‚´ ì•ˆì— ë”°ëœ»í•¨ì´ ë²ˆì¡Œë‹¤",
                "ì´ í–¥ì€ ë¶ˆì•ˆí•œ ë§ˆìŒì„ ë‹¬ë˜ì£¼ì§€ ëª»í–ˆë‹¤",
                "ê°‘ìê¸° ë‹¹í™©ìŠ¤ëŸ¬ìš´ í–¥ì´ ì½”ë¥¼ ì°”ë €ë‹¤",
                "í™”ê°€ ë‚˜ëŠ” ëƒ„ìƒˆê°€ ì½”ë¥¼ ìê·¹í–ˆë‹¤",
                "ë§ˆìŒì´ ì•„í”ˆ í–¥ê¸°ì˜€ë‹¤",
                "ìŠ¬í”ˆ ê¸°ì–µì´ ë– ì˜¤ë¥´ëŠ” í–¥ìˆ˜",
                "ìš°ìš¸í•œ ê¸°ë¶„ì´ ë“œëŠ” í–¥",
                "í¥ë¶„ë˜ëŠ” í–¥ê¸°ê°€ ê°€ìŠ´ì„ ë›°ê²Œ í–ˆë‹¤"
            ]

            # TF-IDF ë²¡í„°ë¼ì´ì € ìƒì„± (ëª¨ë¸ í•™ìŠµ ì‹œì™€ ìœ ì‚¬í•œ ì„¤ì •)
            self.vectorizer = TfidfVectorizer(
                max_features=5000,
                ngram_range=(1, 2),
                lowercase=True,
                stop_words=None,
                token_pattern=r'[ê°€-í£a-zA-Z0-9]+',  # í•œê¸€, ì˜ë¬¸, ìˆ«ì
                min_df=1,
                max_df=0.95
            )

            # ìƒ˜í”Œ í…ìŠ¤íŠ¸ë¡œ í”¼íŒ…
            self.vectorizer.fit(sample_texts)
            self.vectorizer_loaded = True

            logger.info("âœ… ì„ì‹œ ë²¡í„°ë¼ì´ì € ìƒì„± ì™„ë£Œ")
            logger.info(f"ğŸ“Š ì„ì‹œ ì–´íœ˜ í¬ê¸°: {len(self.vectorizer.vocabulary_)}")
            logger.warning("âš ï¸ ì´ê²ƒì€ ê°œë°œìš© ì„ì‹œ ë²¡í„°ë¼ì´ì €ì…ë‹ˆë‹¤. vectorizer.pkl ìˆ˜ë ¹ í›„ êµì²´í•˜ì„¸ìš”!")

        except Exception as e:
            logger.error(f"âŒ ì„ì‹œ ë²¡í„°ë¼ì´ì € ìƒì„± ì‹¤íŒ¨: {e}")

    def preprocess_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        if not text:
            return ""

        import re

        # íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê³µë°±ë§Œ ìœ ì§€)
        text = re.sub(r'[^ê°€-í£a-zA-Z0-9\s]', ' ', text)

        # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ ë³€í™˜
        text = re.sub(r'\s+', ' ', text)

        # ì•ë’¤ ê³µë°± ì œê±°
        text = text.strip()

        return text

    def predict_with_ai_model(self, text: str, include_probabilities: bool = False) -> Dict[str, Any]:
        """AI ëª¨ë¸ì„ ì‚¬ìš©í•œ ê°ì • ì˜ˆì¸¡"""
        if not (self.model_loaded and self.vectorizer_loaded):
            raise Exception("AI ëª¨ë¸ ë˜ëŠ” ë²¡í„°ë¼ì´ì €ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        start_time = datetime.now()

        try:
            # 1. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            processed_text = self.preprocess_text(text)

            if not processed_text:
                raise Exception("ì „ì²˜ë¦¬ í›„ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")

            # 2. TF-IDF ë²¡í„°í™”
            text_vector = self.vectorizer.transform([processed_text])

            # 3. ëª¨ë¸ ì˜ˆì¸¡
            predictions = self.model.predict(text_vector.toarray(), verbose=0)
            predicted_label = int(np.argmax(predictions[0]))
            confidence = float(np.max(predictions[0]))

            # 4. ê²°ê³¼ êµ¬ì„±
            result = {
                "emotion": self.emotion_labels[predicted_label],
                "confidence": confidence,
                "label": predicted_label,
                "method": "AI ëª¨ë¸ (Keras + TF-IDF)",
                "processing_time": (datetime.now() - start_time).total_seconds()
            }

            if include_probabilities:
                result["probabilities"] = {
                    self.emotion_labels[i]: float(prob)
                    for i, prob in enumerate(predictions[0])
                }

            logger.info(f"ğŸ¤– AI ì˜ˆì¸¡ ì™„ë£Œ: '{text[:30]}...' â†’ {result['emotion']} ({confidence:.3f})")

            return result

        except Exception as e:
            logger.error(f"âŒ AI ëª¨ë¸ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            raise e

    def predict_with_rule_based(self, text: str, include_probabilities: bool = False) -> Dict[str, Any]:
        """ë£° ê¸°ë°˜ ê°ì • ë¶„ì„ (í´ë°±)"""
        start_time = datetime.now()

        try:
            # ê¸°ì¡´ emotion_analyzer ì‚¬ìš©
            analysis_result = emotion_analyzer.analyze_emotion(text)

            if not analysis_result.get("success"):
                raise Exception("ë£° ê¸°ë°˜ ë¶„ì„ ì‹¤íŒ¨")

            # Whiff 8ê°€ì§€ ê°ì •ìœ¼ë¡œ ë§¤í•‘
            rule_emotion = analysis_result.get("primary_emotion", "ì¤‘ë¦½")

            # ê¸°ì¡´ ê°ì •ì„ 8ê°€ì§€ ê°ì •ìœ¼ë¡œ ë§¤í•‘
            emotion_mapping = {
                "ê¸°ì¨": (0, "ê¸°ì¨"),
                "ë¶ˆì•ˆ": (1, "ë¶ˆì•ˆ"),
                "ë‹¹í™©": (2, "ë‹¹í™©"),
                "ë¶„ë…¸": (3, "ë¶„ë…¸"),
                "ìƒì²˜": (4, "ìƒì²˜"),
                "ìŠ¬í””": (5, "ìŠ¬í””"),
                "ìš°ìš¸": (6, "ìš°ìš¸"),
                "í¥ë¶„": (7, "í¥ë¶„"),
                "ì¤‘ë¦½": (0, "ê¸°ì¨")  # ê¸°ë³¸ê°’
            }

            label, emotion = emotion_mapping.get(rule_emotion, (0, "ê¸°ì¨"))
            confidence = analysis_result.get("confidence", 0.5)

            result = {
                "emotion": emotion,
                "confidence": confidence,
                "label": label,
                "method": "ë£° ê¸°ë°˜ (í´ë°±)",
                "processing_time": (datetime.now() - start_time).total_seconds()
            }

            if include_probabilities:
                # ê¸°ë³¸ í™•ë¥  ë¶„í¬ ìƒì„±
                probs = [0.125] * 8  # ê· ë“± ë¶„í¬
                probs[label] = confidence
                # ì •ê·œí™”
                total = sum(probs)
                probs = [p / total for p in probs]

                result["probabilities"] = {
                    self.emotion_labels[i]: prob
                    for i, prob in enumerate(probs)
                }

            logger.info(f"ğŸ“‹ ë£° ê¸°ë°˜ ì˜ˆì¸¡ ì™„ë£Œ: '{text[:30]}...' â†’ {emotion} ({confidence:.3f})")

            return result

        except Exception as e:
            logger.error(f"âŒ ë£° ê¸°ë°˜ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            raise e

    def predict_emotion(self, text: str, include_probabilities: bool = False) -> Dict[str, Any]:
        """ê°ì • ì˜ˆì¸¡ ë©”ì¸ í•¨ìˆ˜ (AI ëª¨ë¸ ìš°ì„ , ì‹¤íŒ¨ ì‹œ ë£° ê¸°ë°˜)"""

        # 1. AI ëª¨ë¸ ì‹œë„
        if self.model_loaded and self.vectorizer_loaded:
            try:
                return self.predict_with_ai_model(text, include_probabilities)
            except Exception as e:
                logger.warning(f"âš ï¸ AI ëª¨ë¸ ì‹¤íŒ¨, ë£° ê¸°ë°˜ìœ¼ë¡œ í´ë°±: {e}")

        # 2. ë£° ê¸°ë°˜ í´ë°±
        try:
            return self.predict_with_rule_based(text, include_probabilities)
        except Exception as e:
            logger.error(f"âŒ ëª¨ë“  ì˜ˆì¸¡ ë°©ë²• ì‹¤íŒ¨: {e}")

            # 3. ìµœì¢… ì•ˆì „ì¥ì¹˜
            return {
                "emotion": "ê¸°ì¨",
                "confidence": 0.0,
                "label": 0,
                "method": "ê¸°ë³¸ê°’ (ì—ëŸ¬ ë°œìƒ)",
                "processing_time": 0.0,
                "error": str(e)
            }

    def get_system_status(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ìƒíƒœ ë°˜í™˜"""
        return {
            "ai_model_available": self.model_loaded,
            "vectorizer_available": self.vectorizer_loaded,
            "fallback_available": True,  # ë£° ê¸°ë°˜ì€ í•­ìƒ ì‚¬ìš© ê°€ëŠ¥
            "supported_emotions": list(self.emotion_labels.values()),
            "model_info": {
                "tensorflow_available": TENSORFLOW_AVAILABLE,
                "sklearn_available": SKLEARN_AVAILABLE,
                "model_path": self.model_path,
                "vectorizer_path": self.vectorizer_path,
                "model_file_exists": os.path.exists(self.model_path),
                "vectorizer_file_exists": os.path.exists(self.vectorizer_path)
            }
        }


# â”€â”€â”€ 3. ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
emotion_tagger = None


def get_emotion_tagger():
    """ê°ì • íƒœê¹… ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì˜ì¡´ì„± ì£¼ì…ìš©)"""
    global emotion_tagger
    if emotion_tagger is None:
        emotion_tagger = WhiffEmotionTagger()
    return emotion_tagger


# â”€â”€â”€ 4. ë¼ìš°í„° ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
router = APIRouter(prefix="/emotion", tags=["Emotion Tagging"])

# ì‹œì‘ ì‹œ ëª¨ë¸ ì´ˆê¸°í™”
logger.info("ğŸš€ ê°ì • íƒœê¹… ë¼ìš°í„° ì´ˆê¸°í™” ì‹œì‘...")
try:
    emotion_tagger = WhiffEmotionTagger()
    logger.info("âœ… ê°ì • íƒœê¹… ë¼ìš°í„° ì´ˆê¸°í™” ì™„ë£Œ")
except Exception as e:
    logger.error(f"âŒ ê°ì • íƒœê¹… ë¼ìš°í„° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")


# â”€â”€â”€ 5. API ì—”ë“œí¬ì¸íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@router.post(
    "/predict",
    response_model=EmotionPredictResponse,
    summary="ì‹œí–¥ ì¼ê¸° ê°ì • ì˜ˆì¸¡",
    description=(
            "ğŸ­ **ì‹œí–¥ ì¼ê¸° í…ìŠ¤íŠ¸ì˜ ê°ì •ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤**\n\n"
            "**ğŸ¤– ì˜ˆì¸¡ ë°©ì‹:**\n"
            "1. **AI ëª¨ë¸ ìš°ì„ **: Keras + TF-IDF ê¸°ë°˜ 8ê°€ì§€ ê°ì • ë¶„ë¥˜\n"
            "2. **ë£° ê¸°ë°˜ í´ë°±**: AI ëª¨ë¸ ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë£° ê¸°ë°˜ ë¶„ì„ ì‚¬ìš©\n"
            "3. **ì•ˆì „ì¥ì¹˜**: ëª¨ë“  ë°©ë²• ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜\n\n"
            "**ğŸ“Š ì§€ì› ê°ì • (8ê°€ì§€):**\n"
            "- ê¸°ì¨ (0), ë¶ˆì•ˆ (1), ë‹¹í™© (2), ë¶„ë…¸ (3)\n"
            "- ìƒì²˜ (4), ìŠ¬í”” (5), ìš°ìš¸ (6), í¥ë¶„ (7)\n\n"
            "**ğŸ’¡ ì‚¬ìš© ì˜ˆì‹œ:**\n"
            "```json\n"
            "{\n"
            "  \"text\": \"í–¥ê¸°ë¥¼ ë§¡ìœ¼ë‹ˆ ë‚´ ì•ˆì— ë”°ëœ»í•¨ì´ ë²ˆì¡Œë‹¤\",\n"
            "  \"include_probabilities\": true\n"
            "}\n"
            "```"
    )
)
async def predict_emotion(
        request: EmotionPredictRequest,
        tagger: WhiffEmotionTagger = Depends(get_emotion_tagger)
):
    """ê°ì • ì˜ˆì¸¡ API"""

    try:
        logger.info(f"ğŸ­ ê°ì • ì˜ˆì¸¡ ìš”ì²­: '{request.text[:50]}...'")

        # ê°ì • ì˜ˆì¸¡ ìˆ˜í–‰
        result = tagger.predict_emotion(
            text=request.text,
            include_probabilities=request.include_probabilities
        )

        # ì‘ë‹µ í˜•íƒœë¡œ ë³€í™˜
        response = EmotionPredictResponse(
            emotion=result["emotion"],
            confidence=result["confidence"],
            label=result["label"],
            method=result["method"],
            processing_time=result["processing_time"],
            probabilities=result.get("probabilities")
        )

        logger.info(f"âœ… ê°ì • ì˜ˆì¸¡ ì™„ë£Œ: {response.emotion} ({response.confidence:.3f})")

        return response

    except Exception as e:
        logger.error(f"âŒ ê°ì • ì˜ˆì¸¡ API ì˜¤ë¥˜: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"ê°ì • ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.get(
    "/health",
    response_model=EmotionHealthResponse,
    summary="ê°ì • íƒœê¹… ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸",
    description=(
            "ğŸ” **ê°ì • íƒœê¹… ì‹œìŠ¤í…œì˜ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤**\n\n"
            "**ğŸ“Š í™•ì¸ í•­ëª©:**\n"
            "- AI ëª¨ë¸ ë¡œë”© ìƒíƒœ\n"
            "- ë²¡í„°ë¼ì´ì € ë¡œë”© ìƒíƒœ\n"
            "- ë£° ê¸°ë°˜ í´ë°± ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€\n"
            "- ì§€ì›í•˜ëŠ” ê°ì • ëª©ë¡\n"
            "- ì‹œìŠ¤í…œ í™˜ê²½ ì •ë³´\n\n"
            "**ğŸ’¡ ê°œë°œ ë‹¨ê³„ì—ì„œ í™œìš©:**\n"
            "- ëª¨ë¸ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸\n"
            "- ì˜ì¡´ì„± ì„¤ì¹˜ ìƒíƒœ ì ê²€\n"
            "- ë°°í¬ í›„ ì •ìƒ ë™ì‘ ê²€ì¦"
    )
)
async def check_health(tagger: WhiffEmotionTagger = Depends(get_emotion_tagger)):
    """ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ API"""

    try:
        status = tagger.get_system_status()

        response = EmotionHealthResponse(
            ai_model_available=status["ai_model_available"],
            vectorizer_available=status["vectorizer_available"],
            fallback_available=status["fallback_available"],
            supported_emotions=status["supported_emotions"],
            model_info=status["model_info"]
        )

        return response

    except Exception as e:
        logger.error(f"âŒ ìƒíƒœ í™•ì¸ ì˜¤ë¥˜: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"ìƒíƒœ í™•ì¸ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.post(
    "/test",
    summary="ê°ì • íƒœê¹… í…ŒìŠ¤íŠ¸ (ê°œë°œìš©)",
    description="ê°œë°œ/ë””ë²„ê¹…ìš© ê°ì • íƒœê¹… í…ŒìŠ¤íŠ¸ API"
)
async def test_emotion_prediction(
        texts: List[str],
        tagger: WhiffEmotionTagger = Depends(get_emotion_tagger)
):
    """ê°œë°œìš© ì¼ê´„ í…ŒìŠ¤íŠ¸ API"""

    try:
        results = []

        for text in texts[:10]:  # ìµœëŒ€ 10ê°œê¹Œì§€
            result = tagger.predict_emotion(text, include_probabilities=True)
            results.append({
                "input": text,
                "output": result
            })

        return {
            "test_results": results,
            "system_status": tagger.get_system_status(),
            "total_tests": len(results)
        }

    except Exception as e:
        logger.error(f"âŒ í…ŒìŠ¤íŠ¸ API ì˜¤ë¥˜: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )