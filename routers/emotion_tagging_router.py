# routers/emotion_tagging_router.py
# üéØ Í∞êÏ†ï ÌÉúÍπÖ AI Î™®Îç∏ API ÎùºÏö∞ÌÑ∞ (scent_emotion_model_v6.keras Ïó∞Îèô)

import os
import logging
import numpy as np
import pickle
from datetime import datetime
from typing import Dict, List, Optional, Any
from fastapi import APIRouter, HTTPException, Depends
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field, validator

# TensorFlow ÎèôÏ†Å ÏûÑÌè¨Ìä∏ (vectorizer.pkl Î∞õÍ∏∞ Ï†ÑÍπåÏßÄÎäî Ï£ºÏÑù Ï≤òÎ¶¨)
try:
    import tensorflow as tf
    from tensorflow.keras.models import load_model

    TENSORFLOW_AVAILABLE = True
    logger = logging.getLogger("emotion_tagging")
    logger.info("‚úÖ TensorFlow ÏÇ¨Ïö© Í∞ÄÎä•")
except ImportError as e:
    TENSORFLOW_AVAILABLE = False
    logger = logging.getLogger("emotion_tagging")
    logger.warning(f"‚ö†Ô∏è TensorFlow ÏóÜÏùå: {e}")

# scikit-learn ÎèôÏ†Å ÏûÑÌè¨Ìä∏
try:
    from sklearn.feature_extraction.text import TfidfVectorizer

    SKLEARN_AVAILABLE = True
    logger.info("‚úÖ scikit-learn ÏÇ¨Ïö© Í∞ÄÎä•")
except ImportError as e:
    SKLEARN_AVAILABLE = False
    logger.warning(f"‚ö†Ô∏è scikit-learn ÏóÜÏùå: {e}")

# Í∏∞Ï°¥ Í∞êÏ†ï Î∂ÑÏÑùÍ∏∞ ÏûÑÌè¨Ìä∏ (Î£∞ Í∏∞Î∞ò Ìè¥Î∞±Ïö©)
from utils.emotion_analyzer import emotion_analyzer


# ‚îÄ‚îÄ‚îÄ 1. Ïä§ÌÇ§Îßà Ï†ïÏùò ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
class EmotionPredictRequest(BaseModel):
    text: str = Field(..., min_length=1, max_length=2000, description="Í∞êÏ†ïÏùÑ Î∂ÑÏÑùÌï† ÌÖçÏä§Ìä∏")
    include_probabilities: bool = Field(False, description="Ï†ÑÏ≤¥ Í∞êÏ†ï ÌôïÎ•† Î∂ÑÌè¨ Ìè¨Ìï® Ïó¨Î∂Ä")

    @validator('text')
    def validate_text(cls, v):
        if not v.strip():
            raise ValueError('ÌÖçÏä§Ìä∏Í∞Ä ÎπÑÏñ¥ÏûàÏäµÎãàÎã§.')
        return v.strip()


class EmotionPredictResponse(BaseModel):
    emotion: str = Field(..., description="ÏòàÏ∏°Îêú Í∞êÏ†ï")
    confidence: float = Field(..., ge=0.0, le=1.0, description="Ïã†Î¢∞ÎèÑ (0.0-1.0)")
    label: int = Field(..., ge=0, le=7, description="Í∞êÏ†ï ÎùºÎ≤® (0-7)")
    method: str = Field(..., description="ÏÇ¨Ïö©Îêú Î∂ÑÏÑù Î∞©Î≤ï")
    processing_time: float = Field(..., description="Ï≤òÎ¶¨ ÏãúÍ∞Ñ (Ï¥à)")
    probabilities: Optional[Dict[str, float]] = Field(None, description="Ï†ÑÏ≤¥ Í∞êÏ†ïÎ≥Ñ ÌôïÎ•† Î∂ÑÌè¨")


class EmotionHealthResponse(BaseModel):
    ai_model_available: bool = Field(..., description="AI Î™®Îç∏ ÏÇ¨Ïö© Í∞ÄÎä• Ïó¨Î∂Ä")
    vectorizer_available: bool = Field(..., description="Î≤°ÌÑ∞ÎùºÏù¥Ï†Ä ÏÇ¨Ïö© Í∞ÄÎä• Ïó¨Î∂Ä")
    fallback_available: bool = Field(..., description="Î£∞ Í∏∞Î∞ò Ìè¥Î∞± ÏÇ¨Ïö© Í∞ÄÎä• Ïó¨Î∂Ä")
    supported_emotions: List[str] = Field(..., description="ÏßÄÏõêÌïòÎäî Í∞êÏ†ï Î™©Î°ù")
    model_info: Dict[str, Any] = Field(..., description="Î™®Îç∏ Ï†ïÎ≥¥")


# ‚îÄ‚îÄ‚îÄ 2. Í∞êÏ†ï ÌÉúÍπÖ ÌÅ¥ÎûòÏä§ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
class WhiffEmotionTagger:
    """Whiff Ï†ÑÏö© Í∞êÏ†ï ÌÉúÍπÖ ÌÅ¥ÎûòÏä§"""

    def __init__(self):
        # 8Í∞ÄÏßÄ Í∞êÏ†ï Îß§Ìïë (Îç∞Ïù¥ÌÑ∞ÏÖãÍ≥º ÎèôÏùº)
        self.emotion_labels = {
            0: "Í∏∞ÏÅ®", 1: "Î∂àÏïà", 2: "ÎãπÌô©", 3: "Î∂ÑÎÖ∏",
            4: "ÏÉÅÏ≤ò", 5: "Ïä¨Ìîî", 6: "Ïö∞Ïö∏", 7: "Ìù•Î∂Ñ"
        }

        # ÌååÏùº Í≤ΩÎ°ú ÏÑ§Ï†ï (Í∏∞Ï°¥ modelsÏôÄ Î∂ÑÎ¶¨)
        self.model_path = "emotion_models/scent_emotion_model_v6.keras"
        self.vectorizer_path = "emotion_models/vectorizer.pkl"

        # Î™®Îç∏ ÏÉÅÌÉú
        self.model = None
        self.vectorizer = None
        self.model_loaded = False
        self.vectorizer_loaded = False

        # Ï¥àÍ∏∞Ìôî ÏãúÎèÑ
        self._initialize_model()

        logger.info("üé≠ Whiff Í∞êÏ†ï ÌÉúÍπÖ ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")

    def _initialize_model(self):
        """Î™®Îç∏ Î∞è Î≤°ÌÑ∞ÎùºÏù¥Ï†Ä Ï¥àÍ∏∞Ìôî"""
        try:
            # 1. Keras Î™®Îç∏ Î°úÎìú ÏãúÎèÑ
            if TENSORFLOW_AVAILABLE and os.path.exists(self.model_path):
                logger.info(f"ü§ñ Keras Î™®Îç∏ Î°úÎî© ÏãúÎèÑ: {self.model_path}")

                model_size = os.path.getsize(self.model_path)
                logger.info(f"üìÑ Î™®Îç∏ ÌååÏùº ÌÅ¨Í∏∞: {model_size:,}B ({model_size / 1024 / 1024:.1f}MB)")

                self.model = load_model(self.model_path, compile=False)
                self.model_loaded = True

                logger.info(f"‚úÖ Keras Î™®Îç∏ Î°úÎìú ÏÑ±Í≥µ")
                logger.info(f"üìä Î™®Îç∏ ÏûÖÎ†• shape: {self.model.input_shape}")
                logger.info(f"üìä Î™®Îç∏ Ï∂úÎ†• shape: {self.model.output_shape}")
            else:
                logger.warning(f"‚ö†Ô∏è Keras Î™®Îç∏ ÌååÏùº ÏóÜÏùå: {self.model_path}")

            # 2. Î≤°ÌÑ∞ÎùºÏù¥Ï†Ä Î°úÎìú ÏãúÎèÑ
            if SKLEARN_AVAILABLE and os.path.exists(self.vectorizer_path):
                logger.info(f"üì¶ Î≤°ÌÑ∞ÎùºÏù¥Ï†Ä Î°úÎî© ÏãúÎèÑ: {self.vectorizer_path}")

                with open(self.vectorizer_path, 'rb') as f:
                    self.vectorizer = pickle.load(f)
                self.vectorizer_loaded = True

                logger.info(f"‚úÖ Î≤°ÌÑ∞ÎùºÏù¥Ï†Ä Î°úÎìú ÏÑ±Í≥µ")
                if hasattr(self.vectorizer, 'vocabulary_'):
                    logger.info(f"üìä Ïñ¥Ìúò ÌÅ¨Í∏∞: {len(self.vectorizer.vocabulary_)}")
            else:
                logger.warning(f"‚ö†Ô∏è Î≤°ÌÑ∞ÎùºÏù¥Ï†Ä ÌååÏùº ÏóÜÏùå: {self.vectorizer_path}")

                # üîß ÏûÑÏãú Î≤°ÌÑ∞ÎùºÏù¥Ï†Ä ÏÉùÏÑ± (Í∞úÎ∞úÏö©)
                if SKLEARN_AVAILABLE:
                    logger.info("üîß Í∞úÎ∞úÏö© ÏûÑÏãú Î≤°ÌÑ∞ÎùºÏù¥Ï†Ä ÏÉùÏÑ± Ï§ë...")
                    self._create_temporary_vectorizer()

        except Exception as e:
            logger.error(f"‚ùå Î™®Îç∏ Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")

    def _create_temporary_vectorizer(self):
        """Í∞úÎ∞úÏö© ÏûÑÏãú Î≤°ÌÑ∞ÎùºÏù¥Ï†Ä ÏÉùÏÑ± (vectorizer.pkl Î∞õÍ∏∞ Ï†ÑÍπåÏßÄ)"""
        try:
            # ÏÉòÌîå ÌÖçÏä§Ìä∏Î°ú ÏûÑÏãú Î≤°ÌÑ∞ÎùºÏù¥Ï†Ä ÌõàÎ†®
            sample_texts = [
                "Ìñ•Í∏∞Î•º Îß°ÏúºÎãà ÎÇ¥ ÏïàÏóê Îî∞ÎúªÌï®Ïù¥ Î≤àÏ°åÎã§",
                "Ïù¥ Ìñ•ÏùÄ Î∂àÏïàÌïú ÎßàÏùåÏùÑ Îã¨ÎûòÏ£ºÏßÄ Î™ªÌñàÎã§",
                "Í∞ëÏûêÍ∏∞ ÎãπÌô©Ïä§Îü¨Ïö¥ Ìñ•Ïù¥ ÏΩîÎ•º Ï∞îÎ†ÄÎã§",
                "ÌôîÍ∞Ä ÎÇòÎäî ÎÉÑÏÉàÍ∞Ä ÏΩîÎ•º ÏûêÍ∑πÌñàÎã§",
                "ÎßàÏùåÏù¥ ÏïÑÌîà Ìñ•Í∏∞ÏòÄÎã§",
                "Ïä¨Ìîà Í∏∞ÏñµÏù¥ Îñ†Ïò§Î•¥Îäî Ìñ•Ïàò",
                "Ïö∞Ïö∏Ìïú Í∏∞Î∂ÑÏù¥ ÎìúÎäî Ìñ•",
                "Ìù•Î∂ÑÎêòÎäî Ìñ•Í∏∞Í∞Ä Í∞ÄÏä¥ÏùÑ Îõ∞Í≤å ÌñàÎã§"
            ]

            # TF-IDF Î≤°ÌÑ∞ÎùºÏù¥Ï†Ä ÏÉùÏÑ± (Î™®Îç∏ ÌïôÏäµ ÏãúÏôÄ Ïú†ÏÇ¨Ìïú ÏÑ§Ï†ï)
            self.vectorizer = TfidfVectorizer(
                max_features=5000,
                ngram_range=(1, 2),
                lowercase=True,
                stop_words=None,
                token_pattern=r'[Í∞Ä-Ìû£a-zA-Z0-9]+',  # ÌïúÍ∏Ä, ÏòÅÎ¨∏, Ïà´Ïûê
                min_df=1,
                max_df=0.95
            )

            # ÏÉòÌîå ÌÖçÏä§Ìä∏Î°ú ÌîºÌåÖ
            self.vectorizer.fit(sample_texts)
            self.vectorizer_loaded = True

            logger.info("‚úÖ ÏûÑÏãú Î≤°ÌÑ∞ÎùºÏù¥Ï†Ä ÏÉùÏÑ± ÏôÑÎ£å")
            logger.info(f"üìä ÏûÑÏãú Ïñ¥Ìúò ÌÅ¨Í∏∞: {len(self.vectorizer.vocabulary_)}")
            logger.warning("‚ö†Ô∏è Ïù¥Í≤ÉÏùÄ Í∞úÎ∞úÏö© ÏûÑÏãú Î≤°ÌÑ∞ÎùºÏù¥Ï†ÄÏûÖÎãàÎã§. vectorizer.pkl ÏàòÎ†π ÌõÑ ÍµêÏ≤¥ÌïòÏÑ∏Ïöî!")

        except Exception as e:
            logger.error(f"‚ùå ÏûÑÏãú Î≤°ÌÑ∞ÎùºÏù¥Ï†Ä ÏÉùÏÑ± Ïã§Ìå®: {e}")

    def preprocess_text(self, text: str) -> str:
        """ÌÖçÏä§Ìä∏ Ï†ÑÏ≤òÎ¶¨"""
        if not text:
            return ""

        import re

        # ÌäπÏàòÎ¨∏Ïûê Ï†úÍ±∞ (ÌïúÍ∏Ä, ÏòÅÎ¨∏, Ïà´Ïûê, Í≥µÎ∞±Îßå Ïú†ÏßÄ)
        text = re.sub(r'[^Í∞Ä-Ìû£a-zA-Z0-9\s]', ' ', text)

        # Ïó¨Îü¨ Í≥µÎ∞±ÏùÑ ÌïòÎÇòÎ°ú Î≥ÄÌôò
        text = re.sub(r'\s+', ' ', text)

        # ÏïûÎí§ Í≥µÎ∞± Ï†úÍ±∞
        text = text.strip()

        return text

    def predict_with_ai_model(self, text: str, include_probabilities: bool = False) -> Dict[str, Any]:
        """AI Î™®Îç∏ÏùÑ ÏÇ¨Ïö©Ìïú Í∞êÏ†ï ÏòàÏ∏°"""
        if not (self.model_loaded and self.vectorizer_loaded):
            raise Exception("AI Î™®Îç∏ ÎòêÎäî Î≤°ÌÑ∞ÎùºÏù¥Ï†ÄÍ∞Ä Î°úÎìúÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.")

        start_time = datetime.now()

        try:
            # 1. ÌÖçÏä§Ìä∏ Ï†ÑÏ≤òÎ¶¨
            processed_text = self.preprocess_text(text)

            if not processed_text:
                raise Exception("Ï†ÑÏ≤òÎ¶¨ ÌõÑ ÌÖçÏä§Ìä∏Í∞Ä ÎπÑÏñ¥ÏûàÏäµÎãàÎã§.")

            # 2. TF-IDF Î≤°ÌÑ∞Ìôî
            text_vector = self.vectorizer.transform([processed_text])

            # 3. Î™®Îç∏ ÏòàÏ∏°
            predictions = self.model.predict(text_vector.toarray(), verbose=0)
            predicted_label = int(np.argmax(predictions[0]))
            confidence = float(np.max(predictions[0]))

            # 4. Í≤∞Í≥º Íµ¨ÏÑ±
            result = {
                "emotion": self.emotion_labels[predicted_label],
                "confidence": confidence,
                "label": predicted_label,
                "method": "AI Î™®Îç∏ (Keras + TF-IDF)",
                "processing_time": (datetime.now() - start_time).total_seconds()
            }

            if include_probabilities:
                result["probabilities"] = {
                    self.emotion_labels[i]: float(prob)
                    for i, prob in enumerate(predictions[0])
                }

            logger.info(f"ü§ñ AI ÏòàÏ∏° ÏôÑÎ£å: '{text[:30]}...' ‚Üí {result['emotion']} ({confidence:.3f})")

            return result

        except Exception as e:
            logger.error(f"‚ùå AI Î™®Îç∏ ÏòàÏ∏° Ïã§Ìå®: {e}")
            raise e

    def predict_with_rule_based(self, text: str, include_probabilities: bool = False) -> Dict[str, Any]:
        """Î£∞ Í∏∞Î∞ò Í∞êÏ†ï Î∂ÑÏÑù (Ìè¥Î∞±)"""
        start_time = datetime.now()

        try:
            # Í∏∞Ï°¥ emotion_analyzer ÏÇ¨Ïö©
            analysis_result = emotion_analyzer.analyze_emotion(text)

            if not analysis_result.get("success"):
                raise Exception("Î£∞ Í∏∞Î∞ò Î∂ÑÏÑù Ïã§Ìå®")

            # Whiff 8Í∞ÄÏßÄ Í∞êÏ†ïÏúºÎ°ú Îß§Ìïë
            rule_emotion = analysis_result.get("primary_emotion", "Ï§ëÎ¶Ω")

            # Í∏∞Ï°¥ Í∞êÏ†ïÏùÑ 8Í∞ÄÏßÄ Í∞êÏ†ïÏúºÎ°ú Îß§Ìïë
            emotion_mapping = {
                "Í∏∞ÏÅ®": (0, "Í∏∞ÏÅ®"),
                "Î∂àÏïà": (1, "Î∂àÏïà"),
                "ÎãπÌô©": (2, "ÎãπÌô©"),
                "Î∂ÑÎÖ∏": (3, "Î∂ÑÎÖ∏"),
                "ÏÉÅÏ≤ò": (4, "ÏÉÅÏ≤ò"),
                "Ïä¨Ìîî": (5, "Ïä¨Ìîî"),
                "Ïö∞Ïö∏": (6, "Ïö∞Ïö∏"),
                "Ìù•Î∂Ñ": (7, "Ìù•Î∂Ñ"),
                "Ï§ëÎ¶Ω": (0, "Í∏∞ÏÅ®")  # Í∏∞Î≥∏Í∞í
            }

            label, emotion = emotion_mapping.get(rule_emotion, (0, "Í∏∞ÏÅ®"))
            confidence = analysis_result.get("confidence", 0.5)

            result = {
                "emotion": emotion,
                "confidence": confidence,
                "label": label,
                "method": "Î£∞ Í∏∞Î∞ò (Ìè¥Î∞±)",
                "processing_time": (datetime.now() - start_time).total_seconds()
            }

            if include_probabilities:
                # Í∏∞Î≥∏ ÌôïÎ•† Î∂ÑÌè¨ ÏÉùÏÑ±
                probs = [0.125] * 8  # Í∑†Îì± Î∂ÑÌè¨
                probs[label] = confidence
                # Ï†ïÍ∑úÌôî
                total = sum(probs)
                probs = [p / total for p in probs]

                result["probabilities"] = {
                    self.emotion_labels[i]: prob
                    for i, prob in enumerate(probs)
                }

            logger.info(f"üìã Î£∞ Í∏∞Î∞ò ÏòàÏ∏° ÏôÑÎ£å: '{text[:30]}...' ‚Üí {emotion} ({confidence:.3f})")

            return result

        except Exception as e:
            logger.error(f"‚ùå Î£∞ Í∏∞Î∞ò ÏòàÏ∏° Ïã§Ìå®: {e}")
            raise e

    def predict_emotion(self, text: str, include_probabilities: bool = False) -> Dict[str, Any]:
        """Í∞êÏ†ï ÏòàÏ∏° Î©îÏù∏ Ìï®Ïàò (AI Î™®Îç∏ Ïö∞ÏÑ†, Ïã§Ìå® Ïãú Î£∞ Í∏∞Î∞ò)"""

        # 1. AI Î™®Îç∏ ÏãúÎèÑ
        if self.model_loaded and self.vectorizer_loaded:
            try:
                return self.predict_with_ai_model(text, include_probabilities)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è AI Î™®Îç∏ Ïã§Ìå®, Î£∞ Í∏∞Î∞òÏúºÎ°ú Ìè¥Î∞±: {e}")

        # 2. Î£∞ Í∏∞Î∞ò Ìè¥Î∞±
        try:
            return self.predict_with_rule_based(text, include_probabilities)
        except Exception as e:
            logger.error(f"‚ùå Î™®Îì† ÏòàÏ∏° Î∞©Î≤ï Ïã§Ìå®: {e}")

            # 3. ÏµúÏ¢Ö ÏïàÏ†ÑÏû•Ïπò
            return {
                "emotion": "Í∏∞ÏÅ®",
                "confidence": 0.0,
                "label": 0,
                "method": "Í∏∞Î≥∏Í∞í (ÏóêÎü¨ Î∞úÏÉù)",
                "processing_time": 0.0,
                "error": str(e)
            }

    def get_system_status(self) -> Dict[str, Any]:
        """ÏãúÏä§ÌÖú ÏÉÅÌÉú Î∞òÌôò"""
        return {
            "ai_model_available": self.model_loaded,
            "vectorizer_available": self.vectorizer_loaded,
            "fallback_available": True,  # Î£∞ Í∏∞Î∞òÏùÄ Ìï≠ÏÉÅ ÏÇ¨Ïö© Í∞ÄÎä•
            "supported_emotions": list(self.emotion_labels.values()),
            "model_info": {
                "tensorflow_available": TENSORFLOW_AVAILABLE,
                "sklearn_available": SKLEARN_AVAILABLE,
                "model_path": self.model_path,
                "vectorizer_path": self.vectorizer_path,
                "model_file_exists": os.path.exists(self.model_path),
                "vectorizer_file_exists": os.path.exists(self.vectorizer_path)
            }
        }


# ‚îÄ‚îÄ‚îÄ 3. Ï†ÑÏó≠ Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉùÏÑ± ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
emotion_tagger = None


def get_emotion_tagger():
    """Í∞êÏ†ï ÌÉúÍπÖ Ïù∏Ïä§ÌÑ¥Ïä§ Î∞òÌôò (ÏùòÏ°¥ÏÑ± Ï£ºÏûÖÏö©)"""
    global emotion_tagger
    if emotion_tagger is None:
        emotion_tagger = WhiffEmotionTagger()
    return emotion_tagger


# ‚îÄ‚îÄ‚îÄ 4. ÎùºÏö∞ÌÑ∞ ÏÑ§Ï†ï ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
router = APIRouter(prefix="/emotion", tags=["Emotion Tagging"])

# ÏãúÏûë Ïãú Î™®Îç∏ Ï¥àÍ∏∞Ìôî
logger.info("üöÄ Í∞êÏ†ï ÌÉúÍπÖ ÎùºÏö∞ÌÑ∞ Ï¥àÍ∏∞Ìôî ÏãúÏûë...")
try:
    emotion_tagger = WhiffEmotionTagger()
    logger.info("‚úÖ Í∞êÏ†ï ÌÉúÍπÖ ÎùºÏö∞ÌÑ∞ Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
except Exception as e:
    logger.error(f"‚ùå Í∞êÏ†ï ÌÉúÍπÖ ÎùºÏö∞ÌÑ∞ Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")


# ‚îÄ‚îÄ‚îÄ 5. API ÏóîÎìúÌè¨Ïù∏Ìä∏ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

@router.post(
    "/predict",
    response_model=EmotionPredictResponse,
    summary="ÏãúÌñ• ÏùºÍ∏∞ Í∞êÏ†ï ÏòàÏ∏°",
    description=(
            "üé≠ **ÏãúÌñ• ÏùºÍ∏∞ ÌÖçÏä§Ìä∏Ïùò Í∞êÏ†ïÏùÑ ÏòàÏ∏°Ìï©ÎãàÎã§**\n\n"
            "**ü§ñ ÏòàÏ∏° Î∞©Ïãù:**\n"
            "1. **AI Î™®Îç∏ Ïö∞ÏÑ†**: Keras + TF-IDF Í∏∞Î∞ò 8Í∞ÄÏßÄ Í∞êÏ†ï Î∂ÑÎ•ò\n"
            "2. **Î£∞ Í∏∞Î∞ò Ìè¥Î∞±**: AI Î™®Îç∏ Ïã§Ìå® Ïãú Í∏∞Ï°¥ Î£∞ Í∏∞Î∞ò Î∂ÑÏÑù ÏÇ¨Ïö©\n"
            "3. **ÏïàÏ†ÑÏû•Ïπò**: Î™®Îì† Î∞©Î≤ï Ïã§Ìå® Ïãú Í∏∞Î≥∏Í∞í Î∞òÌôò\n\n"
            "**üìä ÏßÄÏõê Í∞êÏ†ï (8Í∞ÄÏßÄ):**\n"
            "- Í∏∞ÏÅ® (0), Î∂àÏïà (1), ÎãπÌô© (2), Î∂ÑÎÖ∏ (3)\n"
            "- ÏÉÅÏ≤ò (4), Ïä¨Ìîî (5), Ïö∞Ïö∏ (6), Ìù•Î∂Ñ (7)\n\n"
            "**üí° ÏÇ¨Ïö© ÏòàÏãú:**\n"
            "```json\n"
            "{\n"
            "  \"text\": \"Ìñ•Í∏∞Î•º Îß°ÏúºÎãà ÎÇ¥ ÏïàÏóê Îî∞ÎúªÌï®Ïù¥ Î≤àÏ°åÎã§\",\n"
            "  \"include_probabilities\": true\n"
            "}\n"
            "```"
    )
)
async def predict_emotion(
        request: EmotionPredictRequest,
        tagger: WhiffEmotionTagger = Depends(get_emotion_tagger)
):
    """Í∞êÏ†ï ÏòàÏ∏° API"""

    try:
        logger.info(f"üé≠ Í∞êÏ†ï ÏòàÏ∏° ÏöîÏ≤≠: '{request.text[:50]}...'")

        # Í∞êÏ†ï ÏòàÏ∏° ÏàòÌñâ
        result = tagger.predict_emotion(
            text=request.text,
            include_probabilities=request.include_probabilities
        )

        # ÏùëÎãµ ÌòïÌÉúÎ°ú Î≥ÄÌôò
        response = EmotionPredictResponse(
            emotion=result["emotion"],
            confidence=result["confidence"],
            label=result["label"],
            method=result["method"],
            processing_time=result["processing_time"],
            probabilities=result.get("probabilities")
        )

        logger.info(f"‚úÖ Í∞êÏ†ï ÏòàÏ∏° ÏôÑÎ£å: {response.emotion} ({response.confidence:.3f})")

        return response

    except Exception as e:
        logger.error(f"‚ùå Í∞êÏ†ï ÏòàÏ∏° API Ïò§Î•ò: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Í∞êÏ†ï ÏòàÏ∏° Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)}"
        )


@router.get(
    "/health",
    response_model=EmotionHealthResponse,
    summary="Í∞êÏ†ï ÌÉúÍπÖ ÏãúÏä§ÌÖú ÏÉÅÌÉú ÌôïÏù∏",
    description=(
            "üîç **Í∞êÏ†ï ÌÉúÍπÖ ÏãúÏä§ÌÖúÏùò ÏÉÅÌÉúÎ•º ÌôïÏù∏Ìï©ÎãàÎã§**\n\n"
            "**üìä ÌôïÏù∏ Ìï≠Î™©:**\n"
            "- AI Î™®Îç∏ Î°úÎî© ÏÉÅÌÉú\n"
            "- Î≤°ÌÑ∞ÎùºÏù¥Ï†Ä Î°úÎî© ÏÉÅÌÉú\n"
            "- Î£∞ Í∏∞Î∞ò Ìè¥Î∞± ÏÇ¨Ïö© Í∞ÄÎä• Ïó¨Î∂Ä\n"
            "- ÏßÄÏõêÌïòÎäî Í∞êÏ†ï Î™©Î°ù\n"
            "- ÏãúÏä§ÌÖú ÌôòÍ≤Ω Ï†ïÎ≥¥\n\n"
            "**üí° Í∞úÎ∞ú Îã®Í≥ÑÏóêÏÑú ÌôúÏö©:**\n"
            "- Î™®Îç∏ ÌååÏùº Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏\n"
            "- ÏùòÏ°¥ÏÑ± ÏÑ§Ïπò ÏÉÅÌÉú Ï†êÍ≤Ä\n"
            "- Î∞∞Ìè¨ ÌõÑ Ï†ïÏÉÅ ÎèôÏûë Í≤ÄÏ¶ù"
    )
)
async def check_health(tagger: WhiffEmotionTagger = Depends(get_emotion_tagger)):
    """ÏãúÏä§ÌÖú ÏÉÅÌÉú ÌôïÏù∏ API"""

    try:
        status = tagger.get_system_status()

        response = EmotionHealthResponse(
            ai_model_available=status["ai_model_available"],
            vectorizer_available=status["vectorizer_available"],
            fallback_available=status["fallback_available"],
            supported_emotions=status["supported_emotions"],
            model_info=status["model_info"]
        )

        return response

    except Exception as e:
        logger.error(f"‚ùå ÏÉÅÌÉú ÌôïÏù∏ Ïò§Î•ò: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"ÏÉÅÌÉú ÌôïÏù∏ Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)}"
        )


@router.post(
    "/test",
    summary="Í∞êÏ†ï ÌÉúÍπÖ ÌÖåÏä§Ìä∏ (Í∞úÎ∞úÏö©)",
    description="Í∞úÎ∞ú/ÎîîÎ≤ÑÍπÖÏö© Í∞êÏ†ï ÌÉúÍπÖ ÌÖåÏä§Ìä∏ API"
)
async def test_emotion_prediction(
        texts: List[str],
        tagger: WhiffEmotionTagger = Depends(get_emotion_tagger)
):
    """Í∞úÎ∞úÏö© ÏùºÍ¥Ñ ÌÖåÏä§Ìä∏ API"""

    try:
        results = []

        for text in texts[:10]:  # ÏµúÎåÄ 10Í∞úÍπåÏßÄ
            result = tagger.predict_emotion(text, include_probabilities=True)
            results.append({
                "input": text,
                "output": result
            })

        return {
            "test_results": results,
            "system_status": tagger.get_system_status(),
            "total_tests": len(results)
        }

    except Exception as e:
        logger.error(f"‚ùå ÌÖåÏä§Ìä∏ API Ïò§Î•ò: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"ÌÖåÏä§Ìä∏ Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)}"
        )